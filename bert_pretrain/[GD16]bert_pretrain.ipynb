{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 프로젝트 : mini BERT 만들기\n",
    "- vocab size => 8000\n",
    "- 전체 파라미터 사이즈 = 1M \n",
    "- 10Epoch 까지 학습"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Tokenizer 준비\n",
    "2. 데이터 전처리 (1) MASK 생성\n",
    "3. 데이터 전처리 (2) NSP pair 생성\n",
    "4. 데이터 전처리 (3) 데이터셋 완성\n",
    "5. BERT 모델 구현\n",
    "6. pretrain 진행\n",
    "7. 프로젝트 결과"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 루브릭\n",
    "\n",
    "#### 평가문항\n",
    "\n",
    "1. 한글 코퍼스를 가공하여 BERT pretrain용 데이터셋을 잘 생성하였다.\n",
    "    - MLM, NSP task의 특징이 잘 반영된 pretrain용 데이터셋 생성과정이 체계적으로 진행되었다.\n",
    "    \n",
    "2. 구현한 BERT 모델의 학습이 안정적으로 진행됨을 확인하였다.\n",
    "    - 학습진행 과정 중에 MLM, NSP loss의 안정적인 감소가 확인되었다.\n",
    "    \n",
    "3. 1M짜리 mini BERT 모델의 제작과 학습이 정상적으로 진행되었다.\n",
    "    - 학습된 모델 및 학습과정의 시각화 내역이 제출되었다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 목차\n",
    "- Tokenizer 준비\n",
    "- 데이터 전처리 (1) MASK 생성\n",
    "- 데이터 전처리 (2) NSP pair 생성\n",
    "- 데이터 전처리 (3) 데이터셋 완성\n",
    "- BERT 모델 구현\n",
    "- pretrain 진행\n",
    "- 프로젝트 : mini BERT 만들기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 데이터 다운로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# $ mkdir -p ~/aiffel/bert_pretrain/data\n",
    "# $ mkdir -p ~/aiffel/bert_pretrain/models\n",
    "# $ wget https://aiffelstaticprd.blob.core.windows.net/media/documents/kowiki.txt.zip\n",
    "# $ mv kowiki.txt.zip ~/aiffel/bert_pretrain/data\n",
    "# $ cd ~/aiffel/bert_pretrain/data && unzip kowiki.txt.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 라이브러리 설치"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install sentencepiece\n",
    "# !pip install tqdm\n",
    "# !conda install -c conda-forge ipywidgets\n",
    "# !jupyter nbextension enable --py widgetsnbextension <<"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizer 준비\n",
    "- GPT -> BPE\n",
    "- BERT -> WordPiece\n",
    "- 등 subword 기반 토크나이징 기법이 주요한 방법론으로 등장 \n",
    "- SentencPiece 기반 토크나이저 사용 -> BERT pretain "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "In /home/aiffel/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle: \n",
      "The text.latex.preview rcparam was deprecated in Matplotlib 3.3 and will be removed two minor releases later.\n",
      "In /home/aiffel/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle: \n",
      "The mathtext.fallback_to_cm rcparam was deprecated in Matplotlib 3.3 and will be removed two minor releases later.\n",
      "In /home/aiffel/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle: Support for setting the 'mathtext.fallback_to_cm' rcParam is deprecated since 3.3 and will be removed two minor releases later; use 'mathtext.fallback : 'cm' instead.\n",
      "In /home/aiffel/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle: \n",
      "The validate_bool_maybe_none function was deprecated in Matplotlib 3.3 and will be removed two minor releases later.\n",
      "In /home/aiffel/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle: \n",
      "The savefig.jpeg_quality rcparam was deprecated in Matplotlib 3.3 and will be removed two minor releases later.\n",
      "In /home/aiffel/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle: \n",
      "The keymap.all_axes rcparam was deprecated in Matplotlib 3.3 and will be removed two minor releases later.\n",
      "In /home/aiffel/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle: \n",
      "The animation.avconv_path rcparam was deprecated in Matplotlib 3.3 and will be removed two minor releases later.\n",
      "In /home/aiffel/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle: \n",
      "The animation.avconv_args rcparam was deprecated in Matplotlib 3.3 and will be removed two minor releases later.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3.0\n",
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n",
      "/device:GPU:0\n"
     ]
    }
   ],
   "source": [
    "# imports\n",
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.backend as K\n",
    "\n",
    "import os\n",
    "import re\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import collections\n",
    "import json\n",
    "import shutil\n",
    "import zipfile\n",
    "import copy\n",
    "from datetime import datetime\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import sentencepiece as spm\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "random_seed = 1234\n",
    "random.seed(random_seed)\n",
    "np.random.seed(random_seed)\n",
    "tf.random.set_seed(random_seed)\n",
    "\n",
    "# tf version 및 gpu 확인\n",
    "print(tf.__version__)\n",
    "print(tf.config.list_physical_devices('GPU'))\n",
    "print(tf.test.gpu_device_name())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "완료=3\n"
     ]
    }
   ],
   "source": [
    "# 한글 나무위키 코퍼스 \n",
    "# vocab_size = 32000\n",
    "# BERT에 사용되는 [MASK], [SEP], [CLS]등 주요 특수문자를 vocab에 포함해야함\n",
    "\n",
    "#sentencepiece 모델 생성\n",
    "import sentencepiece as spm\n",
    "import os\n",
    "corpus_file = os.getenv('HOME')+'/aiffel/bert_pretrain/data/kowiki.txt'\n",
    "prefix = 'ko_32000'\n",
    "vocab_size = 32000\n",
    "\n",
    "spm.SentencePieceTrainer.train(\n",
    "    f\"--input={corpus_file} --model_prefix={prefix} --vocab_size={vocab_size + 7}\" + \n",
    "    \" --model_type=bpe\" +\n",
    "    \" --max_sentence_length=999999\" + # 문장 최대 길이\n",
    "    \" --pad_id=0 --pad_piece=[PAD]\" + # pad (0)\n",
    "    \" --unk_id=1 --unk_piece=[UNK]\" + # unknown (1)\n",
    "    \" --bos_id=2 --bos_piece=[BOS]\" + # begin of sequence (2)\n",
    "    \" --eos_id=3 --eos_piece=[EOS]\" + # end of sequence (3)\n",
    "    \" --user_defined_symbols=[SEP],[CLS],[MASK]\") # 사용자 정의 토큰\n",
    "\n",
    "print(\"완료=3\")   # 완료메시지가 출력될 때까지 아무 출력내용이 없더라도 기다려 주세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#센텐스피스 모델 생성후 경로 이동 \n",
    "#$ mv ko_32000.* ~/aiffel/bert_pretrain/models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#경로설정\n",
    "data_dir = os.getenv('HOME')+'/aiffel/bert_pretrain/data'\n",
    "model_dir = os.getenv('HOME')+'/aiffel/bert_pretrain/models'\n",
    "\n",
    "# vocab loading\n",
    "vocab = spm.SentencePieceProcessor()\n",
    "vocab.load(f\"{model_dir}/ko_32000.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 특수 token 7개를 제외한 나머지 tokens 들 << 실행시 SIGTRAP 에러 발생\n",
    "vocab_list = []\n",
    "for id in range(7, len(vocab)):\n",
    "    if not vocab.is_unknown(id):\n",
    "        vocab_list.append(vocab.id_to_piece(id))\n",
    "print(vocab_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', '▁추적', '추', '적', '▁비가', '▁내리는', '▁날', '이었', '어', '▁그날', '은', '▁', '왠', '지', '▁손', '님이', '▁많아', '▁첫', '▁번에', '▁삼', '십', '▁전', '▁둘째', '번', '▁오', '십', '▁전', '▁오랜', '만에', '▁받아', '보는', '▁십', '▁전', '짜리', '▁백', '통', '화', '▁서', '푼', '에', '[SEP]', '▁손바닥', '▁위', '엔', '▁기쁨', '의', '▁눈', '물이', '▁흘러', '▁컬', '컬', '한', '▁목에', '▁모', '주', '▁한', '잔', '을', '▁적', '셔', '▁몇', '▁달', '▁포', '▁전부터', '▁콜', '록', '거리는', '▁아내', '▁생각에', '▁그', '토록', '▁먹고', '▁싶다', '던', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "# [CLS], tokens a, [SEP], tokens b, [SEP] 형태의 token 생성\n",
    "string_a = \"추적추적 비가 내리는 날이었어 그날은 왠지 손님이 많아 첫 번에 삼십 전 둘째번 오십 전 오랜만에 받아보는 십 전짜리 백통화 서푼에\"\n",
    "string_b = \"손바닥 위엔 기쁨의 눈물이 흘러 컬컬한 목에 모주 한잔을 적셔 몇 달 포 전부터 콜록거리는 아내 생각에 그토록 먹고 싶다던\"\n",
    "tokens_org = [\"[CLS]\"] + vocab.encode_as_pieces(string_a) + [\"[SEP]\"] + vocab.encode_as_pieces(string_b) + [\"[SEP]\"]\n",
    "print(tokens_org)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 데이터 전처리 (1) MASK 생성\n",
    "- BERT => Masked Language Model => 마스킹된 단어를 맞추는 방식\n",
    "- 전체 토큰의 15% 가 적정 비율\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', '▁추적', '추', '적', '▁비가', '▁내리는', '▁날', '이었', '어', '▁그날', '은', '▁', '왠', '지', '▁손', '님이', '▁많아', '▁첫', '▁번에', '▁삼', '십', '▁전', '▁둘째', '번', '▁오', '십', '▁전', '▁오랜', '만에', '▁받아', '보는', '▁십', '▁전', '짜리', '▁백', '통', '화', '▁서', '푼', '에', '[SEP]', '▁손바닥', '▁위', '엔', '▁기쁨', '의', '▁눈', '물이', '▁흘러', '▁컬', '컬', '한', '▁목에', '▁모', '주', '▁한', '잔', '을', '▁적', '셔', '▁몇', '▁달', '▁포', '▁전부터', '▁콜', '록', '거리는', '▁아내', '▁생각에', '▁그', '토록', '▁먹고', '▁싶다', '던', '[SEP]']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(tokens_org)\n",
    "\n",
    "# 전체 token의 15% mask\n",
    "mask_cnt = int((len(tokens_org) - 3) * 0.15)\n",
    "mask_cnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#_대, [MASK], 민국 => [MASK] = '한'\n",
    "# '대한민국' 이라는 패턴을 자주 보게 됨\n",
    "# 이를 방지하기 위해 띄어쓰기 단위로 한꺼번에 마스킹 처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3] ['▁추적', '추', '적']\n",
      "[4] ['▁비가']\n",
      "[5] ['▁내리는']\n",
      "[6, 7, 8] ['▁날', '이었', '어']\n",
      "[9, 10] ['▁그날', '은']\n",
      "[11, 12, 13] ['▁', '왠', '지']\n",
      "[14, 15] ['▁손', '님이']\n",
      "[16] ['▁많아']\n",
      "[17] ['▁첫']\n",
      "[18] ['▁번에']\n",
      "[19, 20] ['▁삼', '십']\n",
      "[21] ['▁전']\n",
      "[22, 23] ['▁둘째', '번']\n",
      "[24, 25] ['▁오', '십']\n",
      "[26] ['▁전']\n",
      "[27, 28] ['▁오랜', '만에']\n",
      "[29, 30] ['▁받아', '보는']\n",
      "[31] ['▁십']\n",
      "[32, 33] ['▁전', '짜리']\n",
      "[34, 35, 36] ['▁백', '통', '화']\n",
      "[37, 38, 39] ['▁서', '푼', '에']\n",
      "[41] ['▁손바닥']\n",
      "[42, 43] ['▁위', '엔']\n",
      "[44, 45] ['▁기쁨', '의']\n",
      "[46, 47] ['▁눈', '물이']\n",
      "[48] ['▁흘러']\n",
      "[49, 50, 51] ['▁컬', '컬', '한']\n",
      "[52] ['▁목에']\n",
      "[53, 54] ['▁모', '주']\n",
      "[55, 56, 57] ['▁한', '잔', '을']\n",
      "[58, 59] ['▁적', '셔']\n",
      "[60] ['▁몇']\n",
      "[61] ['▁달']\n",
      "[62] ['▁포']\n",
      "[63] ['▁전부터']\n",
      "[64, 65, 66] ['▁콜', '록', '거리는']\n",
      "[67] ['▁아내']\n",
      "[68] ['▁생각에']\n",
      "[69, 70] ['▁그', '토록']\n",
      "[71] ['▁먹고']\n",
      "[72, 73] ['▁싶다', '던']\n"
     ]
    }
   ],
   "source": [
    "# 띄어쓰기 단위로 mask 하기 위해서 index 분할\n",
    "cand_idx = []  # word 단위의 index array\n",
    "for (i, token) in enumerate(tokens_org):\n",
    "    if token == \"[CLS]\" or token == \"[SEP]\":\n",
    "        continue\n",
    "    if 0 < len(cand_idx) and not token.startswith(u\"\\u2581\"):\n",
    "        cand_idx[-1].append(i)\n",
    "    else:\n",
    "        cand_idx.append([i])\n",
    "\n",
    "# 결과확인\n",
    "for cand in cand_idx:\n",
    "    print(cand, [tokens_org[i] for i in cand])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[18],\n",
       " [44, 45],\n",
       " [24, 25],\n",
       " [49, 50, 51],\n",
       " [31],\n",
       " [63],\n",
       " [41],\n",
       " [52],\n",
       " [22, 23],\n",
       " [71],\n",
       " [17],\n",
       " [19, 20],\n",
       " [60],\n",
       " [32, 33],\n",
       " [62],\n",
       " [46, 47],\n",
       " [29, 30],\n",
       " [72, 73],\n",
       " [6, 7, 8],\n",
       " [64, 65, 66],\n",
       " [67],\n",
       " [9, 10],\n",
       " [26],\n",
       " [55, 56, 57],\n",
       " [61],\n",
       " [34, 35, 36],\n",
       " [37, 38, 39],\n",
       " [21],\n",
       " [58, 59],\n",
       " [48],\n",
       " [69, 70],\n",
       " [4],\n",
       " [27, 28],\n",
       " [42, 43],\n",
       " [14, 15],\n",
       " [68],\n",
       " [5],\n",
       " [11, 12, 13],\n",
       " [1, 2, 3],\n",
       " [16],\n",
       " [53, 54]]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# random mask를 위해서 순서를 섞음\n",
    "random.shuffle(cand_idx)\n",
    "cand_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokens_org\n",
      "['[CLS]', '▁추적', '추', '적', '▁비가', '▁내리는', '▁날', '이었', '어', '▁그날', '은', '▁', '왠', '지', '▁손', '님이', '▁많아', '▁첫', '▁번에', '▁삼', '십', '▁전', '▁둘째', '번', '▁오', '십', '▁전', '▁오랜', '만에', '▁받아', '보는', '▁십', '▁전', '짜리', '▁백', '통', '화', '▁서', '푼', '에', '[SEP]', '▁손바닥', '▁위', '엔', '▁기쁨', '의', '▁눈', '물이', '▁흘러', '▁컬', '컬', '한', '▁목에', '▁모', '주', '▁한', '잔', '을', '▁적', '셔', '▁몇', '▁달', '▁포', '▁전부터', '▁콜', '록', '거리는', '▁아내', '▁생각에', '▁그', '토록', '▁먹고', '▁싶다', '던', '[SEP]'] \n",
      "\n",
      "tokens\n",
      "['[CLS]', '▁추적', '추', '적', '▁비가', '▁내리는', '▁날', '이었', '어', '▁그날', '은', '▁', '왠', '지', '▁손', '님이', '▁많아', '▁첫', '[MASK]', '▁삼', '십', '▁전', '▁둘째', '번', '[MASK]', '[MASK]', '▁전', '▁오랜', '만에', '▁받아', '보는', '▁그레이트', '▁전', '짜리', '▁백', '통', '화', '▁서', '푼', '에', '[SEP]', '▁손바닥', '▁위', '엔', '[MASK]', '[MASK]', '▁눈', '물이', '▁흘러', '[MASK]', '[MASK]', '[MASK]', '▁목에', '▁모', '주', '▁한', '잔', '을', '▁적', '셔', '▁몇', '▁달', '▁포', '▁전부터', '▁콜', '록', '거리는', '▁아내', '▁생각에', '▁그', '토록', '▁먹고', '▁싶다', '던', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "# tokens가 mask되므로 재 실행을 위해서 넣어줌 (테스트용)\n",
    "tokens = copy.deepcopy(tokens_org)\n",
    "\n",
    "mask_lms = []  # mask 된 값\n",
    "for index_set in cand_idx:\n",
    "    if len(mask_lms) >= mask_cnt:  # 핸재 mask된 개수가 15%를 넘으면 중지\n",
    "          break\n",
    "    if len(mask_lms) + len(index_set) > mask_cnt:  # 이번에 mask할 개수를 포함해 15%를 넘으면 skip\n",
    "          continue\n",
    "    dice = random.random()  # 0..1 사이의 확률 값\n",
    "\n",
    "    for index in index_set:\n",
    "        masked_token = None\n",
    "        if dice < 0.8:  # 80% replace with [MASK]\n",
    "            masked_token = \"[MASK]\"\n",
    "        elif dice < 0.9: # 10% keep original\n",
    "            masked_token = tokens[index]\n",
    "        else:  # 10% random word\n",
    "            masked_token = random.choice(vocab_list)\n",
    "        mask_lms.append({\"index\": index, \"label\": tokens[index]})\n",
    "        tokens[index] = masked_token\n",
    "\n",
    "print(\"tokens_org\")\n",
    "print(tokens_org, \"\\n\")\n",
    "print(\"tokens\")\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mask_idx   : [18, 24, 25, 31, 44, 45, 49, 50, 51, 63]\n",
      "mask_label : ['▁번에', '▁오', '십', '▁십', '▁기쁨', '의', '▁컬', '컬', '한', '▁전부터']\n"
     ]
    }
   ],
   "source": [
    "# 순서 정렬 및 mask_idx, mask_label 생성\n",
    "mask_lms = sorted(mask_lms, key=lambda x: x[\"index\"])\n",
    "mask_idx = [p[\"index\"] for p in mask_lms]\n",
    "mask_label = [p[\"label\"] for p in mask_lms]\n",
    "\n",
    "print(\"mask_idx   :\", mask_idx)\n",
    "print(\"mask_label :\", mask_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### create_pretrain_mask() : Masked LM을 위한 코퍼스 생성 메소드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_pretrain_mask(tokens, mask_cnt, vocab_list):\n",
    "    \"\"\"\n",
    "    마스크 생성\n",
    "    :param tokens: tokens\n",
    "    :param mask_cnt: mask 개수 (전체 tokens의 15%)\n",
    "    :param vocab_list: vocab list (random token 용)\n",
    "    :return tokens: mask된 tokens\n",
    "    :return mask_idx: mask된 token의 index\n",
    "    :return mask_label: mask된 token의 원래 값\n",
    "    \"\"\"\n",
    "    # 단어 단위로 mask 하기 위해서 index 분할\n",
    "    cand_idx = []  # word 단위의 index array\n",
    "    for (i, token) in enumerate(tokens):\n",
    "        if token == \"[CLS]\" or token == \"[SEP]\":\n",
    "            continue\n",
    "        if 0 < len(cand_idx) and not token.startswith(u\"\\u2581\"):\n",
    "            cand_idx[-1].append(i)\n",
    "        else:\n",
    "            cand_idx.append([i])\n",
    "    # random mask를 위해서 순서를 섞음\n",
    "    random.shuffle(cand_idx)\n",
    "\n",
    "    mask_lms = []  # mask 된 값\n",
    "    for index_set in cand_idx:\n",
    "        if len(mask_lms) >= mask_cnt:  # 핸재 mask된 개수가 15%를 넘으면 중지\n",
    "            break\n",
    "        if len(mask_lms) + len(index_set) > mask_cnt:  # 이번에 mask할 개수를 포함해 15%를 넘으면 skip\n",
    "            continue\n",
    "        dice = random.random()  # 0..1 사이의 확률 값\n",
    "        for index in index_set:\n",
    "            masked_token = None\n",
    "            if dice < 0.8:  # 80% replace with [MASK]\n",
    "                masked_token = \"[MASK]\"\n",
    "            elif dice < 0.9: # 10% keep original\n",
    "                masked_token = tokens[index]\n",
    "            else:  # 10% random word\n",
    "                masked_token = random.choice(vocab_list)\n",
    "            mask_lms.append({\"index\": index, \"label\": tokens[index]})\n",
    "            tokens[index] = masked_token\n",
    "    # mask_lms 정렬 후 mask_idx, mask_label 추출\n",
    "    mask_lms = sorted(mask_lms, key=lambda x: x[\"index\"])\n",
    "    mask_idx = [p[\"index\"] for p in mask_lms]  # mask된 token의 index\n",
    "    mask_label = [p[\"label\"] for p in mask_lms]  # mask된 token의 원래 값\n",
    "\n",
    "    return tokens, mask_idx, mask_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokens_org\n",
      "['[CLS]', '▁추적', '추', '적', '▁비가', '▁내리는', '▁날', '이었', '어', '▁그날', '은', '▁', '왠', '지', '▁손', '님이', '▁많아', '▁첫', '▁번에', '▁삼', '십', '▁전', '▁둘째', '번', '▁오', '십', '▁전', '▁오랜', '만에', '▁받아', '보는', '▁십', '▁전', '짜리', '▁백', '통', '화', '▁서', '푼', '에', '[SEP]', '▁손바닥', '▁위', '엔', '▁기쁨', '의', '▁눈', '물이', '▁흘러', '▁컬', '컬', '한', '▁목에', '▁모', '주', '▁한', '잔', '을', '▁적', '셔', '▁몇', '▁달', '▁포', '▁전부터', '▁콜', '록', '거리는', '▁아내', '▁생각에', '▁그', '토록', '▁먹고', '▁싶다', '던', '[SEP]'] \n",
      "\n",
      "tokens\n",
      "['[CLS]', '▁추적', '추', '적', '▁비가', '▁내리는', '▁날', '이었', '어', '▁그날', '은', '▁', '왠', '지', '▁손', '님이', '[MASK]', '[MASK]', '▁번에', '▁삼', '십', '▁전', '▁둘째', '번', '▁오', '십', '▁전', '▁오랜', '만에', '▁받아', '보는', '▁십', '▁전', '짜리', '▁백', '통', '화', '[MASK]', '[MASK]', '[MASK]', '[SEP]', '▁손바닥', '▁위', '엔', '▁기쁨', '의', '▁눈', '물이', '[MASK]', '▁컬', '컬', '한', '[MASK]', '끊', '▁규장', '▁한', '잔', '을', '▁적', '셔', '▁몇', '▁달', '▁포', '▁전부터', '▁콜', '록', '거리는', '[MASK]', '▁생각에', '▁그', '토록', '▁먹고', '▁싶다', '던', '[SEP]'] \n",
      "\n",
      "mask_idx   : [16, 17, 37, 38, 39, 48, 52, 53, 54, 67]\n",
      "mask_label : ['▁많아', '▁첫', '▁서', '푼', '에', '▁흘러', '▁목에', '▁모', '주', '▁아내']\n"
     ]
    }
   ],
   "source": [
    "# tokens가 mask되므로 재 실행을 위해서 넣어줌 (테스트용)\n",
    "tokens = copy.deepcopy(tokens_org)\n",
    "\n",
    "tokens, mask_idx, mask_label = create_pretrain_mask(tokens, mask_cnt, vocab_list)\n",
    "\n",
    "print(\"tokens_org\")\n",
    "print(tokens_org, \"\\n\")\n",
    "print(\"tokens\")\n",
    "print(tokens, \"\\n\")\n",
    "\n",
    "print(\"mask_idx   :\", mask_idx)\n",
    "print(\"mask_label :\", mask_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 데이터 전처리 (2) NSP pair 생성\n",
    "- BERT Pretrain task => Next Sentence Prediction (NSP)\n",
    "    - 문장 2개를 붙여 놓고 두 문장이 이어지는 것인지 아닌지 문장 호응관계를 맞출 수 있게 하는것\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "string = \"\"\"추적추적 비가 내리는 날이었어\n",
    "그날은 왠지 손님이 많아\n",
    "첫 번에 삼십 전 둘째 번 오십 전\n",
    "오랜만에 받아보는 십 전짜리 백통화 서푼에\n",
    "손바닥 위엔 기쁨의 눈물이 흘러\n",
    "컬컬한 목에 모주 한잔을 적셔\n",
    "몇 달 포 전부터 콜록거리는 아내\n",
    "생각에 그토록 먹고 싶다던\n",
    "설렁탕 한 그릇을 이제는 살 수 있어\n",
    "집으로 돌아가는 길 난 문득 떠올라\n",
    "아내의 목소리가 거칠어만 가는 희박한 숨소리가\n",
    "오늘은 왠지 나가지 말라던 내 옆에 있어 달라던\n",
    "그리도 나가고 싶으면 일찍이라도 들어와 달라던\n",
    "아내의 간절한 목소리가 들려와\n",
    "나를 원망하듯 비는 점점 거세져\n",
    "싸늘히 식어가는 아내가 떠올라 걱정은 더해져\n",
    "난 몰라 오늘은 운수 좋은 날\n",
    "난 맨날 이렇게 살 수 있으면 얼마나 좋을까\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['▁추적', '추', '적', '▁비가', '▁내리는', '▁날', '이었', '어'],\n",
       " ['▁그날', '은', '▁', '왠', '지', '▁손', '님이', '▁많아'],\n",
       " ['▁첫', '▁번에', '▁삼', '십', '▁전', '▁둘째', '▁번', '▁오', '십', '▁전']]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 줄 단위로 tokenize\n",
    "doc = [vocab.encode_as_pieces(line) for line in string.split(\"\\n\")]\n",
    "doc[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 최대 길이\n",
    "n_test_seq = 64\n",
    "# 최소 길이\n",
    "min_seq = 8\n",
    "# [CLS], tokens_a, [SEB], tokens_b, [SEP]\n",
    "max_seq = n_test_seq - 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current_chunk: 7 66 [['▁추적', '추', '적', '▁비가', '▁내리는', '▁날', '이었', '어'], ['▁그날', '은', '▁', '왠', '지', '▁손', '님이', '▁많아'], ['▁첫', '▁번에', '▁삼', '십', '▁전', '▁둘째', '▁번', '▁오', '십', '▁전'], ['▁오랜', '만에', '▁받아', '보는', '▁십', '▁전', '짜리', '▁백', '통', '화', '▁서', '푼', '에'], ['▁손바닥', '▁위', '엔', '▁기쁨', '의', '▁눈', '물이', '▁흘러'], ['▁컬', '컬', '한', '▁목에', '▁모', '주', '▁한', '잔', '을', '▁적', '셔'], ['▁몇', '▁달', '▁포', '▁전부터', '▁콜', '록', '거리는', '▁아내']]\n",
      "tokens_a: 16 ['▁추적', '추', '적', '▁비가', '▁내리는', '▁날', '이었', '어', '▁그날', '은', '▁', '왠', '지', '▁손', '님이', '▁많아']\n",
      "tokens_b: 50 ['▁첫', '▁번에', '▁삼', '십', '▁전', '▁둘째', '▁번', '▁오', '십', '▁전', '▁오랜', '만에', '▁받아', '보는', '▁십', '▁전', '짜리', '▁백', '통', '화', '▁서', '푼', '에', '▁손바닥', '▁위', '엔', '▁기쁨', '의', '▁눈', '물이', '▁흘러', '▁컬', '컬', '한', '▁목에', '▁모', '주', '▁한', '잔', '을', '▁적', '셔', '▁몇', '▁달', '▁포', '▁전부터', '▁콜', '록', '거리는', '▁아내']\n",
      "\n",
      "current_chunk: 7 65 [['▁생각에', '▁그', '토록', '▁먹고', '▁싶다', '던'], ['▁설', '렁', '탕', '▁한', '▁그릇', '을', '▁이제는', '▁살', '▁수', '▁있어'], ['▁집으로', '▁돌아가는', '▁길', '▁난', '▁문', '득', '▁떠올', '라'], ['▁아내의', '▁목소리가', '▁거칠', '어', '만', '▁가는', '▁희', '박한', '▁숨', '소', '리가'], ['▁오늘', '은', '▁', '왠', '지', '▁나가지', '▁말라', '던', '▁내', '▁옆에', '▁있어', '▁달라', '던'], ['▁그리', '도', '▁나가', '고', '▁싶', '으면', '▁일찍', '이라도', '▁들어와', '▁달라', '던'], ['▁아내의', '▁간', '절한', '▁목소리가', '▁들려', '와']]\n",
      "tokens_a: 35 ['▁생각에', '▁그', '토록', '▁먹고', '▁싶다', '던', '▁설', '렁', '탕', '▁한', '▁그릇', '을', '▁이제는', '▁살', '▁수', '▁있어', '▁집으로', '▁돌아가는', '▁길', '▁난', '▁문', '득', '▁떠올', '라', '▁아내의', '▁목소리가', '▁거칠', '어', '만', '▁가는', '▁희', '박한', '▁숨', '소', '리가']\n",
      "tokens_b: 30 ['▁오늘', '은', '▁', '왠', '지', '▁나가지', '▁말라', '던', '▁내', '▁옆에', '▁있어', '▁달라', '던', '▁그리', '도', '▁나가', '고', '▁싶', '으면', '▁일찍', '이라도', '▁들어와', '▁달라', '던', '▁아내의', '▁간', '절한', '▁목소리가', '▁들려', '와']\n",
      "\n",
      "current_chunk: 4 41 [['▁나를', '▁원', '망', '하', '듯', '▁비는', '▁점점', '▁거세', '져'], ['▁싸', '늘', '히', '▁식', '어', '가는', '▁아내가', '▁떠올', '라', '▁걱', '정은', '▁더', '해져'], ['▁난', '▁몰', '라', '▁오늘', '은', '▁운수', '▁좋은', '▁날'], ['▁난', '▁맨', '날', '▁이렇게', '▁살', '▁수', '▁있으면', '▁얼마나', '▁좋', '을', '까']]\n",
      "tokens_a: 30 ['▁나를', '▁원', '망', '하', '듯', '▁비는', '▁점점', '▁거세', '져', '▁싸', '늘', '히', '▁식', '어', '가는', '▁아내가', '▁떠올', '라', '▁걱', '정은', '▁더', '해져', '▁난', '▁몰', '라', '▁오늘', '은', '▁운수', '▁좋은', '▁날']\n",
      "tokens_b: 11 ['▁난', '▁맨', '날', '▁이렇게', '▁살', '▁수', '▁있으면', '▁얼마나', '▁좋', '을', '까']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "current_chunk = []  # line 단위 tokens\n",
    "current_length = 0\n",
    "for i in range(len(doc)):  # doc 전체를 loop\n",
    "    current_chunk.append(doc[i])  # line 단위로 추가\n",
    "    current_length += len(doc[i])  # current_chunk의 token 수\n",
    "    if 1 < len(current_chunk) and (i == len(doc) - 1 or current_length >= max_seq):  # 마지막 줄 이거나 길이가 max_seq 이상 인 경우\n",
    "        print(\"current_chunk:\", len(current_chunk), current_length, current_chunk)\n",
    "\n",
    "        #######################################\n",
    "        # token a\n",
    "        a_end = 1\n",
    "        if 1 < len(current_chunk):\n",
    "            a_end = random.randrange(1, len(current_chunk))\n",
    "        tokens_a = []\n",
    "        for j in range(a_end):\n",
    "            tokens_a.extend(current_chunk[j])\n",
    "        # token b\n",
    "        tokens_b = []\n",
    "        for j in range(a_end, len(current_chunk)):\n",
    "            tokens_b.extend(current_chunk[j])\n",
    "          \n",
    "        print(\"tokens_a:\", len(tokens_a), tokens_a)\n",
    "        print(\"tokens_b:\", len(tokens_b), tokens_b)\n",
    "        #######################################\n",
    "        print()\n",
    "\n",
    "        current_chunk = []\n",
    "        current_length = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 두 문장의 최대 길이 유지 하며, trim 적용 -> 50 % 확률로 True/False 케이스 생성\n",
    "def trim_tokens(tokens_a, tokens_b, max_seq):\n",
    "    \"\"\"\n",
    "    tokens_a, tokens_b의 길이를 줄임 최대 길이: max_seq\n",
    "    :param tokens_a: tokens A\n",
    "    :param tokens_b: tokens B\n",
    "    :param max_seq: 두 tokens 길이의 최대 값\n",
    "    \"\"\"\n",
    "    while True:\n",
    "        total_length = len(tokens_a) + len(tokens_b)\n",
    "        if total_length <= max_seq:\n",
    "            break\n",
    "\n",
    "        if len(tokens_a) > len(tokens_b):\n",
    "            del tokens_a[0]\n",
    "        else:\n",
    "            tokens_b.pop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current_chunk: 7 66 [['▁추적', '추', '적', '▁비가', '▁내리는', '▁날', '이었', '어'], ['▁그날', '은', '▁', '왠', '지', '▁손', '님이', '▁많아'], ['▁첫', '▁번에', '▁삼', '십', '▁전', '▁둘째', '▁번', '▁오', '십', '▁전'], ['▁오랜', '만에', '▁받아', '보는', '▁십', '▁전', '짜리', '▁백', '통', '화', '▁서', '푼', '에'], ['▁손바닥', '▁위', '엔', '▁기쁨', '의', '▁눈', '물이', '▁흘러'], ['▁컬', '컬', '한', '▁목에', '▁모', '주', '▁한', '잔', '을', '▁적', '셔'], ['▁몇', '▁달', '▁포', '▁전부터', '▁콜', '록', '거리는', '▁아내']]\n",
      "is_next: 1\n",
      "tokens_a: 42 ['▁날', '이었', '어', '▁그날', '은', '▁', '왠', '지', '▁손', '님이', '▁많아', '▁첫', '▁번에', '▁삼', '십', '▁전', '▁둘째', '▁번', '▁오', '십', '▁전', '▁오랜', '만에', '▁받아', '보는', '▁십', '▁전', '짜리', '▁백', '통', '화', '▁서', '푼', '에', '▁손바닥', '▁위', '엔', '▁기쁨', '의', '▁눈', '물이', '▁흘러']\n",
      "tokens_b: 19 ['▁컬', '컬', '한', '▁목에', '▁모', '주', '▁한', '잔', '을', '▁적', '셔', '▁몇', '▁달', '▁포', '▁전부터', '▁콜', '록', '거리는', '▁아내']\n",
      "\n",
      "current_chunk: 7 65 [['▁생각에', '▁그', '토록', '▁먹고', '▁싶다', '던'], ['▁설', '렁', '탕', '▁한', '▁그릇', '을', '▁이제는', '▁살', '▁수', '▁있어'], ['▁집으로', '▁돌아가는', '▁길', '▁난', '▁문', '득', '▁떠올', '라'], ['▁아내의', '▁목소리가', '▁거칠', '어', '만', '▁가는', '▁희', '박한', '▁숨', '소', '리가'], ['▁오늘', '은', '▁', '왠', '지', '▁나가지', '▁말라', '던', '▁내', '▁옆에', '▁있어', '▁달라', '던'], ['▁그리', '도', '▁나가', '고', '▁싶', '으면', '▁일찍', '이라도', '▁들어와', '▁달라', '던'], ['▁아내의', '▁간', '절한', '▁목소리가', '▁들려', '와']]\n",
      "is_next: 0\n",
      "tokens_a: 37 ['만', '▁가는', '▁희', '박한', '▁숨', '소', '리가', '▁오늘', '은', '▁', '왠', '지', '▁나가지', '▁말라', '던', '▁내', '▁옆에', '▁있어', '▁달라', '던', '▁그리', '도', '▁나가', '고', '▁싶', '으면', '▁일찍', '이라도', '▁들어와', '▁달라', '던', '▁아내의', '▁간', '절한', '▁목소리가', '▁들려', '와']\n",
      "tokens_b: 24 ['▁생각에', '▁그', '토록', '▁먹고', '▁싶다', '던', '▁설', '렁', '탕', '▁한', '▁그릇', '을', '▁이제는', '▁살', '▁수', '▁있어', '▁집으로', '▁돌아가는', '▁길', '▁난', '▁문', '득', '▁떠올', '라']\n",
      "\n",
      "current_chunk: 4 41 [['▁나를', '▁원', '망', '하', '듯', '▁비는', '▁점점', '▁거세', '져'], ['▁싸', '늘', '히', '▁식', '어', '가는', '▁아내가', '▁떠올', '라', '▁걱', '정은', '▁더', '해져'], ['▁난', '▁몰', '라', '▁오늘', '은', '▁운수', '▁좋은', '▁날'], ['▁난', '▁맨', '날', '▁이렇게', '▁살', '▁수', '▁있으면', '▁얼마나', '▁좋', '을', '까']]\n",
      "is_next: 0\n",
      "tokens_a: 32 ['▁싸', '늘', '히', '▁식', '어', '가는', '▁아내가', '▁떠올', '라', '▁걱', '정은', '▁더', '해져', '▁난', '▁몰', '라', '▁오늘', '은', '▁운수', '▁좋은', '▁날', '▁난', '▁맨', '날', '▁이렇게', '▁살', '▁수', '▁있으면', '▁얼마나', '▁좋', '을', '까']\n",
      "tokens_b: 9 ['▁나를', '▁원', '망', '하', '듯', '▁비는', '▁점점', '▁거세', '져']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "current_chunk = []  # line 단위 tokens\n",
    "current_length = 0\n",
    "for i in range(len(doc)):  # doc 전체를 loop\n",
    "    current_chunk.append(doc[i])  # line 단위로 추가\n",
    "    current_length += len(doc[i])  # current_chunk의 token 수\n",
    "    if 1 < len(current_chunk) and (i == len(doc) - 1 or current_length >= max_seq):  # 마지막 줄 이거나 길이가 max_seq 이상 인 경우\n",
    "        print(\"current_chunk:\", len(current_chunk), current_length, current_chunk)\n",
    "\n",
    "        # token a\n",
    "        a_end = 1\n",
    "        if 1 < len(current_chunk):\n",
    "            a_end = random.randrange(1, len(current_chunk))\n",
    "        tokens_a = []\n",
    "        for j in range(a_end):\n",
    "            tokens_a.extend(current_chunk[j])\n",
    "        # token b\n",
    "        tokens_b = []\n",
    "        for j in range(a_end, len(current_chunk)):\n",
    "            tokens_b.extend(current_chunk[j])\n",
    "\n",
    "        #######################################\n",
    "        if random.random() < 0.5:  # 50% 확률로 swap\n",
    "            is_next = 0\n",
    "            tokens_t = tokens_a\n",
    "            tokens_a = tokens_b\n",
    "            tokens_b = tokens_t\n",
    "        else:\n",
    "            is_next = 1\n",
    "        # max_seq 보다 큰 경우 길이 조절\n",
    "        trim_tokens(tokens_a, tokens_b, max_seq)\n",
    "        assert 0 < len(tokens_a)\n",
    "        assert 0 < len(tokens_b)\n",
    "\n",
    "        print(\"is_next:\", is_next)\n",
    "        print(\"tokens_a:\", len(tokens_a), tokens_a)\n",
    "        print(\"tokens_b:\", len(tokens_b), tokens_b)\n",
    "        #######################################\n",
    "        print()\n",
    "\n",
    "        current_chunk = []\n",
    "        current_length = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current_chunk: 7 66 [['▁추적', '추', '적', '▁비가', '▁내리는', '▁날', '이었', '어'], ['▁그날', '은', '▁', '왠', '지', '▁손', '님이', '▁많아'], ['▁첫', '▁번에', '▁삼', '십', '▁전', '▁둘째', '▁번', '▁오', '십', '▁전'], ['▁오랜', '만에', '▁받아', '보는', '▁십', '▁전', '짜리', '▁백', '통', '화', '▁서', '푼', '에'], ['▁손바닥', '▁위', '엔', '▁기쁨', '의', '▁눈', '물이', '▁흘러'], ['▁컬', '컬', '한', '▁목에', '▁모', '주', '▁한', '잔', '을', '▁적', '셔'], ['▁몇', '▁달', '▁포', '▁전부터', '▁콜', '록', '거리는', '▁아내']]\n",
      "is_next: 1\n",
      "tokens_a: 8 ['▁추적', '추', '적', '▁비가', '▁내리는', '▁날', '이었', '어']\n",
      "tokens_b: 53 ['▁그날', '은', '▁', '왠', '지', '▁손', '님이', '▁많아', '▁첫', '▁번에', '▁삼', '십', '▁전', '▁둘째', '▁번', '▁오', '십', '▁전', '▁오랜', '만에', '▁받아', '보는', '▁십', '▁전', '짜리', '▁백', '통', '화', '▁서', '푼', '에', '▁손바닥', '▁위', '엔', '▁기쁨', '의', '▁눈', '물이', '▁흘러', '▁컬', '컬', '한', '▁목에', '▁모', '주', '▁한', '잔', '을', '▁적', '셔', '▁몇', '▁달', '▁포']\n",
      "tokens: 64 ['[CLS]', '▁추적', '추', '적', '▁비가', '▁내리는', '▁날', '이었', '어', '[SEP]', '▁그날', '은', '▁', '왠', '지', '▁손', '님이', '▁많아', '▁첫', '▁번에', '▁삼', '십', '▁전', '▁둘째', '▁번', '▁오', '십', '▁전', '▁오랜', '만에', '▁받아', '보는', '▁십', '▁전', '짜리', '▁백', '통', '화', '▁서', '푼', '에', '▁손바닥', '▁위', '엔', '▁기쁨', '의', '▁눈', '물이', '▁흘러', '▁컬', '컬', '한', '▁목에', '▁모', '주', '▁한', '잔', '을', '▁적', '셔', '▁몇', '▁달', '▁포', '[SEP]']\n",
      "segment: 64 [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "masked tokens: 64 ['[CLS]', '▁추적', '추', '적', '▁비가', '▁내리는', '▁날', '이었', '어', '[SEP]', '▁그날', '은', '▁', '왠', '지', '▁손', '님이', '▁방식으로', '▁첫', '[MASK]', '▁삼', '십', '▁전', '▁둘째', '[MASK]', '▁오', '십', '▁전', '▁오랜', '만에', '▁받아', '보는', '▁십', '▁전', '짜리', '▁백', '통', '화', '▁서', '푼', '에', '▁손바닥', '▁위', '엔', '▁기쁨', '의', '▁눈', '물이', '▁흘러', '▁컬', '컬', '한', '[MASK]', '▁모', '주', '▁한', '잔', '을', '▁적', '셔', '[MASK]', '▁달', '▁포', '[SEP]']\n",
      "masked index: 9 [17, 19, 24, 27, 52, 55, 56, 57, 60]\n",
      "masked label: 9 ['▁많아', '▁번에', '▁번', '▁전', '▁목에', '▁한', '잔', '을', '▁몇']\n",
      "\n",
      "current_chunk: 7 65 [['▁생각에', '▁그', '토록', '▁먹고', '▁싶다', '던'], ['▁설', '렁', '탕', '▁한', '▁그릇', '을', '▁이제는', '▁살', '▁수', '▁있어'], ['▁집으로', '▁돌아가는', '▁길', '▁난', '▁문', '득', '▁떠올', '라'], ['▁아내의', '▁목소리가', '▁거칠', '어', '만', '▁가는', '▁희', '박한', '▁숨', '소', '리가'], ['▁오늘', '은', '▁', '왠', '지', '▁나가지', '▁말라', '던', '▁내', '▁옆에', '▁있어', '▁달라', '던'], ['▁그리', '도', '▁나가', '고', '▁싶', '으면', '▁일찍', '이라도', '▁들어와', '▁달라', '던'], ['▁아내의', '▁간', '절한', '▁목소리가', '▁들려', '와']]\n",
      "is_next: 1\n",
      "tokens_a: 16 ['▁생각에', '▁그', '토록', '▁먹고', '▁싶다', '던', '▁설', '렁', '탕', '▁한', '▁그릇', '을', '▁이제는', '▁살', '▁수', '▁있어']\n",
      "tokens_b: 45 ['▁집으로', '▁돌아가는', '▁길', '▁난', '▁문', '득', '▁떠올', '라', '▁아내의', '▁목소리가', '▁거칠', '어', '만', '▁가는', '▁희', '박한', '▁숨', '소', '리가', '▁오늘', '은', '▁', '왠', '지', '▁나가지', '▁말라', '던', '▁내', '▁옆에', '▁있어', '▁달라', '던', '▁그리', '도', '▁나가', '고', '▁싶', '으면', '▁일찍', '이라도', '▁들어와', '▁달라', '던', '▁아내의', '▁간']\n",
      "tokens: 64 ['[CLS]', '▁생각에', '▁그', '토록', '▁먹고', '▁싶다', '던', '▁설', '렁', '탕', '▁한', '▁그릇', '을', '▁이제는', '▁살', '▁수', '▁있어', '[SEP]', '▁집으로', '▁돌아가는', '▁길', '▁난', '▁문', '득', '▁떠올', '라', '▁아내의', '▁목소리가', '▁거칠', '어', '만', '▁가는', '▁희', '박한', '▁숨', '소', '리가', '▁오늘', '은', '▁', '왠', '지', '▁나가지', '▁말라', '던', '▁내', '▁옆에', '▁있어', '▁달라', '던', '▁그리', '도', '▁나가', '고', '▁싶', '으면', '▁일찍', '이라도', '▁들어와', '▁달라', '던', '▁아내의', '▁간', '[SEP]']\n",
      "segment: 64 [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "masked tokens: 64 ['[CLS]', '▁생각에', '▁그', '토록', '▁먹고', '▁싶다', '던', '▁설', '렁', '탕', '▁한', '[MASK]', '[MASK]', '욜', '▁살', '▁수', '▁있어', '[SEP]', '▁집으로', '▁돌아가는', '▁길', '▁난', '▁문', '득', '▁어머니와', '▁이와테', '▁아내의', '▁목소리가', '▁거칠', '어', '만', '▁하시', '▁희', '박한', '▁숨', '소', '리가', '▁오늘', '은', '▁', '왠', '지', '▁나가지', '▁말라', '던', '▁내', '▁옆에', '▁있어', '▁달라', '던', '▁그리', '도', '[MASK]', '[MASK]', '▁싶', '으면', '▁일찍', '이라도', '▁들어와', '▁달라', '던', '[MASK]', '▁간', '[SEP]']\n",
      "masked index: 9 [11, 12, 13, 24, 25, 31, 52, 53, 61]\n",
      "masked label: 9 ['▁그릇', '을', '▁이제는', '▁떠올', '라', '▁가는', '▁나가', '고', '▁아내의']\n",
      "\n",
      "current_chunk: 4 41 [['▁나를', '▁원', '망', '하', '듯', '▁비는', '▁점점', '▁거세', '져'], ['▁싸', '늘', '히', '▁식', '어', '가는', '▁아내가', '▁떠올', '라', '▁걱', '정은', '▁더', '해져'], ['▁난', '▁몰', '라', '▁오늘', '은', '▁운수', '▁좋은', '▁날'], ['▁난', '▁맨', '날', '▁이렇게', '▁살', '▁수', '▁있으면', '▁얼마나', '▁좋', '을', '까']]\n",
      "is_next: 0\n",
      "tokens_a: 11 ['▁난', '▁맨', '날', '▁이렇게', '▁살', '▁수', '▁있으면', '▁얼마나', '▁좋', '을', '까']\n",
      "tokens_b: 30 ['▁나를', '▁원', '망', '하', '듯', '▁비는', '▁점점', '▁거세', '져', '▁싸', '늘', '히', '▁식', '어', '가는', '▁아내가', '▁떠올', '라', '▁걱', '정은', '▁더', '해져', '▁난', '▁몰', '라', '▁오늘', '은', '▁운수', '▁좋은', '▁날']\n",
      "tokens: 44 ['[CLS]', '▁난', '▁맨', '날', '▁이렇게', '▁살', '▁수', '▁있으면', '▁얼마나', '▁좋', '을', '까', '[SEP]', '▁나를', '▁원', '망', '하', '듯', '▁비는', '▁점점', '▁거세', '져', '▁싸', '늘', '히', '▁식', '어', '가는', '▁아내가', '▁떠올', '라', '▁걱', '정은', '▁더', '해져', '▁난', '▁몰', '라', '▁오늘', '은', '▁운수', '▁좋은', '▁날', '[SEP]']\n",
      "segment: 44 [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "masked tokens: 44 ['[CLS]', '▁난', '▁맨', '날', '[MASK]', '▁살', '▁수', '▁있으면', '▁얼마나', '▁좋', '을', '까', '[SEP]', '▁나를', '▁원', '망', '하', '듯', '[MASK]', '▁점점', '▁거세', '져', '▁싸', '늘', '히', '▁식', '어', '가는', '[MASK]', '▁떠올', '라', '▁걱', '정은', '▁더', '해져', '▁난', '[MASK]', '[MASK]', '▁오늘', '은', '[MASK]', '▁좋은', '▁날', '[SEP]']\n",
      "masked index: 6 [4, 18, 28, 36, 37, 40]\n",
      "masked label: 6 ['▁이렇게', '▁비는', '▁아내가', '▁몰', '라', '▁운수']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 두 문장 사이 segment 처리\n",
    "# 첫 번째 문장의 segment 는 모두 0\n",
    "# 두 번째 문장의 segment 는 모두 1로 채운뒤\n",
    "# 둘사이에 구분자인 [SEP]등을 넣어 마무리\n",
    "instances = []\n",
    "current_chunk = []  # line 단위 tokens\n",
    "current_length = 0\n",
    "for i in range(len(doc)):  # doc 전체를 loop\n",
    "    current_chunk.append(doc[i])  # line 단위로 추가\n",
    "    current_length += len(doc[i])  # current_chunk의 token 수\n",
    "    if 1 < len(current_chunk) and (i == len(doc) - 1 or current_length >= max_seq):  # 마지막 줄 이거나 길이가 max_seq 이상 인 경우\n",
    "        print(\"current_chunk:\", len(current_chunk), current_length, current_chunk)\n",
    "\n",
    "        # token a\n",
    "        a_end = 1\n",
    "        if 1 < len(current_chunk):\n",
    "            a_end = random.randrange(1, len(current_chunk))\n",
    "        tokens_a = []\n",
    "        for j in range(a_end):\n",
    "            tokens_a.extend(current_chunk[j])\n",
    "        # token b\n",
    "        tokens_b = []\n",
    "        for j in range(a_end, len(current_chunk)):\n",
    "            tokens_b.extend(current_chunk[j])\n",
    "\n",
    "        if random.random() < 0.5:  # 50% 확률로 swap\n",
    "            is_next = 0\n",
    "            tokens_t = tokens_a\n",
    "            tokens_a = tokens_b\n",
    "            tokens_b = tokens_t\n",
    "        else:\n",
    "            is_next = 1\n",
    "        # max_seq 보다 큰 경우 길이 조절\n",
    "        trim_tokens(tokens_a, tokens_b, max_seq)\n",
    "        assert 0 < len(tokens_a)\n",
    "        assert 0 < len(tokens_b)\n",
    "\n",
    "        print(\"is_next:\", is_next)\n",
    "        print(\"tokens_a:\", len(tokens_a), tokens_a)\n",
    "        print(\"tokens_b:\", len(tokens_b), tokens_b)\n",
    "        #######################################\n",
    "        # tokens & aegment 생성\n",
    "        tokens = [\"[CLS]\"] + tokens_a + [\"[SEP]\"] + tokens_b + [\"[SEP]\"]\n",
    "        segment = [0] * (len(tokens_a) + 2) + [1] * (len(tokens_b) + 1)\n",
    "        print(\"tokens:\", len(tokens), tokens)\n",
    "        print(\"segment:\", len(segment), segment)\n",
    "        # mask\n",
    "        tokens, mask_idx, mask_label = create_pretrain_mask(tokens, int((len(tokens) - 3) * 0.15), vocab_list)\n",
    "        print(\"masked tokens:\", len(tokens), tokens)\n",
    "        print(\"masked index:\", len(mask_idx), mask_idx)\n",
    "        print(\"masked label:\", len(mask_label), mask_label)\n",
    "\n",
    "        instance = {\n",
    "            \"tokens\": tokens,\n",
    "            \"segment\": segment,\n",
    "            \"is_next\": is_next,\n",
    "            \"mask_idx\": mask_idx,\n",
    "            \"mask_label\": mask_label\n",
    "        }\n",
    "        instances.append(instance)\n",
    "        #######################################\n",
    "        print()\n",
    "\n",
    "        current_chunk = []\n",
    "        current_length = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'tokens': ['[CLS]', '▁추적', '추', '적', '▁비가', '▁내리는', '▁날', '이었', '어', '[SEP]', '▁그날', '은', '▁', '왠', '지', '▁손', '님이', '▁방식으로', '▁첫', '[MASK]', '▁삼', '십', '▁전', '▁둘째', '[MASK]', '▁오', '십', '▁전', '▁오랜', '만에', '▁받아', '보는', '▁십', '▁전', '짜리', '▁백', '통', '화', '▁서', '푼', '에', '▁손바닥', '▁위', '엔', '▁기쁨', '의', '▁눈', '물이', '▁흘러', '▁컬', '컬', '한', '[MASK]', '▁모', '주', '▁한', '잔', '을', '▁적', '셔', '[MASK]', '▁달', '▁포', '[SEP]'], 'segment': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'is_next': 1, 'mask_idx': [17, 19, 24, 27, 52, 55, 56, 57, 60], 'mask_label': ['▁많아', '▁번에', '▁번', '▁전', '▁목에', '▁한', '잔', '을', '▁몇']}\n",
      "{'tokens': ['[CLS]', '▁생각에', '▁그', '토록', '▁먹고', '▁싶다', '던', '▁설', '렁', '탕', '▁한', '[MASK]', '[MASK]', '욜', '▁살', '▁수', '▁있어', '[SEP]', '▁집으로', '▁돌아가는', '▁길', '▁난', '▁문', '득', '▁어머니와', '▁이와테', '▁아내의', '▁목소리가', '▁거칠', '어', '만', '▁하시', '▁희', '박한', '▁숨', '소', '리가', '▁오늘', '은', '▁', '왠', '지', '▁나가지', '▁말라', '던', '▁내', '▁옆에', '▁있어', '▁달라', '던', '▁그리', '도', '[MASK]', '[MASK]', '▁싶', '으면', '▁일찍', '이라도', '▁들어와', '▁달라', '던', '[MASK]', '▁간', '[SEP]'], 'segment': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'is_next': 1, 'mask_idx': [11, 12, 13, 24, 25, 31, 52, 53, 61], 'mask_label': ['▁그릇', '을', '▁이제는', '▁떠올', '라', '▁가는', '▁나가', '고', '▁아내의']}\n",
      "{'tokens': ['[CLS]', '▁난', '▁맨', '날', '[MASK]', '▁살', '▁수', '▁있으면', '▁얼마나', '▁좋', '을', '까', '[SEP]', '▁나를', '▁원', '망', '하', '듯', '[MASK]', '▁점점', '▁거세', '져', '▁싸', '늘', '히', '▁식', '어', '가는', '[MASK]', '▁떠올', '라', '▁걱', '정은', '▁더', '해져', '▁난', '[MASK]', '[MASK]', '▁오늘', '은', '[MASK]', '▁좋은', '▁날', '[SEP]'], 'segment': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'is_next': 0, 'mask_idx': [4, 18, 28, 36, 37, 40], 'mask_label': ['▁이렇게', '▁비는', '▁아내가', '▁몰', '라', '▁운수']}\n"
     ]
    }
   ],
   "source": [
    "# 최종 데이터셋 결과 확인\n",
    "for instance in instances:\n",
    "    print(instance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  create_pretrain_instances() : Next Sentence Prediction을 위한 코퍼스 생성 메소드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_pretrain_instances(vocab, doc, n_seq, mask_prob, vocab_list):\n",
    "    \"\"\"\n",
    "    doc별 pretrain 데이터 생성\n",
    "    \"\"\"\n",
    "    # for CLS], [SEP], [SEP]\n",
    "    max_seq = n_seq - 3\n",
    "\n",
    "    instances = []\n",
    "    current_chunk = []\n",
    "    current_length = 0\n",
    "    for i in range(len(doc)):\n",
    "        current_chunk.append(doc[i])  # line\n",
    "        current_length += len(doc[i])\n",
    "        if 1 < len(current_chunk) and (i == len(doc) - 1 or current_length >= max_seq):\n",
    "            # token a\n",
    "            a_end = 1\n",
    "            if 1 < len(current_chunk):\n",
    "                a_end = random.randrange(1, len(current_chunk))\n",
    "            tokens_a = []\n",
    "            for j in range(a_end):\n",
    "                tokens_a.extend(current_chunk[j])\n",
    "            # token b\n",
    "            tokens_b = []\n",
    "            for j in range(a_end, len(current_chunk)):\n",
    "                tokens_b.extend(current_chunk[j])\n",
    "\n",
    "            if random.random() < 0.5:  # 50% 확률로 swap\n",
    "                is_next = 0\n",
    "                tokens_t = tokens_a\n",
    "                tokens_a = tokens_b\n",
    "                tokens_b = tokens_t\n",
    "            else:\n",
    "                is_next = 1\n",
    "            # max_seq 보다 큰 경우 길이 조절\n",
    "            trim_tokens(tokens_a, tokens_b, max_seq)\n",
    "            assert 0 < len(tokens_a)\n",
    "            assert 0 < len(tokens_b)\n",
    "            # tokens & aegment 생성\n",
    "            tokens = [\"[CLS]\"] + tokens_a + [\"[SEP]\"] + tokens_b + [\"[SEP]\"]\n",
    "            segment = [0] * (len(tokens_a) + 2) + [1] * (len(tokens_b) + 1)\n",
    "            # mask\n",
    "            tokens, mask_idx, mask_label = create_pretrain_mask(tokens, int((len(tokens) - 3) * mask_prob), vocab_list)\n",
    "\n",
    "            instance = {\n",
    "                \"tokens\": tokens,\n",
    "                \"segment\": segment,\n",
    "                \"is_next\": is_next,\n",
    "                \"mask_idx\": mask_idx,\n",
    "                \"mask_label\": mask_label\n",
    "            }\n",
    "            instances.append(instance)\n",
    "\n",
    "            current_chunk = []\n",
    "            current_length = 0\n",
    "    return instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'tokens': ['[CLS]', '▁날', '이었', '어', '[MASK]', '[MASK]', '▁', '왠', '지', '▁손', '님이', '▁많아', '▁첫', '▁번에', '[MASK]', '[MASK]', '▁전', '▁둘째', '▁번', '▁오', '십', '▁전', '▁오랜', '만에', '▁받아', '보는', '▁십', '▁전', '짜리', '▁백', '통', '화', '▁서', '푼', '에', '[SEP]', '[MASK]', '▁위', '엔', '▁기쁨', '의', '▁눈', '물이', '▁흘러', '▁컬', '컬', '한', '▁목에', '▁모', '주', '[MASK]', '[MASK]', '[MASK]', '▁적', '셔', '▁몇', '부에는', '▁포', '▁전부터', '▁콜', '록', '거리는', '▁아내', '[SEP]'], 'segment': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'is_next': 1, 'mask_idx': [4, 5, 14, 15, 36, 50, 51, 52, 56], 'mask_label': ['▁그날', '은', '▁삼', '십', '▁손바닥', '▁한', '잔', '을', '▁달']}\n",
      "{'tokens': ['[CLS]', '만', '▁가는', '▁희', '박한', '▁숨', '소', '리가', '▁오늘', '은', '▁', '왠', '지', '▁나가지', '▁말라', '던', '▁내', '▁옆에', '▁있어', '▁달라', '던', '▁그리', '도', '▁나가', '고', '[MASK]', '[MASK]', '▁일찍', '이라도', '▁들어와', '껍', '▁무역을', '▁아내의', '▁간', '절한', '▁목소리가', '▁들려', '와', '[SEP]', '▁생각에', '▁그', '토록', '[MASK]', '▁싶다', '던', '▁설', '렁', '탕', '▁한', '[MASK]', '[MASK]', '[MASK]', '▁살', '▁수', '꾹', '▁집으로', '▁돌아가는', '▁길', '▁난', '▁문', '득', '▁떠올', '라', '[SEP]'], 'segment': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'is_next': 0, 'mask_idx': [25, 26, 30, 31, 42, 49, 50, 51, 54], 'mask_label': ['▁싶', '으면', '▁달라', '던', '▁먹고', '▁그릇', '을', '▁이제는', '▁있어']}\n",
      "{'tokens': ['[CLS]', '▁나를', '▁원', '망', '하', '듯', '▁비는', '[MASK]', '▁거세', '져', '▁싸', '늘', '히', '▁식', '어', '가는', '▁아내가', '▁떠올', '라', '[MASK]', '[MASK]', '▁더', '해져', '▁난', '▁몰', '라', '▁오늘', '은', '▁운수', '▁좋은', '▁날', '[SEP]', '▁난', '▁맨', '날', '▁이렇게', '▁살', '▁수', '▁있으면', '▁얼마나', '나는', '▁세기', '01', '[SEP]'], 'segment': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'is_next': 1, 'mask_idx': [7, 19, 20, 40, 41, 42], 'mask_label': ['▁점점', '▁걱', '정은', '▁좋', '을', '까']}\n"
     ]
    }
   ],
   "source": [
    "instances = create_pretrain_instances(vocab, doc, n_test_seq, 0.15, vocab_list)\n",
    "\n",
    "# 최종 데이터셋 결과 확인\n",
    "for instance in instances:\n",
    "    print(instance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 데이터 전처리 (3) 데이터셋 완성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3957761"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_file = os.getenv('HOME')+'/aiffel/bert_pretrain/data/kowiki.txt'\n",
    "\n",
    "# line count 확인\n",
    "total = 0\n",
    "with open(corpus_file, 'r') as in_f:\n",
    "    for line in in_f:\n",
    "        total += 1\n",
    "\n",
    "total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb47b22ae4d54e458392eec7d4adfd54",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3957761.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21 lines : ['▁지미', '▁카터']\n",
      "['▁제임스', '▁얼', '▁\"', '지', '미', '\"', '▁카터', '▁주니어', '(,', '▁1924', '년', '▁10', '월', '▁1', '일', '▁~', '▁)', '는', '▁민주당', '▁출신', '▁미국', '▁39', '번째', '▁대통령', '▁(19', '77', '년', '▁~', '▁1981', '년', ')', '이다', '.']\n",
      "['▁그는', '▁2002', '년', '▁말', '▁인권', '과', '▁중재', '▁역할에', '▁대한', '▁공로를', '▁인정받아', '▁노벨', '▁평화', '상을', '▁받게', '▁되었다', '.']\n",
      "\n",
      "14 lines : ['▁수학']\n",
      "['▁수학', '(', '數', '學', ',', '▁)', '은', '▁양', ',', '▁구조', ',', '▁공간', ',', '▁변화', ',', '▁미', '적', '분', '▁등의', '▁개념을', '▁다루는', '▁학문이다', '.', '▁현대', '▁수학', '은', '▁형식', '▁논', '리를', '▁이용해서', '▁공', '리로', '▁구성된', '▁추상', '적', '▁구조를', '▁연구하는', '▁학문', '으로', '▁여겨', '지기도', '▁한다', '.', '▁수학', '은', '▁그', '▁구조와', '▁발전', '▁과정', '에서는', '▁자연', '과학', '에', '▁속하는', '▁물리', '학을', '▁비롯한', '▁다른', '▁학문', '들과', '▁깊은', '▁연', '관을', '▁맺고', '▁있다', '.', '▁하지만', ',', '▁어느', '▁과학의', '▁분야', '들과는', '▁달리', ',', '▁자연', '계에서', '▁관측', '되지', '▁않는', '▁개념', '들에', '▁대해서', '까지', '▁이론을', '▁일반화', '▁및', '▁추상', '화', '시킬', '▁수', '▁있다는', '▁차이가', '▁있다고', '▁한다', '.', '▁수', '학자들은', '▁그러한', '▁개념', '들에', '▁대해서', '▁추측', '을', '▁하고', ',', '▁적절', '하게', '▁선택', '된', '▁정의', '와', '▁공리', '로부터의', '▁엄', '밀한', '▁연', '역을', '▁통해서', '▁추측', '들의', '▁진', '위를', '▁파악', '한다', '.']\n",
      "['▁수', '학의', '▁기초를', '▁확실히', '▁세우', '기', '▁위해', ',', '▁수리', '논', '리', '학과', '▁집합', '론이', '▁발전', '하였고', ',', '▁이와', '▁더불어', '▁범주', '론이', '▁최근', '에도', '▁발전', '되고', '▁있다', '.', '▁“', '근', '본', '▁위기', '”', '라는', '▁말은', '▁대략', '▁1900', '년에서', '▁1930', '년', '▁사이에', '▁일어난', ',', '▁수', '학의', '▁엄', '밀한', '▁기초', '에', '▁대한', '▁탐', '구를', '▁상징', '적으로', '▁보여주는', '▁말이다', '.', '▁수', '학의', '▁엄', '밀한', '▁기초', '에', '▁대한', '▁몇', '▁가지', '▁의견', '▁불', '일', '치는', '▁오늘날에도', '▁계속되고', '▁있다', '.', '▁수', '학의', '▁기초', '에', '▁대한', '▁위', '기는', '▁그', '▁당시', '▁수많은', '▁논쟁', '에', '▁의해', '▁촉발', '되었으며', ',', '▁그', '▁논쟁', '에는', '▁칸', '토', '어의', '▁집합', '론과', '▁브라우', '어', '-', '힐', '베르트', '▁논쟁이', '▁포함되었다', '.']\n",
      "\n",
      "4 lines : ['▁수학', '▁상수']\n",
      "['▁수학에서', '▁상수', '란', '▁그', '▁값이', '▁변하지', '▁않는', '▁불변', '량으로', ',', '▁변', '수의', '▁반대', '말', '이다', '.', '▁물리', '▁상수', '와는', '▁달리', ',', '▁수학', '▁상', '수는', '▁물리적', '▁측정', '과는', '▁상관없이', '▁정의된다', '.']\n",
      "['▁특정', '▁수학', '▁상수', ',', '▁예를', '▁들면', '▁골', '롬', '-', '딕', '맨', '▁상수', ',', '▁프랑', '세', '즈', '-', '로', '빈', '슨', '▁상수', ',', '▁formula', '_1', ',', '▁레', '비', '▁상수', '같은', '▁상', '수는', '▁다른', '▁수학', '상수', '▁또는', '▁함수', '와', '▁약한', '▁상관', '관계', '▁또는', '▁강한', '▁상관', '관계를', '▁갖는다', '.']\n",
      "\n",
      "10 lines : ['▁문학']\n",
      "['▁문학', '(', '文', '學', ')', '은', '▁언어를', '▁예술적', '▁표현의', '▁제', '재로', '▁삼아', '▁새로운', '▁의미를', '▁창출', '하여', ',', '▁인간과', '▁사회를', '▁진실', '되게', '▁묘사', '하는', '▁예술의', '▁하위', '분야', '이다', '.', '▁간단하게', '▁설명', '하면', ',', '▁언어를', '▁통해', '▁인간의', '▁삶을', '▁미', '적', '(', '美', '的', ')', '으로', '▁형상', '화한', '▁것이라고', '▁볼', '▁수', '▁있다', '.', '▁문학', '은', '▁원래', '▁문예', '(', '文', '藝', ')', '라고', '▁부르는', '▁것이', '▁옳', '으며', ',', '▁문학을', '▁학문의', '▁대상', '으로서', '▁탐구', '하는', '▁학문의', '▁명칭', '▁역시', '▁문예', '학', '이다', '.', '▁문예', '학은', '▁음악', '사', '학', ',', '▁미술', '사', '학', '▁등과', '▁함께', '▁예술', '학의', '▁핵심', '분야', '로서', '▁인문', '학의', '▁하위', '범', '주에', '▁포함된다', '.']\n",
      "['▁반영', '론적', '▁관', '점에', '▁의한', '▁감', '상은', '▁작품을', '▁창작', '된', '▁당시', '▁시대', '▁정', '황', '과', '▁연결', '시켜', '▁감상', '하는', '▁입장', '이고', ',', '▁내재', '적', '▁관', '점의', '▁감', '상은', '▁작품의', '▁형식', ',', '▁내용에', '▁국한', '하여', '▁감상', '하는', '▁것이다', '.', '▁표현', '론적', '▁관', '점의', '▁감', '상은', '▁작가의', '▁전기', '적', '▁사실과', '▁작품을', '▁연결', '시켜', '▁감상', '하는', '▁것이고', ',', '▁수용', '론적', '▁관', '점의', '▁감', '상은', '▁독', '자와', '▁작품을', '▁연결', '시켜', '▁감상', '하는', '▁것을', '▁말한다', '.']\n",
      "\n",
      "10 lines : ['▁나라', '▁목록']\n",
      "['▁이', '▁문서는', '▁나라', '▁목록', '이며', ',', '▁전', '▁세계', '▁20', '6', '개', '▁나라의', '▁각', '▁현황', '과', '▁주권', '▁승인', '▁정보를', '▁개', '요', '▁형태로', '▁나열', '하고', '▁있다', '.']\n",
      "['▁위', '▁목록에', '▁포함되지', '▁않은', '▁다음', '▁국가는', '▁몬테', '비', '데오', '▁협약', '의', '▁모든', '▁조건을', '▁만족', '하지', '▁못', '하거나', ',', '▁자주', '적이고', '▁독립', '적', '임을', '▁주장', '하지', '▁않는', '▁국가이다', '.']\n",
      "\n",
      "\n",
      "['▁화학']\n",
      "['▁화학', '(', '化', '學', ',', '▁)', '은', '▁물질의', '▁성질', ',', '▁조성', ',', '▁구조', ',', '▁변화', '▁및', '▁그에', '▁수반', '하는', '▁에너지의', '▁변화를', '▁연구하는', '▁자연과', '학의', '▁한', '▁분야이다', '.', '▁물리학', '도', '▁역시', '▁물질을', '▁다루는', '▁학문', '이지만', ',', '▁물리학', '이', '▁원', '소와', '▁화합', '물을', '▁모두', '▁포함한', '▁물체의', '▁운동과', '▁에너지', ',', '▁열', '적', '·', '전기', '적', '·', '광', '학적', '·', '기계', '적', '▁속', '성을', '▁다루고', '▁이러한', '▁현상', '으로부터', '▁통일된', '▁이론을', '▁구축', '하려는', '▁것과는', '▁달리', '▁화학', '에서는', '▁물질', '▁자체를', '▁연구', '▁대상으로', '▁한다', '.', '▁화학', '은', '▁이미', '▁존재하는', '▁물질을', '▁이용하여', '▁특정한', '▁목적에', '▁맞는', '▁새로운', '▁물질을', '▁합성', '하는', '▁길을', '▁제공하며', ',', '▁이는', '▁농작', '물의', '▁증', '산', ',', '▁질병의', '▁치료', '▁및', '▁예방', ',', '▁에너지', '▁효율', '▁증대', ',', '▁환경', '오', '염', '▁감소', '▁등', '▁여러', '▁가지', '▁이', '점을', '▁제공한다', '.']\n",
      "['▁유기', '화', '학은', '▁탄', '소로', '▁이루어진', '▁화합', '물을', '▁연구하는', '▁분', '과', '이다', '.', '▁원래', '▁유기', '▁화합', '물은', '▁식물', '이나', '▁동물', '로부터', '▁추출', '해', '낸', '▁화합', '물을', '▁뜻', '하였으나', '▁지금은', '▁유기', '▁화합', '물의', '▁범위가', '▁크게', '▁넓', '어져', '▁탄소', '▁사슬', '▁또는', '▁탄소', '▁고', '리를', '▁가진', '▁모든', '▁화합', '물을', '▁뜻한다', '.', '▁유기', '화', '학의', '▁오랜', '▁관심', '사는', '▁유기', '▁화합', '물의', '▁합성', '▁메커니즘', '이다', '.', '▁현대에', '▁들어서', '▁핵', '자기', '▁공명', '법과', '▁X', '선', '▁결정', '학', '▁등이', '▁개발되어', '▁유기', '▁화합물', '▁분석', '에', '▁있어서', '▁매우', '▁중요한', '▁방법으로', '▁자리잡았다', '.', '▁플라스틱', ',', '▁합성', '섬유', '등의', '▁고분', '자', '물질', '▁등도', '▁유기', '화', '학에서', '▁다루', '어진다', '.']\n"
     ]
    }
   ],
   "source": [
    "# 위키가 주제별로 잘 나눠지는지 여부 확인\n",
    "count = 5\n",
    "\n",
    "with open(corpus_file, 'r') as in_f:\n",
    "    doc = []  # 단락 단위로 문서 저장\n",
    "    for line in tqdm(in_f, total=total):\n",
    "        line = line.strip()\n",
    "        if line == \"\":  # line이 빈줄 일 경우 (새로운 단락을 의미 함)\n",
    "            if 0 < len(doc):\n",
    "                if 0 < count:\n",
    "                    count -= 1\n",
    "                    print(len(doc), \"lines :\", doc[0])\n",
    "                    print(doc[1])\n",
    "                    print(doc[-1])\n",
    "                    print()\n",
    "                else:\n",
    "                    break\n",
    "                doc = []\n",
    "        else:  # doc에 저장\n",
    "            pieces = vocab.encode_as_pieces(line)\n",
    "            if 0 < len(pieces):\n",
    "                doc.append(pieces)\n",
    "    if 0 < len(doc):  # 마지막에 처리되지 않은 doc가 있는 경우\n",
    "        print(doc[0])\n",
    "        print(doc[1])\n",
    "        print(doc[-1])\n",
    "        doc = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88daf480a6e045bdb0b8d253b426146c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3957761.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc: 21 instances: 10\n",
      "{'tokens': ['[CLS]', '▁X', '선', '[MASK]', '[MASK]', '▁등이', '▁개발되어', '[MASK]', '▁화합물', '▁분석', '에', '▁있어서', '▁매우', '▁중요한', '▁방법으로', '▁자리잡았다', '.', '▁플라스틱', ',', '▁합성', '섬유', '등의', '▁고분', '자', '물질', '[MASK]', '▁유기', '화', '학에서', '▁다루', '어진다', '.', '[SEP]', '▁유기', '화', '학은', '[MASK]', '[MASK]', '▁이루어진', '▁화합', '물을', '▁연구하는', '▁분', '과', '이다', '.', '▁원래', '▁유기', '▁화합', '물은', '▁식물', '이나', '▁동물', '로부터', '▁추출', '해', '낸', '▁화합', '물을', '▁뜻', '하였으나', '▁지금은', '▁유기', '[SEP]'], 'segment': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'is_next': 0, 'mask_idx': [3, 4, 7, 25, 26, 27, 28, 36, 37], 'mask_label': ['▁결정', '학', '▁유기', '▁등도', '▁유기', '화', '학에서', '▁탄', '소로']}\n",
      "{'tokens': ['[CLS]', '▁X', '선', '▁결정', '학', '▁등이', '▁개발되어', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '▁있어서', '▁매우', '▁중요한', '▁방법으로', '▁자리잡았다', '.', '[MASK]', '[MASK]', '▁합성', '섬유', '등의', '▁고분', '자', '물질', '▁등도', '▁유기', '화', '학에서', '▁다루', '어진다', '.', '[SEP]', '▁유기', '화', '학은', '▁탄', '소로', '▁이루어진', '▁화합', '물을', '▁연구하는', '▁분', '과', '이다', '.', '▁원래', '▁유기', '▁화합', '물은', '▁식물', '이나', '▁동물', '로부터', '▁추출', '해', '낸', '▁화합', '물을', '▁뜻', '하였으나', '▁지금은', '▁유기', '[SEP]'], 'segment': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'is_next': 0, 'mask_idx': [7, 8, 9, 10, 17, 18, 26, 27, 28], 'mask_label': ['▁유기', '▁화합물', '▁분석', '에', '▁플라스틱', ',', '▁유기', '화', '학에서']}\n",
      "\n",
      "doc: 14 instances: 7\n",
      "{'tokens': ['[CLS]', '▁X', '선', '▁결정', '학', '▁등이', '▁개발되어', '▁유기', '▁화합물', '▁분석', '에', '[MASK]', '[MASK]', '▁중요한', '▁방법으로', '▁자리잡았다', '.', '▁플라스틱', ',', '▁합성', '섬유', '등의', '▁고분', '자', '물질', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '▁다루', '어진다', '.', '[SEP]', '▁유기', '화', '학은', '▁탄', '소로', '▁이루어진', '▁화합', '물을', '[MASK]', '▁분', '과', '이다', '.', '▁원래', '▁유기', '▁화합', '물은', '▁식물', '이나', '▁동물', '로부터', '▁추출', '해', '낸', '▁화합', '물을', '[MASK]', '[MASK]', '▁지금은', '▁유기', '[SEP]'], 'segment': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'is_next': 1, 'mask_idx': [11, 12, 25, 26, 27, 28, 41, 59, 60], 'mask_label': ['▁있어서', '▁매우', '▁등도', '▁유기', '화', '학에서', '▁연구하는', '▁뜻', '하였으나']}\n",
      "{'tokens': ['[CLS]', '▁X', '선', '▁결정', '학', '▁등이', '[MASK]', '▁달린다', '▁화합물', '▁분석', '에', '▁있어서', '▁매우', '▁중요한', '[MASK]', '▁자리잡았다', '.', '▁플라스틱', ',', '▁합성', '섬유', '등의', '▁고분', '자', '물질', '▁등도', '▁유기', '화', '학에서', '▁다루', '어진다', '.', '[SEP]', '▁유기', '화', '학은', '▁탄', '소로', '▁이루어진', '▁화합', '물을', '▁연구하는', '▁분', '과', '이다', '.', '▁원래', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '▁동물', '로부터', '▁추출', '해', '낸', '▁화합', '물을', '▁뜻', '하였으나', '▁지금은', '[MASK]', '[SEP]'], 'segment': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'is_next': 1, 'mask_idx': [6, 7, 14, 47, 48, 49, 50, 51, 62], 'mask_label': ['▁개발되어', '▁유기', '▁방법으로', '▁유기', '▁화합', '물은', '▁식물', '이나', '▁유기']}\n",
      "\n",
      "doc: 4 instances: 2\n",
      "{'tokens': ['[CLS]', '▁X', '선', '▁결정', '학', '[MASK]', '[MASK]', '▁유기', '[MASK]', '▁분석', '에', '[MASK]', '▁매우', '▁중요한', '▁방법으로', '▁자리잡았다', '.', '▁플라스틱', ',', '▁합성', '섬유', '등의', '▁고분', '자', '물질', '▁등도', '▁유기', '화', '학에서', '▁다루', '어진다', '.', '[SEP]', '[MASK]', '[MASK]', '[MASK]', '▁탄', '소로', '▁이루어진', '▁화합', '물을', '▁연구하는', '▁분', '과', '이다', '.', '▁원래', '▁유기', '▁화합', '물은', '▁식물', '이나', '▁동물', '로부터', '▁추출', '해', '낸', '▁화합', '물을', '[MASK]', '[MASK]', '▁지금은', '▁유기', '[SEP]'], 'segment': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'is_next': 0, 'mask_idx': [5, 6, 8, 11, 33, 34, 35, 59, 60], 'mask_label': ['▁등이', '▁개발되어', '▁화합물', '▁있어서', '▁유기', '화', '학은', '▁뜻', '하였으나']}\n",
      "{'tokens': ['[CLS]', '▁X', '선', '▁결정', '학', '▁등이', '▁개발되어', '▁유기', '▁화합물', '▁분석', '에', '▁있어서', '▁매우', '▁중요한', '▁방법으로', '[MASK]', '[MASK]', '▁플라스틱', ',', '▁합성', '섬유', '등의', '▁고분', '자', '물질', '▁등도', '▁유기', '화', '학에서', '▁다루', '어진다', '.', '[SEP]', '▁유기', '화', '학은', '▁탄', '소로', '▁이루어진', '▁화합', '물을', '▁연구하는', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '▁원래', '[MASK]', '▁화합', '물은', '▁식물', '이나', '▁동물', '로부터', '▁추출', '해', '낸', '▁화합', '물을', '▁뜻', '하였으나', '▁지금은', '[MASK]', '[SEP]'], 'segment': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'is_next': 0, 'mask_idx': [15, 16, 25, 42, 43, 44, 45, 47, 62], 'mask_label': ['▁자리잡았다', '.', '▁등도', '▁분', '과', '이다', '.', '▁유기', '▁유기']}\n",
      "\n",
      "doc: 10 instances: 5\n",
      "{'tokens': ['[CLS]', '▁X', '선', '▁결정', '학', '▁등이', '▁개발되어', '[MASK]', '[MASK]', '▁분석', '에', '▁있어서', '▁매우', '▁중요한', '▁방법으로', '▁자리잡았다', '.', '▁플라스틱', ',', '▁합성', '섬유', '등의', '▁고분', '자', '물질', '▁등도', '▁유기', '화', '학에서', '▁다루', '어진다', '.', '[SEP]', '▁유기', '화', '학은', '▁탄', '소로', '▁이루어진', '▁화합', '물을', '▁연구하는', '▁분', '과', '이다', '.', '▁원래', '▁유기', '▁화합', '물은', '▁식물', '이나', '▁동물', '로부터', '▁추출', '해', '낸', '▁화합', '물을', '[MASK]', '[MASK]', '▁지금은', '▁유기', '[SEP]'], 'segment': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'is_next': 0, 'mask_idx': [7, 8, 39, 40, 52, 53, 59, 60, 62], 'mask_label': ['▁유기', '▁화합물', '▁화합', '물을', '▁동물', '로부터', '▁뜻', '하였으나', '▁유기']}\n",
      "{'tokens': ['[CLS]', '▁X', '선', '▁결정', '학', '▁등이', '▁개발되어', '▁유기', '[MASK]', '[MASK]', '[MASK]', '▁있어서', '▁매우', '▁중요한', '▁방법으로', '▁자리잡았다', '.', '▁플라스틱', ',', '▁합성', '섬유', '등의', '▁고분', '자', '물질', '[MASK]', '▁유기', '화', '학에서', '▁다루', '어진다', '.', '[SEP]', '▁유기', '화', '학은', '▁탄', '소로', '[MASK]', '▁화합', '물을', '▁연구하는', '▁분', '과', '이다', '.', '▁원래', '▁유기', '[MASK]', '[MASK]', '▁식물', '이나', '▁동물', '로부터', '▁추출', '해', '낸', '[MASK]', '[MASK]', '▁뜻', '하였으나', '▁지금은', '▁유기', '[SEP]'], 'segment': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'is_next': 0, 'mask_idx': [8, 9, 10, 25, 38, 48, 49, 57, 58], 'mask_label': ['▁화합물', '▁분석', '에', '▁등도', '▁이루어진', '▁화합', '물은', '▁화합', '물을']}\n",
      "\n",
      "doc: 10 instances: 5\n",
      "{'tokens': ['[CLS]', '▁X', '선', '▁결정', '학', '▁등이', '▁개발되어', '▁유기', '▁화합물', '▁분석', '에', '▁있어서', '▁매우', '[MASK]', '▁방법으로', '▁자리잡았다', '.', '▁플라스틱', ',', '[MASK]', '[MASK]', '[MASK]', '▁고분', '자', '물질', '[MASK]', '▁유기', '화', '학에서', '▁다루', '어진다', '.', '[SEP]', '▁유기', '화', '학은', '▁탄', '소로', '▁이루어진', '▁화합', '물을', '▁연구하는', '▁분', '과', '이다', '.', '▁원래', '▁유기', '▁화합', '물은', '▁식물', '이나', '▁동물', '로부터', '▁추출', '해', '낸', '▁화합', '물을', '▁뜻', '하였으나', '[MASK]', '▁유기', '[SEP]'], 'segment': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'is_next': 0, 'mask_idx': [13, 19, 20, 21, 22, 23, 24, 25, 61], 'mask_label': ['▁중요한', '▁합성', '섬유', '등의', '▁고분', '자', '물질', '▁등도', '▁지금은']}\n",
      "{'tokens': ['[CLS]', '▁X', '선', '▁결정', '학', '▁등이', '▁개발되어', '▁유기', '▁화합물', '▁분석', '에', '▁있어서', '▁매우', '▁중요한', '▁방법으로', '▁자리잡았다', '.', '▁플라스틱', ',', '▁합성', '섬유', '등의', '▁고분', '자', '물질', '▁등도', '▁유기', '화', '학에서', '▁다루', '어진다', '.', '[SEP]', '▁유기', '화', '학은', '▁탄', '소로', '▁이루어진', '▁화합', '물을', '▁연구하는', '▁분', '과', '이다', '.', '[MASK]', '▁유기', '[MASK]', '[MASK]', '▁식물', '이나', '▁동물', '로부터', '▁추출', '해', '낸', '▁화합', '물을', '▁뜻', '하였으나', '▁지금은', '▁유기', '[SEP]'], 'segment': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'is_next': 1, 'mask_idx': [15, 16, 42, 43, 44, 45, 46, 48, 49], 'mask_label': ['▁자리잡았다', '.', '▁분', '과', '이다', '.', '▁원래', '▁화합', '물은']}\n",
      "\n",
      "doc: 31 instances: 15\n",
      "{'tokens': ['[CLS]', '[MASK]', '[MASK]', '▁결정', '학', '▁등이', '▁개발되어', '▁유기', '▁화합물', '▁분석', '에', '▁있어서', '▁매우', '[MASK]', '▁방법으로', '▁자리잡았다', '.', '▁플라스틱', ',', '▁합성', '섬유', '등의', '▁고분', '자', '물질', '▁등도', '[MASK]', '[MASK]', '[MASK]', '▁다루', '어진다', '.', '[SEP]', '▁유기', '화', '학은', '▁탄', '소로', '▁이루어진', '▁화합', '물을', '▁연구하는', '▁분', '과', '이다', '.', '▁원래', '▁유기', '▁화합', '물은', '▁식물', '이나', '[MASK]', '[MASK]', '▁추출', '해', '낸', '▁화합', '물을', '▁뜻', '하였으나', '▁지금은', '▁유기', '[SEP]'], 'segment': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'is_next': 0, 'mask_idx': [1, 2, 11, 13, 26, 27, 28, 52, 53], 'mask_label': ['▁X', '선', '▁있어서', '▁중요한', '▁유기', '화', '학에서', '▁동물', '로부터']}\n",
      "{'tokens': ['[CLS]', '▁X', '선', '[MASK]', '[MASK]', '▁등이', '▁개발되어', '▁유기', '▁화합물', '▁분석', '에', '▁있어서', '[MASK]', '[MASK]', '▁방법으로', '▁자리잡았다', '.', '▁플라스틱', ',', '▁합성', '섬유', '등의', '▁고분', '자', '물질', '▁등도', '▁유기', '화', '학에서', '▁다루', '어진다', '.', '[SEP]', '▁유기', '화', '학은', '[MASK]', '[MASK]', '▁이루어진', '▁화합', '물을', '[MASK]', '▁분', '과', '이다', '.', '▁원래', '▁유기', '▁화합', '물은', '▁식물', '이나', '▁동물', '로부터', '▁추출', '해', '낸', '▁화합', '물을', '▁뜻', '하였으나', '▁지금은', '▁유기', '[SEP]'], 'segment': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'is_next': 0, 'mask_idx': [3, 4, 12, 13, 36, 37, 41, 48, 49], 'mask_label': ['▁결정', '학', '▁매우', '▁중요한', '▁탄', '소로', '▁연구하는', '▁화합', '물은']}\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# instance 생성 기능 확인\n",
    "count = 5\n",
    "\n",
    "with open(corpus_file, 'r') as in_f:\n",
    "    doc = []  # 단락 단위로 문서 저장\n",
    "    for line in tqdm(in_f, total=total):\n",
    "        line = line.strip()\n",
    "        if line == \"\":  # line이 빈줄 일 경우 (새로운 단락을 의미 함)\n",
    "            if 0 < len(doc):\n",
    "                instances = create_pretrain_instances(vocab, doc, n_test_seq, 0.15, vocab_list)\n",
    "                # save\n",
    "                print(\"doc:\", len(doc), \"instances:\", len(instances))\n",
    "                print(instances[0])\n",
    "                print(instances[-1])\n",
    "                print()\n",
    "                doc = []\n",
    "                if 0 < count:  # 테스트를 위해서 부분 처리 함\n",
    "                    count -= 1\n",
    "                else:\n",
    "                    break\n",
    "        else:  # doc에 저장\n",
    "            if 0 < len(pieces):\n",
    "                doc.append(pieces)\n",
    "    if 0 < len(doc):  # 마지막에 처리되지 않은 doc가 있는 경우\n",
    "        instances = create_pretrain_instances(doc, 128)\n",
    "        # save\n",
    "        print(\"doc:\", len(doc), \"instances:\", len(instances))\n",
    "        print(instances[0])\n",
    "        print(instances[-1])\n",
    "        print()\n",
    "        doc = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### make_pretrain_data() : BERT pretrain 데이터셋 생성 메소드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_pretrain_data(vocab, in_file, out_file, n_seq, mask_prob=0.15):\n",
    "    \"\"\" pretrain 데이터 생성 \"\"\"\n",
    "    def save_pretrain_instances(out_f, doc):\n",
    "        instances = create_pretrain_instances(vocab, doc, n_seq, mask_prob, vocab_list)\n",
    "        for instance in instances:\n",
    "            out_f.write(json.dumps(instance, ensure_ascii=False))\n",
    "            out_f.write(\"\\n\")\n",
    "\n",
    "    # 특수문자 7개를 제외한 vocab_list 생성\n",
    "    vocab_list = []\n",
    "    for id in range(7, len(vocab)):\n",
    "        if not vocab.is_unknown(id):\n",
    "            vocab_list.append(vocab.id_to_piece(id))\n",
    "\n",
    "    # line count 확인\n",
    "    line_cnt = 0\n",
    "    with open(in_file, \"r\") as in_f:\n",
    "        for line in in_f:\n",
    "            line_cnt += 1\n",
    "\n",
    "    with open(in_file, \"r\") as in_f:\n",
    "        with open(out_file, \"w\") as out_f:\n",
    "            doc = []\n",
    "            for line in tqdm(in_f, total=line_cnt):\n",
    "                line = line.strip()\n",
    "                if line == \"\":  # line이 빈줄 일 경우 (새로운 단락을 의미 함)\n",
    "                    if 0 < len(doc):\n",
    "                        save_pretrain_instances(out_f, doc)\n",
    "                        doc = []\n",
    "                else:  # line이 빈줄이 아닐 경우 tokenize 해서 doc에 저장\n",
    "                    pieces = vocab.encode_as_pieces(line)\n",
    "                    if 0 < len(pieces):\n",
    "                        doc.append(pieces)\n",
    "            if 0 < len(doc):  # 마지막에 처리되지 않은 doc가 있는 경우\n",
    "                save_pretrain_instances(out_f, doc)\n",
    "                doc = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9828bc8f3c084442baef8db70b0c8396",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3957761.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "pretrain_json_path = os.getenv('HOME')+'/aiffel/bert_pretrain/data/bert_pre_train.json'\n",
    "\n",
    "make_pretrain_data(vocab, corpus_file, pretrain_json_path, 128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "862285"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 라인수\n",
    "total = 0\n",
    "with open(pretrain_json_path, \"r\") as f:\n",
    "    for line in f:\n",
    "        total += 1\n",
    "total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 데이터셋 파일 생성 후 메모리 사용량 최소화\n",
    "- json 데이터 파일 크기 1.4GB -> 실제 BERT 학습용 dml 백분의 일 크기\n",
    "- `np.memmap` 사용하여 메모리 사용량 최소화 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(memmap([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], dtype=int32),\n",
       " memmap([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], dtype=int32),\n",
       " memmap([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], dtype=int32),\n",
       " memmap([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], dtype=int32),\n",
       " 0,\n",
       " 0,\n",
       " memmap([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], dtype=int32),\n",
       " memmap([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], dtype=int32))"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_seq = 128\n",
    "# [CLS], tokens_a, [SEP], tokens_b, [SEP]\n",
    "max_seq = n_seq - 3\n",
    "\n",
    "# 만약 일반적인 Numpy Array에다 데이터를 로딩한다면 이렇게 되겠지만\n",
    "# enc_tokens = np.zeros((total, n_seq), np.int32)\n",
    "# dec_tokens = np.zeros((total, n_seq), np.int32)\n",
    "# labels_nsp = np.zeros((total,), np.int32)\n",
    "# labels_mlm = np.zeros((total, n_seq), np.int32)\n",
    "\n",
    "# np.memmap을 사용하면 메모리를 적은 메모리에서도 대용량 데이터 처리가 가능 함\n",
    "enc_tokens = np.memmap(filename='enc_tokens.memmap', mode='w+', dtype=np.int32, shape=(total, n_seq))\n",
    "segments = np.memmap(filename='segments.memmap', mode='w+', dtype=np.int32, shape=(total, n_seq))\n",
    "labels_nsp = np.memmap(filename='labels_nsp.memmap', mode='w+', dtype=np.int32, shape=(total,))\n",
    "labels_mlm = np.memmap(filename='labels_mlm.memmap', mode='w+', dtype=np.int32, shape=(total, n_seq))\n",
    "\n",
    "\n",
    "enc_tokens[0], enc_tokens[-1], segments[0], segments[-1], labels_nsp[0], labels_nsp[-1], labels_mlm[0], labels_mlm[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a751d9b5bd6c43f8b60e2e2ae426b676",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=862285.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'tokens': ['[CLS]', '▁태어났다', '.', '▁조지아', '▁공과', '대학교를', '▁졸업하였다', '.', '▁그', '▁후', '▁해군에', '▁들어가', '▁전함', '·', '원자', '력', '·', '잠', '수', '함의', '▁승무', '원으로', '▁일하였다', '.', '▁1953', '년', '▁미국', '▁해군', '▁대', '위로', '▁예편', '하였고', '▁이후', '▁땅', '콩', '·', '면', '화', '▁등을', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '▁그의', '▁별명이', '▁\"', '땅', '콩', '▁농부', '\"', '▁(', 'P', 'ean', 'ut', '▁F', 'ar', 'mer', ')', '로', '[MASK]', '[MASK]', '[SEP]', '▁1962', '년', '▁조지아', '[MASK]', '▁상원', '▁의원', '▁선거에서', '▁낙선', '하나', '▁그', '▁선거가', '▁부정', '선거', '▁', '였', '음을', '[MASK]', '[MASK]', '▁되어', '▁당선', '되고', ',', '▁1966', '년', '[MASK]', '▁주', '▁지사', '▁선거에', '▁낙선', '하지만', '▁1970', '년', '▁조지아', '[MASK]', '[MASK]', '[MASK]', '▁역임했다', '.', '▁대통령이', '▁되기', '▁전', '▁조지아', '주', '▁상원의', '원을', '▁두', '번', '▁연', '임', '했으며', ',', '▁1971', '년부터', '[MASK]', '[MASK]', '▁조지아', '▁지', '사로', '▁근무했다', '.', '▁조지아', '▁주지', '[SEP]'], 'segment': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'is_next': 1, 'mask_idx': [39, 40, 41, 42, 43, 44, 45, 62, 63, 68, 81, 82, 89, 98, 99, 100, 118, 119], 'mask_label': ['▁가', '꿔', '▁많은', '▁돈을', '▁벌', '었다', '.', '▁알려졌다', '.', '▁주', '▁입증', '하게', '▁조지아', '▁주', '▁지', '사를', '▁1975', '년까지']}\n",
      "enc_token: [5, 1605, 27599, 5551, 14146, 15991, 8637, 27599, 13, 81, 25987, 2247, 15033, 27873, 14475, 27813, 27873, 28196, 27636, 10185, 16285, 1232, 22935, 27599, 4777, 27625, 243, 2780, 14, 1509, 22095, 414, 165, 1697, 28290, 27873, 27703, 27683, 593, 6, 6, 6, 6, 6, 6, 6, 307, 16905, 103, 28313, 28290, 19041, 27718, 98, 27878, 15784, 2543, 309, 337, 5771, 27616, 27603, 6, 6, 4, 3715, 27625, 5551, 6, 11234, 2378, 5249, 9858, 3294, 13, 20590, 2386, 2163, 27596, 27671, 969, 6, 6, 607, 2387, 317, 27604, 3926, 27625, 6, 37, 18995, 8198, 9858, 1447, 1921, 27625, 5551, 6, 6, 6, 4267, 27599, 4864, 6436, 25, 5551, 27646, 18205, 928, 157, 27821, 61, 27773, 530, 27604, 3372, 523, 6, 6, 5551, 18, 982, 13264, 27599, 5551, 5053, 4]\n",
      "segment: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "label_nsp: 1\n",
      "label_mlm: [    0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0    21 29007   399  5540   813    17 27599     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0  4578 27599     0     0     0     0    37     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0  8047   173     0\n",
      "     0     0     0     0     0  5551     0     0     0     0     0     0\n",
      "     0     0    37    18   451     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0  3409   673\n",
      "     0     0     0     0     0     0     0     0]\n",
      "\n",
      "{'tokens': ['[CLS]', '▁1976', '년', '▁대통령', '▁선거에', '▁민주당', '▁후보로', '▁출마하여', '▁도덕', '주의', '▁정책으로', '▁내세워', ',', '[MASK]', '[MASK]', '▁누르고', '▁당선되었다', '.', '[SEP]', '[MASK]', '▁대통령은', '▁에너지', '[MASK]', '▁촉구', '했으나', '▁공화', '당의', '▁반대로', '▁무산되었다', '.', '▁카', '터는', '▁이집', '트와', '[MASK]', '[MASK]', '▁조정', '하여', ',', '▁캠프', '▁데이비', '드에서', '▁안', '와', '르', '▁사다', '트', '▁대통령과', '▁메', '나', '헴', '▁베', '긴', '▁수상', '과', '▁함께', '[MASK]', '▁평화를', '▁위한', '▁캠프', '데이', '비', '드', '▁협정을', '[MASK]', '[MASK]', '▁그러나', '▁이것은', '▁공화', '당과', '[MASK]', '▁유대인', '▁단체의', '▁반발을', '▁일으켰다', '.', '▁1979', '년', '▁백악', '관에서', '[MASK]', '▁간의', '▁평화', '조약', '으로', '▁이끌', '어졌다', '.', '▁또한', '▁소련과', '▁제', '2', '차', '[MASK]', '▁무기', '▁제한', '▁협', '상에', '[MASK]', '[MASK]', '[MASK]', '▁카', '터는', '▁1970', '년대', '▁후반', '[MASK]', '▁대한민국', '▁등', '▁인권', '▁후진', '국의', '▁국민들의', '▁인', '권을', '▁지키기', '▁위해', '▁노력', '했으며', ',', '[MASK]', '▁이후', '▁계속해서', '▁도덕', '정', '치를', '▁내세', '[SEP]'], 'segment': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'is_next': 1, 'mask_idx': [13, 14, 19, 22, 34, 35, 56, 64, 65, 70, 80, 89, 93, 98, 99, 100, 106, 120], 'mask_label': ['▁포', '드를', '▁카터', '▁개발을', '▁이스라엘', '을', '▁중동', '▁체결했다', '.', '▁미국의', '▁양국', '▁소련과', '▁전략', '▁조인', '했다', '.', '▁당시', '▁취임']}\n",
      "enc_token: [5, 3306, 27625, 663, 8198, 4867, 4896, 19160, 6244, 238, 22033, 19990, 27604, 6, 6, 10071, 7965, 27599, 4, 6, 5906, 3634, 6, 9747, 1003, 4460, 1547, 4771, 18474, 27599, 207, 4612, 2703, 3604, 6, 6, 3358, 54, 27604, 10251, 3640, 3552, 172, 27665, 27699, 7025, 27677, 13799, 334, 27637, 29887, 271, 28099, 1011, 27644, 280, 6, 14237, 521, 10251, 4282, 27694, 27681, 15990, 6, 6, 330, 1487, 4460, 4040, 6, 7455, 15747, 21408, 6564, 27599, 2995, 27625, 10312, 6749, 6, 2714, 2793, 8993, 9, 1435, 2521, 27599, 276, 23197, 30, 27619, 27751, 6, 3841, 1956, 617, 1824, 6, 6, 6, 207, 4612, 1921, 596, 1840, 6, 410, 50, 5636, 17092, 137, 18896, 42, 917, 15177, 231, 3375, 530, 27604, 6, 165, 6357, 6244, 27642, 1233, 5890, 4]\n",
      "segment: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "label_nsp: 1\n",
      "label_mlm: [    0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0   119  1486     0     0     0     0 25250     0     0  8085     0\n",
      "     0     0     0     0     0     0     0     0     0     0  3426 27607\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0  8021     0     0     0\n",
      "     0     0     0     0 19102 27599     0     0     0     0   679     0\n",
      "     0     0     0     0     0     0     0     0 13195     0     0     0\n",
      "     0     0     0     0     0 23197     0     0     0  2835     0     0\n",
      "     0     0 15876    31 27599     0     0     0     0     0   316     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "  2659     0     0     0     0     0     0     0]\n",
      "\n",
      "{'tokens': ['[CLS]', '[MASK]', '▁1982', '년까지', '▁3', '단', '계에', '▁걸쳐', '▁주한미', '군을', '▁철수', '하기로', '▁했다', '.', '▁그러나', '▁주한미', '군', '사령', '부와', '▁정보', '기관', '·', '의', '회의', '[MASK]', '[MASK]', '▁부딪', '혀', '[MASK]', '[MASK]', '▁완전', '철', '수', '▁대신', '▁6,000', '명을', '▁감축', '하는', '▁데', '▁그쳤다', '.', '▁또한', '▁박정희', '▁정권의', '[MASK]', '▁문제', '▁등', '과의', '▁논란', '으로', '▁불', '협', '화', '음을', '▁냈', '으나', ',', '▁1979', '년', '▁6', '월', '▁하', '순', ',', '▁대한민국을', '▁방문하여', '[MASK]', '▁다소', '▁회복', '되었다', '.', '[SEP]', '▁그러나', '▁주', '[MASK]', '▁미국', '▁대사관', '▁인', '질', '▁사건에서', '▁인', '질', '▁구출', '▁실패를', '▁이유로', '▁1980', '년', '▁대통령', '▁선거에서', '▁공화', '당의', '▁로', '널드', '黃', '▁아미', '▁후보', '에게', '▁', '져', '▁결국', '▁재', '선에', '▁실패했다', '.', '▁또한', '▁임기', '▁말기에', '▁터', '진', '▁소련의', '▁아프가니스탄', '▁침공', '▁사건으로', '▁인해', '▁1980', '년', '▁하계', '▁올림픽에', '▁반공', '국가', '들의', '▁보이', '콧', '을', '▁내세', '웠다', '.', '[SEP]'], 'segment': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'is_next': 0, 'mask_idx': [1, 24, 25, 28, 29, 44, 46, 47, 66, 68, 69, 70, 74, 93, 94, 121, 122, 123], 'mask_label': [',', '▁반', '대에', '▁주한미', '군은', '▁인권', '▁등', '과의', '▁관계가', '▁회복', '되었다', '.', '▁이란', '▁레이', '건', '▁보이', '콧', '을']}\n",
      "enc_token: [5, 6, 2760, 673, 49, 27737, 1949, 1633, 24438, 1262, 5337, 2390, 345, 27599, 330, 24438, 27722, 3069, 2576, 1071, 1468, 27873, 27601, 511, 6, 6, 11574, 28178, 6, 6, 4626, 27917, 27636, 1083, 23453, 859, 26346, 38, 189, 11330, 27599, 276, 5298, 13574, 6, 550, 50, 786, 2408, 9, 128, 27993, 27683, 969, 15139, 191, 27604, 2995, 27625, 125, 27662, 27, 27946, 27604, 16187, 14905, 6, 5421, 3332, 43, 27599, 4, 330, 37, 6, 243, 18590, 42, 27892, 23937, 42, 27892, 11560, 22684, 1827, 1640, 27625, 663, 5249, 4460, 1547, 194, 8631, 28871, 4593, 958, 113, 27596, 27944, 875, 174, 2087, 9510, 27599, 276, 11034, 12145, 870, 27713, 5569, 7676, 3232, 6322, 751, 1640, 27625, 2219, 5825, 13948, 4398, 247, 3052, 28805, 27607, 5890, 1853, 27599, 4]\n",
      "segment: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "label_nsp: 0\n",
      "label_mlm: [    0 27604     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "   141   867     0     0 24438   941     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0  5636     0    50   786\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0  4857     0  3332    43 27599     0\n",
      "     0     0  3290     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0  1169 27803     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0  3052 28805 27607     0     0     0     0]\n",
      "\n",
      "{'tokens': ['[CLS]', '[MASK]', '▁이후', '▁민간', '▁자원을', '▁적극', '▁활용한', '▁비영리', '▁기구', '인', '▁카터', '▁재', '단을', '[MASK]', '[MASK]', '▁민주주의', '[MASK]', '[MASK]', '▁위해', '▁제', '▁3', '세계의', '[MASK]', '▁감시', '▁활동', '▁및', '▁기니', '▁벌', '레', '에', '▁의한', '▁드라', '쿤', '쿠르', '스', '▁질병', '▁방', '재를', '[MASK]', '▁힘썼다', '.', '▁미국의', '▁빈곤', '층', '▁지원', '▁활동', ',', '▁사랑의', '▁집', '짓', '기', '[MASK]', '[MASK]', '[MASK]', '▁분쟁', '▁중재', '▁등의', '▁활동도', '▁했다', '.', '[SEP]', '▁1979', '년', '▁~', '▁1980', '년', '▁대한민국의', '▁정치적', '▁격', '변', '기', '[MASK]', '▁대통령', '이었던', '▁그는', '▁이에', '▁대해', '▁애매', '한', '▁태도를', '▁보였고', ',', '▁이는', '▁후에', '▁대한민국', '▁내에서', '▁고조', '되는', '▁반', '미', '▁운동의', '▁한', '▁원인이', '▁됐다', '.', '▁10', '월', '▁26', '일', ',', '▁박정희', '▁대통령이', '▁김재', '규', '[MASK]', '[MASK]', '[MASK]', '▁의해', '[MASK]', '[MASK]', '▁것에', '▁대해', '▁그는', '▁이', '▁사건으로', '▁큰', '[MASK]', '▁받았으며', ',', '▁사이', '러스', '▁밴', '스', '▁국무', '장', '관을', '▁조', '[SEP]'], 'segment': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'is_next': 0, 'mask_idx': [1, 13, 14, 16, 17, 22, 38, 51, 52, 53, 67, 71, 104, 105, 106, 108, 109, 116], 'mask_label': ['▁퇴임', '▁설립한', '▁뒤', '▁실현', '을', '▁선거', '▁위해', '▁운동', ',', '▁국제', '▁정치적', '▁당시의', '▁중앙정보', '부', '장에', '▁살해', '된', '▁충격을']}\n",
      "enc_token: [5, 6, 165, 3174, 17304, 2929, 20639, 16068, 6673, 27628, 25250, 174, 1574, 6, 6, 9889, 6, 6, 231, 30, 49, 21655, 6, 7049, 375, 228, 18137, 813, 27740, 27600, 1332, 17378, 28956, 16453, 27626, 5225, 95, 4445, 6, 19607, 27599, 679, 14197, 28083, 770, 375, 27604, 14003, 313, 28333, 27614, 6, 6, 6, 4476, 13267, 507, 27328, 345, 27599, 4, 2995, 27625, 203, 1640, 27625, 447, 2843, 1032, 27889, 27614, 6, 663, 1277, 202, 695, 433, 26003, 27612, 11162, 19177, 27604, 594, 1140, 410, 3428, 8964, 267, 141, 27686, 8711, 34, 6418, 3842, 27599, 131, 27662, 981, 27629, 27604, 5298, 4864, 9918, 27958, 6, 6, 6, 355, 6, 6, 2057, 433, 202, 8, 6322, 459, 6, 5325, 27604, 328, 2086, 1228, 27626, 4444, 27651, 1657, 53, 4]\n",
      "segment: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "label_nsp: 0\n",
      "label_mlm: [    0 13826     0     0     0     0     0     0     0     0     0     0\n",
      "     0  7301   339     0  5031 27607     0     0     0     0   822     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0   231     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0   887 27604   605     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0  2843     0     0     0  3195\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0 18525 27638  1312     0\n",
      "  2591 27711     0     0     0     0     0     0 10688     0     0     0\n",
      "     0     0     0     0     0     0     0     0]\n",
      "\n",
      "{'tokens': ['[CLS]', '▁초래', '한', '▁인물', '▁및', '▁단체를', '▁직접', '▁만나', '▁분쟁', '의', '▁원인을', '▁근본적으로', '[MASK]', '▁위해', '▁힘썼다', '.', '▁이', '▁과정에서', '▁미국', '▁행정', '부와', '▁갈등을', '▁보이기도', '▁했지만', ',', '▁전직', '▁대통령의', '▁권한', '과', '[MASK]', '[MASK]', '▁유명', '▁인사', '들의', '▁활약으로', '▁해결', '해', '▁나갔다', '.', '▁1978', '년에', '▁채', '결', '된', '▁캠프', '데이', '비', '드', '[MASK]', '[MASK]', '▁이', '행이', '▁지지', '부', '진', '▁하자', '[MASK]', '▁분쟁', '▁분', '제를', '▁해결하기', '▁위해', '▁1993', '년', '▁퇴임', '▁후', '▁직접', '▁이스라엘', '과', '▁팔', '레인', '스타', '인의', '▁오슬로', '▁협정을', '▁이끌어', '▁내는', '▁데', '도', '▁성공했다', '.', '[SEP]', '▁카', '터는', '▁카터', '▁행정부', '▁이후', '▁미국이', '▁북', '핵', '▁위기', ',', '▁코소보', '▁전쟁', ',', '▁이라크', '[MASK]', '▁같이', '▁미국이', '▁군사적', '▁행동을', '▁최후', '로', '▁선택하는', '▁전통적', '▁사고를', '▁버리고', '▁군사적', '▁행동을', '▁선행', '하는', '▁행위에', '▁대해', '[MASK]', '▁유', '감을', '▁표시', '▁하며', '▁미국의', '▁군사적', '[MASK]', '[MASK]', '▁반대', '▁입장을', '[MASK]', '▁있다', '.', '[SEP]'], 'segment': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'is_next': 0, 'mask_idx': [12, 16, 29, 30, 48, 49, 55, 56, 58, 59, 96, 98, 108, 113, 119, 120, 121, 124], 'mask_label': ['▁해결하기', '▁이', '▁재', '야', '▁협', '정의', '▁하자', '▁중동', '▁분', '제를', '▁전쟁과', '▁미국이', '▁행동을', '▁깊은', '▁군사적', '▁활동에', '▁강한', '▁보이고']}\n",
      "enc_token: [5, 8200, 27612, 1178, 228, 19762, 1069, 2142, 4476, 27601, 13635, 24806, 6, 231, 19607, 27599, 8, 2208, 243, 895, 2576, 12627, 23828, 3379, 27604, 5605, 5744, 3753, 27644, 6, 6, 939, 3329, 247, 17462, 2317, 27645, 9946, 27599, 3331, 169, 481, 27783, 27711, 10251, 4282, 27694, 27681, 6, 6, 8, 7071, 1565, 27638, 27713, 3121, 6, 4476, 147, 1104, 13425, 231, 2062, 27625, 13826, 81, 1069, 3426, 27644, 961, 5346, 936, 692, 25213, 15990, 7027, 7573, 189, 27627, 6608, 27599, 4, 207, 4612, 25250, 21862, 165, 8424, 251, 28166, 10622, 27604, 26202, 506, 27604, 6157, 6, 733, 8424, 9641, 5616, 12241, 27603, 26028, 15644, 13098, 8275, 9641, 5616, 16374, 38, 19690, 433, 6, 46, 2196, 2466, 1368, 679, 9641, 6, 6, 1216, 5168, 6, 28, 27599, 4]\n",
      "segment: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "label_nsp: 0\n",
      "label_mlm: [    0     0     0     0     0     0     0     0     0     0     0     0\n",
      " 13425     0     0     0     8     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0   174 27775     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "   617  2028     0     0     0     0     0  3121  8021     0   147  1104\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      " 17305     0  8424     0     0     0     0     0     0     0     0     0\n",
      "  5616     0     0     0     0  4508     0     0     0     0     0  9641\n",
      "  8253  2632     0     0  7010     0     0     0]\n",
      "\n",
      "{'tokens': ['[CLS]', '에서도', '[MASK]', '[MASK]', '[MASK]', '▁유엔', '에', '▁유엔', '인권', '고등', '판', '무', '관의', '▁제도를', '▁시행', '하도록', '▁노력', '하여', '▁독재', '자들의', '▁인권', '▁유', '린', '에', '▁대해', '▁제', '약을', '▁하고', ',', '▁국제', '형사', '재판', '소를', '▁만드는', '▁데', '▁기여', '하여', '▁독재', '자들', '▁같은', '▁인권', '유', '린', '범죄', '자를', '[MASK]', '[MASK]', '▁회', '부', '하여', '▁국제적인', '▁처벌을', '▁받게', '▁하는', '▁등', '▁인권', '▁신', '장에', '▁크', '나', '▁큰', '▁기여를', '▁했다', '.', '[SEP]', '▁1993', '년', '▁1', '차', '[MASK]', '[MASK]', '▁위기', '▁당시', '▁북한에', '▁대한', '▁미국의', '▁군사적', '▁행동이', '▁임', '박', '했으나', ',', '▁미국', '▁전직', '▁대통령', '으로는', '[MASK]', '▁북한', '을', '▁방문', '하고', '▁미국과', '[MASK]', '▁양국의', '▁중재', '에', '▁큰', '▁기여를', '▁해', '▁위기를', '▁해결', '했다는', '▁평가를', '▁받았다', '.', '[MASK]', '▁이', '▁때', '▁김영삼', '▁대통령과', '▁김일성', '▁주', '석의', '▁만남', '을', '▁주선', '했다', '.', '▁하지만', '[MASK]', '▁수', '주일', '▁후', '[MASK]', '[MASK]', '▁갑자기', '▁사망하여', '[SEP]'], 'segment': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'is_next': 0, 'mask_idx': [2, 3, 4, 21, 22, 23, 45, 46, 69, 70, 86, 92, 105, 119, 120, 121, 123, 124], 'mask_label': ['▁관심이', '▁깊', '어', '▁유', '린', '에', '▁재판', '소로', '▁북', '핵', '▁처음으로', '▁북', '▁또한', '▁그로부터', '▁수', '주일', '▁김일', '성이']}\n",
      "enc_token: [5, 643, 6, 6, 6, 3708, 27600, 3708, 12972, 5059, 27841, 27725, 2429, 6520, 2404, 1816, 3375, 54, 7559, 2653, 5636, 46, 27870, 27600, 433, 30, 3297, 644, 27604, 605, 17905, 4731, 1358, 3002, 189, 2187, 54, 7559, 3989, 226, 5636, 27690, 27870, 8822, 690, 6, 6, 270, 27638, 54, 12835, 19956, 4256, 419, 50, 5636, 90, 1312, 252, 27637, 459, 13856, 345, 27599, 4, 2062, 27625, 7, 27751, 6, 6, 10622, 316, 25086, 92, 679, 9641, 18507, 273, 27914, 1003, 27604, 243, 5605, 663, 1030, 6, 1876, 27607, 2017, 48, 5672, 6, 25764, 13267, 27600, 459, 13856, 87, 11239, 2317, 2351, 4549, 772, 27599, 6, 8, 84, 9133, 13799, 11444, 37, 5361, 11842, 27607, 25754, 31, 27599, 589, 6, 19, 10106, 81, 6, 6, 5908, 26809, 4]\n",
      "segment: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "label_nsp: 0\n",
      "label_mlm: [    0     0  8181  1910 27633     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0    46 27870 27600\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0  2474  2661     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0   251 28166     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0  1307     0     0     0     0     0   251     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0   276     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0 14313\n",
      "    19 10106     0  4636   684     0     0     0]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# json 파일을 라인 단위로 읽어 들여 np.memmap에 로딩\n",
    "with open(pretrain_json_path, \"r\") as f:\n",
    "    for i, line in enumerate(tqdm(f, total=total)):\n",
    "        if 5 < i:  # 테스트를 위해서 5개만 확인\n",
    "            break\n",
    "        data = json.loads(line)\n",
    "        # encoder token\n",
    "        enc_token = [vocab.piece_to_id(p) for p in data[\"tokens\"]]\n",
    "        enc_token += [0] * (n_seq - len(enc_token))\n",
    "        # segment\n",
    "        segment = data[\"segment\"]\n",
    "        segment += [0] * (n_seq - len(segment))\n",
    "        # nsp label\n",
    "        label_nsp = data[\"is_next\"]\n",
    "        # mlm label\n",
    "        mask_idx = np.array(data[\"mask_idx\"], dtype=np.int)\n",
    "        mask_label = np.array([vocab.piece_to_id(p) for p in data[\"mask_label\"]], dtype=np.int)\n",
    "        label_mlm = np.full(n_seq, dtype=np.int, fill_value=0)\n",
    "        label_mlm[mask_idx] = mask_label\n",
    "\n",
    "        print(data)\n",
    "        print(\"enc_token:\", enc_token)\n",
    "        print(\"segment:\", segment)\n",
    "        print(\"label_nsp:\", label_nsp)\n",
    "        print(\"label_mlm:\", label_mlm)\n",
    "        print()\n",
    "\n",
    "        assert len(enc_token) == len(segment) == len(label_mlm) == n_seq\n",
    "\n",
    "        enc_tokens[i] = enc_token\n",
    "        segments[i] = segment\n",
    "        labels_nsp[i] = label_nsp\n",
    "        labels_mlm[i] = label_mlm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load_pre_train_data() : 학습에 필요한 데이터를 로딩하는 함수\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pre_train_data(vocab, filename, n_seq, count=None):\n",
    "    \"\"\"\n",
    "    학습에 필요한 데이터를 로드\n",
    "    :param vocab: vocab\n",
    "    :param filename: 전처리된 json 파일\n",
    "    :param n_seq: 시퀀스 길이 (number of sequence)\n",
    "    :param count: 데이터 수 제한 (None이면 전체)\n",
    "    :return enc_tokens: encoder inputs\n",
    "    :return segments: segment inputs\n",
    "    :return labels_nsp: nsp labels\n",
    "    :return labels_mlm: mlm labels\n",
    "    \"\"\"\n",
    "    total = 0\n",
    "    with open(filename, \"r\") as f:\n",
    "        for line in f:\n",
    "            total += 1\n",
    "            # 데이터 수 제한\n",
    "            if count is not None and count <= total:\n",
    "                break\n",
    "    \n",
    "    # np.memmap을 사용하면 메모리를 적은 메모리에서도 대용량 데이터 처리가 가능 함\n",
    "    enc_tokens = np.memmap(filename='enc_tokens.memmap', mode='w+', dtype=np.int32, shape=(total, n_seq))\n",
    "    segments = np.memmap(filename='segments.memmap', mode='w+', dtype=np.int32, shape=(total, n_seq))\n",
    "    labels_nsp = np.memmap(filename='labels_nsp.memmap', mode='w+', dtype=np.int32, shape=(total,))\n",
    "    labels_mlm = np.memmap(filename='labels_mlm.memmap', mode='w+', dtype=np.int32, shape=(total, n_seq))\n",
    "\n",
    "    with open(filename, \"r\") as f:\n",
    "        for i, line in enumerate(tqdm(f, total=total)):\n",
    "            if total <= i:\n",
    "                print(\"data load early stop\", total, i)\n",
    "                break\n",
    "            data = json.loads(line)\n",
    "            # encoder token\n",
    "            enc_token = [vocab.piece_to_id(p) for p in data[\"tokens\"]]\n",
    "            enc_token += [0] * (n_seq - len(enc_token))\n",
    "            # segment\n",
    "            segment = data[\"segment\"]\n",
    "            segment += [0] * (n_seq - len(segment))\n",
    "            # nsp label\n",
    "            label_nsp = data[\"is_next\"]\n",
    "            # mlm label\n",
    "            mask_idx = np.array(data[\"mask_idx\"], dtype=np.int)\n",
    "            mask_label = np.array([vocab.piece_to_id(p) for p in data[\"mask_label\"]], dtype=np.int)\n",
    "            label_mlm = np.full(n_seq, dtype=np.int, fill_value=0)\n",
    "            label_mlm[mask_idx] = mask_label\n",
    "\n",
    "            assert len(enc_token) == len(segment) == len(label_mlm) == n_seq\n",
    "\n",
    "            enc_tokens[i] = enc_token\n",
    "            segments[i] = segment\n",
    "            labels_nsp[i] = label_nsp\n",
    "            labels_mlm[i] = label_mlm\n",
    "\n",
    "    return (enc_tokens, segments), (labels_nsp, labels_mlm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa93ececf5a245f49b330d6cabb54bc3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=128000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data load early stop 128000 128000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 128000건만 메모리에 로딩\n",
    "pre_train_inputs, pre_train_labels = load_pre_train_data(vocab, pretrain_json_path, 128, count=128000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(memmap([    5,  1605, 27599,  5551, 14146, 15991,  8637, 27599,    13,\n",
       "            81, 25987,  2247, 15033, 27873, 14475, 27813, 27873, 28196,\n",
       "         27636, 10185, 16285,  1232, 22935, 27599,  4777, 27625,   243,\n",
       "          2780,    14,  1509, 22095,   414,   165,  1697, 28290, 27873,\n",
       "         27703, 27683,   593,     6,     6,     6,     6,     6,     6,\n",
       "             6,   307, 16905,   103, 28313, 28290, 19041, 27718,    98,\n",
       "         27878, 15784,  2543,   309,   337,  5771, 27616, 27603,     6,\n",
       "             6,     4,  3715, 27625,  5551,     6, 11234,  2378,  5249,\n",
       "          9858,  3294,    13, 20590,  2386,  2163, 27596, 27671,   969,\n",
       "             6,     6,   607,  2387,   317, 27604,  3926, 27625,     6,\n",
       "            37, 18995,  8198,  9858,  1447,  1921, 27625,  5551,     6,\n",
       "             6,     6,  4267, 27599,  4864,  6436,    25,  5551, 27646,\n",
       "         18205,   928,   157, 27821,    61, 27773,   530, 27604,  3372,\n",
       "           523,     6,     6,  5551,    18,   982, 13264, 27599,  5551,\n",
       "          5053,     4], dtype=int32),\n",
       " memmap([    5,  2191, 27599,  5078,    81, 27604,   342,  4812, 27625,\n",
       "           294, 14456,     6, 24930,  2540, 27600,   488,  4871,  2524,\n",
       "         13358,   171, 27599,   330, 27604, 14456,     6,  4842, 27682,\n",
       "         27625,  2131, 14135,  9943,   761, 28254,   658,   171, 27599,\n",
       "            13,     6, 13069, 11312,   496,  8020,  1910, 27633,  4216,\n",
       "           664,   926,  5238,  1053,  5583, 27614,  2419,  2470, 27604,\n",
       "         21622, 27602,   838, 15403, 27760,   824,  3525, 13998,  7619,\n",
       "         27599,     4,   371, 27604,    29, 28018, 27793,  1326, 27787,\n",
       "            66,   412, 22289,    28, 27599,  1326,  5884,     6,  2573,\n",
       "            66,  1599, 27653,   639,  3883,   353, 27599, 17506,  5263,\n",
       "          1326,  5884,    19,   402,     6,     6,     6,     6,     6,\n",
       "          2742,    31, 27599,     6, 16343,     6,     6,     6,     6,\n",
       "             6,     6,     6,     6, 10603, 27616,     9, 25545,  1599,\n",
       "         27653,   639,   254,   238,   787,  2654,   784, 27604,  1925,\n",
       "           259,     4], dtype=int32),\n",
       " memmap([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], dtype=int32),\n",
       " memmap([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], dtype=int32),\n",
       " 1,\n",
       " 0,\n",
       " memmap([    0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,    21, 29007,   399,  5540,   813,    17,\n",
       "         27599,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,  4578,\n",
       "         27599,     0,     0,     0,     0,    37,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "          8047,   173,     0,     0,     0,     0,     0,     0,  5551,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,    37,\n",
       "            18,   451,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,  3409,   673,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0], dtype=int32),\n",
       " memmap([    0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,    89,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0, 15170,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,  4031,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0, 11497,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,   231, 11497,  9933, 27607,  2042,\n",
       "             0,     0,     0, 11364,     0,  1911, 27604,    68, 27650,\n",
       "         27617,  3065, 28116, 27601,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0], dtype=int32))"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 처음과 마지막 확인\n",
    "pre_train_inputs[0][0], pre_train_inputs[0][-1], pre_train_inputs[1][0], pre_train_inputs[1][-1], pre_train_labels[0][0], pre_train_labels[0][-1], pre_train_labels[1][0], pre_train_labels[1][-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BERT 모델 구현\n",
    "- 트랜스포머 인코더 레이어로 구현\n",
    "    - 3개의 임베딩 레이어를 지님\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "def get_pad_mask(tokens, i_pad=0):\n",
    "    \"\"\"\n",
    "    pad mask 계산하는 함수\n",
    "    :param tokens: tokens (bs, n_seq)\n",
    "    :param i_pad: id of pad\n",
    "    :return mask: pad mask (pad: 1, other: 0)\n",
    "    \"\"\"\n",
    "    mask = tf.cast(tf.math.equal(tokens, i_pad), tf.float32)\n",
    "    mask = tf.expand_dims(mask, axis=1)\n",
    "    return mask\n",
    "\n",
    "\n",
    "def get_ahead_mask(tokens, i_pad=0):\n",
    "    \"\"\"\n",
    "    ahead mask 계산하는 함수\n",
    "    :param tokens: tokens (bs, n_seq)\n",
    "    :param i_pad: id of pad\n",
    "    :return mask: ahead and pad mask (ahead or pad: 1, other: 0)\n",
    "    \"\"\"\n",
    "    n_seq = tf.shape(tokens)[1]\n",
    "    ahead_mask = 1 - tf.linalg.band_part(tf.ones((n_seq, n_seq)), -1, 0)\n",
    "    ahead_mask = tf.expand_dims(ahead_mask, axis=0)\n",
    "    pad_mask = get_pad_mask(tokens, i_pad)\n",
    "    mask = tf.maximum(ahead_mask, pad_mask)\n",
    "    return mask\n",
    "print(\"슝=3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "@tf.function(experimental_relax_shapes=True)\n",
    "def gelu(x):\n",
    "    \"\"\"\n",
    "    gelu activation 함수\n",
    "    :param x: 입력 값\n",
    "    :return: gelu activation result\n",
    "    \"\"\"\n",
    "    return 0.5 * x * (1 + K.tanh(x * 0.7978845608 * (1 + 0.044715 * x * x)))\n",
    "print(\"슝=3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "def kernel_initializer(stddev=0.02):\n",
    "    \"\"\"\n",
    "    parameter initializer 생성\n",
    "    :param stddev: 생성할 랜덤 변수의 표준편차\n",
    "    \"\"\"\n",
    "    return tf.keras.initializers.TruncatedNormal(stddev=stddev)\n",
    "\n",
    "\n",
    "def bias_initializer():\n",
    "    \"\"\"\n",
    "    bias initializer 생성\n",
    "    \"\"\"\n",
    "    return tf.zeros_initializer\n",
    "print(\"슝=3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "class Config(dict):\n",
    "    \"\"\"\n",
    "    json을 config 형태로 사용하기 위한 Class\n",
    "    :param dict: config dictionary\n",
    "    \"\"\"\n",
    "    __getattr__ = dict.__getitem__\n",
    "    __setattr__ = dict.__setitem__\n",
    "\n",
    "    @classmethod\n",
    "    def load(cls, file):\n",
    "        \"\"\"\n",
    "        file에서 Config를 생성 함\n",
    "        :param file: filename\n",
    "        \"\"\"\n",
    "        with open(file, 'r') as f:\n",
    "            config = json.loads(f.read())\n",
    "            return Config(config)\n",
    "print(\"슝=3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "# token embedding 구현\n",
    "class SharedEmbedding(tf.keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    Weighed Shaed Embedding Class\n",
    "    \"\"\"\n",
    "    def __init__(self, config, name=\"weight_shared_embedding\"):\n",
    "        \"\"\"\n",
    "        생성자\n",
    "        :param config: Config 객체\n",
    "        :param name: layer name\n",
    "        \"\"\"\n",
    "        super().__init__(name=name)\n",
    "\n",
    "        self.n_vocab = config.n_vocab\n",
    "        self.d_model = config.d_model\n",
    "    \n",
    "    def build(self, input_shape):\n",
    "        \"\"\"\n",
    "        shared weight 생성\n",
    "        :param input_shape: Tensor Shape (not used)\n",
    "        \"\"\"\n",
    "        with tf.name_scope(\"shared_embedding_weight\"):\n",
    "            self.shared_weights = self.add_weight(\n",
    "                \"weights\",\n",
    "                shape=[self.n_vocab, self.d_model],\n",
    "                initializer=kernel_initializer()\n",
    "            )\n",
    "\n",
    "    def call(self, inputs, mode=\"embedding\"):\n",
    "        \"\"\"\n",
    "        layer 실행\n",
    "        :param inputs: 입력\n",
    "        :param mode: 실행 모드\n",
    "        :return: embedding or linear 실행 결과\n",
    "        \"\"\"\n",
    "        # mode가 embedding일 경우 embedding lookup 실행\n",
    "        if mode == \"embedding\":\n",
    "            return self._embedding(inputs)\n",
    "        # mode가 linear일 경우 linear 실행\n",
    "        elif mode == \"linear\":\n",
    "            return self._linear(inputs)\n",
    "        # mode가 기타일 경우 오류 발생\n",
    "        else:\n",
    "            raise ValueError(f\"mode {mode} is not valid.\")\n",
    "    \n",
    "    def _embedding(self, inputs):\n",
    "        \"\"\"\n",
    "        embedding lookup\n",
    "        :param inputs: 입력\n",
    "        \"\"\"\n",
    "        embed = tf.gather(self.shared_weights, tf.cast(inputs, tf.int32))\n",
    "        return embed\n",
    "\n",
    "    def _linear(self, inputs):  # (bs, n_seq, d_model)\n",
    "        \"\"\"\n",
    "        linear 실행\n",
    "        :param inputs: 입력\n",
    "        \"\"\"\n",
    "        n_batch = tf.shape(inputs)[0]\n",
    "        n_seq = tf.shape(inputs)[1]\n",
    "        inputs = tf.reshape(inputs, [-1, self.d_model])  # (bs * n_seq, d_model)\n",
    "        outputs = tf.matmul(inputs, self.shared_weights, transpose_b=True)\n",
    "        outputs = tf.reshape(outputs, [n_batch, n_seq, self.n_vocab])  # (bs, n_seq, n_vocab)\n",
    "        return outputs\n",
    "print(\"슝=3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "# Positional Embedding  구현\n",
    "class PositionalEmbedding(tf.keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    Positional Embedding Class\n",
    "    \"\"\"\n",
    "    def __init__(self, config, name=\"position_embedding\"):\n",
    "        \"\"\"\n",
    "        생성자\n",
    "        :param config: Config 객체\n",
    "        :param name: layer name\n",
    "        \"\"\"\n",
    "        super().__init__(name=name)\n",
    "        \n",
    "        self.embedding = tf.keras.layers.Embedding(config.n_seq, config.d_model, embeddings_initializer=kernel_initializer())\n",
    "\n",
    "    def call(self, inputs):\n",
    "        \"\"\"\n",
    "        layer 실행\n",
    "        :param inputs: 입력\n",
    "        :return embed: positional embedding lookup 결과\n",
    "        \"\"\"\n",
    "        position = tf.cast(tf.math.cumsum(tf.ones_like(inputs), axis=1, exclusive=True), tf.int32)\n",
    "        embed = self.embedding(position)\n",
    "        return embed\n",
    "print(\"슝=3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Segment Embedding은 별도의 레이어를 구현하지 않고 BERT 클래스에서 간단히 포함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "class ScaleDotProductAttention(tf.keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    Scale Dot Product Attention Class\n",
    "    \"\"\"\n",
    "    def __init__(self, name=\"scale_dot_product_attention\"):\n",
    "        \"\"\"\n",
    "        생성자\n",
    "        :param name: layer name\n",
    "        \"\"\"\n",
    "        super().__init__(name=name)\n",
    "\n",
    "    def call(self, Q, K, V, attn_mask):\n",
    "        \"\"\"\n",
    "        layer 실행\n",
    "        :param Q: Q value\n",
    "        :param K: K value\n",
    "        :param V: V value\n",
    "        :param attn_mask: 실행 모드\n",
    "        :return attn_out: attention 실행 결과\n",
    "        \"\"\"\n",
    "        attn_score = tf.matmul(Q, K, transpose_b=True)\n",
    "        scale = tf.math.sqrt(tf.cast(tf.shape(K)[-1], tf.float32))\n",
    "        attn_scale = tf.math.divide(attn_score, scale)\n",
    "        attn_scale -= 1.e9 * attn_mask\n",
    "        attn_prob = tf.nn.softmax(attn_scale, axis=-1)\n",
    "        attn_out = tf.matmul(attn_prob, V)\n",
    "        return attn_out\n",
    "print(\"슝=3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "class MultiHeadAttention(tf.keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    Multi Head Attention Class\n",
    "    \"\"\"\n",
    "    def __init__(self, config, name=\"multi_head_attention\"):\n",
    "        \"\"\"\n",
    "        생성자\n",
    "        :param config: Config 객체\n",
    "        :param name: layer name\n",
    "        \"\"\"\n",
    "        super().__init__(name=name)\n",
    "\n",
    "        self.d_model = config.d_model\n",
    "        self.n_head = config.n_head\n",
    "        self.d_head = config.d_head\n",
    "\n",
    "        # Q, K, V input dense layer\n",
    "        self.W_Q = tf.keras.layers.Dense(config.n_head * config.d_head, kernel_initializer=kernel_initializer(), bias_initializer=bias_initializer())\n",
    "        self.W_K = tf.keras.layers.Dense(config.n_head * config.d_head, kernel_initializer=kernel_initializer(), bias_initializer=bias_initializer())\n",
    "        self.W_V = tf.keras.layers.Dense(config.n_head * config.d_head, kernel_initializer=kernel_initializer(), bias_initializer=bias_initializer())\n",
    "        # Scale Dot Product Attention class\n",
    "        self.attention = ScaleDotProductAttention(name=\"self_attention\")\n",
    "        # output dense layer\n",
    "        self.W_O = tf.keras.layers.Dense(config.d_model, kernel_initializer=kernel_initializer(), bias_initializer=bias_initializer())\n",
    "\n",
    "    def call(self, Q, K, V, attn_mask):\n",
    "        \"\"\"\n",
    "        layer 실행\n",
    "        :param Q: Q value\n",
    "        :param K: K value\n",
    "        :param V: V value\n",
    "        :param attn_mask: 실행 모드\n",
    "        :return attn_out: attention 실행 결과\n",
    "        \"\"\"\n",
    "        # reshape Q, K, V, attn_mask\n",
    "        batch_size = tf.shape(Q)[0]\n",
    "        Q_m = tf.transpose(tf.reshape(self.W_Q(Q), [batch_size, -1, self.n_head, self.d_head]), [0, 2, 1, 3])  # (bs, n_head, Q_len, d_head)\n",
    "        K_m = tf.transpose(tf.reshape(self.W_K(K), [batch_size, -1, self.n_head, self.d_head]), [0, 2, 1, 3])  # (bs, n_head, K_len, d_head)\n",
    "        V_m = tf.transpose(tf.reshape(self.W_V(V), [batch_size, -1, self.n_head, self.d_head]), [0, 2, 1, 3])  # (bs, n_head, K_len, d_head)\n",
    "        attn_mask_m = tf.expand_dims(attn_mask, axis=1)\n",
    "        # Scale Dot Product Attention with multi head Q, K, V, attn_mask\n",
    "        attn_out = self.attention(Q_m, K_m, V_m, attn_mask_m)  # (bs, n_head, Q_len, d_head)\n",
    "        # transpose and liner\n",
    "        attn_out_m = tf.transpose(attn_out, perm=[0, 2, 1, 3])  # (bs, Q_len, n_head, d_head)\n",
    "        attn_out = tf.reshape(attn_out_m, [batch_size, -1, config.n_head * config.d_head])  # (bs, Q_len, d_model)\n",
    "        attn_out = self.W_O(attn_out) # (bs, Q_len, d_model)\n",
    "\n",
    "        return attn_out\n",
    "print(\"슝=3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "# transformer encoder 레이어 구성\n",
    "class PositionWiseFeedForward(tf.keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    Position Wise Feed Forward Class\n",
    "    \"\"\"\n",
    "    def __init__(self, config, name=\"feed_forward\"):\n",
    "        \"\"\"\n",
    "        생성자\n",
    "        :param config: Config 객체\n",
    "        :param name: layer name\n",
    "        \"\"\"\n",
    "        super().__init__(name=name)\n",
    "\n",
    "        self.W_1 = tf.keras.layers.Dense(config.d_ff, activation=gelu, kernel_initializer=kernel_initializer(), bias_initializer=bias_initializer())\n",
    "        self.W_2 = tf.keras.layers.Dense(config.d_model, kernel_initializer=kernel_initializer(), bias_initializer=bias_initializer())\n",
    "\n",
    "    def call(self, inputs):\n",
    "        \"\"\"\n",
    "        layer 실행\n",
    "        :param inputs: inputs\n",
    "        :return ff_val: feed forward 실행 결과\n",
    "        \"\"\"\n",
    "        ff_val = self.W_2(self.W_1(inputs))\n",
    "        return ff_val\n",
    "print(\"슝=3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "class EncoderLayer(tf.keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    Encoder Layer Class\n",
    "    \"\"\"\n",
    "    def __init__(self, config, name=\"encoder_layer\"):\n",
    "        \"\"\"\n",
    "        생성자\n",
    "        :param config: Config 객체\n",
    "        :param name: layer name\n",
    "        \"\"\"\n",
    "        super().__init__(name=name)\n",
    "\n",
    "        self.self_attention = MultiHeadAttention(config)\n",
    "        self.norm1 = tf.keras.layers.LayerNormalization(epsilon=config.layernorm_epsilon)\n",
    "\n",
    "        self.ffn = PositionWiseFeedForward(config)\n",
    "        self.norm2 = tf.keras.layers.LayerNormalization(epsilon=config.layernorm_epsilon)\n",
    "\n",
    "        self.dropout = tf.keras.layers.Dropout(config.dropout)\n",
    " \n",
    "    def call(self, enc_embed, self_mask):\n",
    "        \"\"\"\n",
    "        layer 실행\n",
    "        :param enc_embed: enc_embed 또는 이전 EncoderLayer의 출력\n",
    "        :param self_mask: enc_tokens의 pad mask\n",
    "        :return enc_out: EncoderLayer 실행 결과\n",
    "        \"\"\"\n",
    "        self_attn_val = self.self_attention(enc_embed, enc_embed, enc_embed, self_mask)\n",
    "        norm1_val = self.norm1(enc_embed + self.dropout(self_attn_val))\n",
    "\n",
    "        ffn_val = self.ffn(norm1_val)\n",
    "        enc_out = self.norm2(norm1_val + self.dropout(ffn_val))\n",
    "\n",
    "        return enc_out\n",
    "print(\"슝=3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "# BERT 레이어 조합\n",
    "class BERT(tf.keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    BERT Class\n",
    "    \"\"\"\n",
    "    def __init__(self, config, name=\"bert\"):\n",
    "        \"\"\"\n",
    "        생성자\n",
    "        :param config: Config 객체\n",
    "        :param name: layer name\n",
    "        \"\"\"\n",
    "        super().__init__(name=name)\n",
    "\n",
    "        self.i_pad = config.i_pad\n",
    "        self.embedding = SharedEmbedding(config)\n",
    "        self.position = PositionalEmbedding(config)\n",
    "        self.segment = tf.keras.layers.Embedding(2, config.d_model, embeddings_initializer=kernel_initializer())\n",
    "        self.norm = tf.keras.layers.LayerNormalization(epsilon=config.layernorm_epsilon)\n",
    "        \n",
    "        self.encoder_layers = [EncoderLayer(config, name=f\"encoder_layer_{i}\") for i in range(config.n_layer)]\n",
    "\n",
    "        self.dropout = tf.keras.layers.Dropout(config.dropout)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        \"\"\"\n",
    "        layer 실행\n",
    "        :param inputs: (enc_tokens, segments)\n",
    "        :return logits: dec_tokens에 대한 다음 토큰 예측 결과 logits\n",
    "        \"\"\"\n",
    "        enc_tokens, segments = inputs\n",
    "\n",
    "        enc_self_mask = tf.keras.layers.Lambda(get_pad_mask, output_shape=(1, None), name='enc_self_mask')(enc_tokens, self.i_pad)\n",
    "\n",
    "        enc_embed = self.get_embedding(enc_tokens, segments)\n",
    "\n",
    "        enc_out = self.dropout(enc_embed)\n",
    "        for encoder_layer in self.encoder_layers:\n",
    "            enc_out = encoder_layer(enc_out, enc_self_mask)\n",
    "\n",
    "        logits_cls = enc_out[:,0]\n",
    "        logits_lm = self.embedding(enc_out, mode=\"linear\")\n",
    "        return logits_cls, logits_lm\n",
    "    \n",
    "    def get_embedding(self, tokens, segments):\n",
    "        \"\"\"\n",
    "        token embedding, position embedding lookup\n",
    "        :param tokens: 입력 tokens\n",
    "        :param segments: 입력 segments\n",
    "        :return embed: embedding 결과\n",
    "        \"\"\"\n",
    "        embed = self.embedding(tokens) + self.position(tokens) + self.segment(segments)\n",
    "        embed = self.norm(embed)\n",
    "        return embed\n",
    "print(\"슝=3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "# Pretrain 용 BERT 모델 구성\n",
    "# Encoder Layer class 정의\n",
    "class PooledOutput(tf.keras.layers.Layer):\n",
    "    def __init__(self, config, n_output, name=\"pooled_output\"):\n",
    "        super().__init__(name=name)\n",
    "\n",
    "        self.dense1 = tf.keras.layers.Dense(config.d_model, activation=tf.nn.tanh, kernel_initializer=kernel_initializer(), bias_initializer=bias_initializer())\n",
    "        self.dense2 = tf.keras.layers.Dense(n_output, use_bias=False, activation=tf.nn.softmax, name=\"nsp\", kernel_initializer=kernel_initializer(), bias_initializer=bias_initializer())\n",
    " \n",
    "    def call(self, inputs):\n",
    "        outputs = self.dense1(inputs)\n",
    "        outputs = self.dense2(outputs)\n",
    "        return outputs\n",
    "print(\"슝=3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "def build_model_pre_train(config):\n",
    "    enc_tokens = tf.keras.layers.Input((None,), name=\"enc_tokens\")\n",
    "    segments = tf.keras.layers.Input((None,), name=\"segments\")\n",
    "\n",
    "    bert = BERT(config)\n",
    "    logits_cls, logits_lm = bert((enc_tokens, segments))\n",
    "\n",
    "    logits_cls = PooledOutput(config, 2, name=\"pooled_nsp\")(logits_cls)\n",
    "    outputs_nsp = tf.keras.layers.Softmax(name=\"nsp\")(logits_cls)\n",
    "\n",
    "    outputs_mlm = tf.keras.layers.Softmax(name=\"mlm\")(logits_lm)\n",
    "\n",
    "    model = tf.keras.Model(inputs=(enc_tokens, segments), outputs=(outputs_nsp, outputs_mlm))\n",
    "    return model\n",
    "print(\"슝=3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'d_model': 256,\n",
       " 'n_head': 4,\n",
       " 'd_head': 64,\n",
       " 'dropout': 0.1,\n",
       " 'd_ff': 1024,\n",
       " 'layernorm_epsilon': 0.001,\n",
       " 'n_layer': 3,\n",
       " 'n_seq': 256,\n",
       " 'n_vocab': 32007,\n",
       " 'i_pad': 0}"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 프리트레인용 버트 모델(테스트용) 생성 \n",
    "config = Config({\"d_model\": 256, \"n_head\": 4, \"d_head\": 64, \"dropout\": 0.1, \"d_ff\": 1024, \"layernorm_epsilon\": 0.001, \"n_layer\": 3, \"n_seq\": 256, \"n_vocab\": 0, \"i_pad\": 0})\n",
    "config.n_vocab = len(vocab)\n",
    "config.i_pad = vocab.pad_id()\n",
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x7f6ef4bd5710> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x7f6ef4bd5710> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aiffel/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/indexed_slices.py:432: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  else:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 7ms/step - loss: 11.2227 - nsp_loss: 0.7743 - mlm_loss: 10.4484 - nsp_acc: 0.5000 - mlm_acc: 0.0000e+00\n",
      "Epoch 2/2\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 10.0172 - nsp_loss: 0.6251 - mlm_loss: 9.3921 - nsp_acc: 0.8000 - mlm_acc: 0.0200\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f6ef4e1b9d0>"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 프리트레인용 버트 모델(테스트용) 학습\n",
    "n_seq = 10\n",
    "\n",
    "# make test inputs\n",
    "enc_tokens = np.random.randint(0, len(vocab), (10, n_seq))\n",
    "segments = np.random.randint(0, 2, (10, n_seq))\n",
    "labels_nsp = np.random.randint(0, 2, (10,))\n",
    "labels_mlm = np.random.randint(0, len(vocab), (10, n_seq))\n",
    "\n",
    "test_model = build_model_pre_train(config)\n",
    "test_model.compile(loss=tf.keras.losses.sparse_categorical_crossentropy, optimizer=tf.keras.optimizers.Adam(), metrics=[\"acc\"])\n",
    "\n",
    "# test model fit\n",
    "test_model.fit((enc_tokens, segments), (labels_nsp, labels_mlm), epochs=2, batch_size=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### pretrain 진행\n",
    "- loss나 accuracy같이 기본적으로 필요한 계산 함수 미리 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def lm_loss(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    loss 계산 함수\n",
    "    :param y_true: 정답 (bs, n_seq)\n",
    "    :param y_pred: 예측 값 (bs, n_seq, n_vocab)\n",
    "    \"\"\"\n",
    "    # loss 계산\n",
    "    loss = tf.keras.losses.SparseCategoricalCrossentropy(reduction=tf.keras.losses.Reduction.NONE)(y_true, y_pred)\n",
    "    # pad(0) 인 부분 mask\n",
    "    mask = tf.cast(tf.math.not_equal(y_true, 0), dtype=loss.dtype)\n",
    "    loss *= mask\n",
    "    return loss * 20  # mlm을 더 잘 학습하도록 20배 증가 시킴\n",
    "print(\"슝=3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "def lm_acc(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    acc 계산 함수\n",
    "    :param y_true: 정답 (bs, n_seq)\n",
    "    :param y_pred: 예측 값 (bs, n_seq, n_vocab)\n",
    "    \"\"\"\n",
    "    # 정답 여부 확인\n",
    "    y_pred_class = tf.cast(K.argmax(y_pred, axis=-1), tf.float32)\n",
    "    matches = tf.cast(K.equal(y_true, y_pred_class), tf.float32)\n",
    "    # pad(0) 인 부분 mask\n",
    "    mask = tf.cast(tf.math.not_equal(y_true, 0), dtype=matches.dtype)\n",
    "    matches *= mask\n",
    "    # 정확도 계산\n",
    "    accuracy = K.sum(matches) / K.maximum(K.sum(mask), 1)\n",
    "    return accuracy\n",
    "print(\"슝=3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "# Learning Rate 스케줄링 구현\n",
    "# WarmUp 이후 consine 형태로 감소하는 스케줄로 적용\n",
    "class CosineSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "    \"\"\"\n",
    "    CosineSchedule Class\n",
    "    \"\"\"\n",
    "    def __init__(self, train_steps=4000, warmup_steps=2000, max_lr=2.5e-4):\n",
    "        \"\"\"\n",
    "        생성자\n",
    "        :param train_steps: 학습 step 총 합\n",
    "        :param warmup_steps: warmup steps\n",
    "        :param max_lr: 최대 learning rate\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        assert 0 < warmup_steps < train_steps\n",
    "        self.warmup_steps = warmup_steps\n",
    "        self.train_steps = train_steps\n",
    "        self.max_lr = max_lr\n",
    "\n",
    "    def __call__(self, step_num):\n",
    "        \"\"\"\n",
    "        learning rate 계산\n",
    "        :param step_num: 현재 step number\n",
    "        :retrun: 계산된 learning rate\n",
    "        \"\"\"\n",
    "        state = tf.cast(step_num <= self.warmup_steps, tf.float32)\n",
    "        lr1 = tf.cast(step_num, tf.float32) / self.warmup_steps\n",
    "        progress = tf.cast(step_num - self.warmup_steps, tf.float32) / max(1, self.train_steps - self.warmup_steps)\n",
    "        lr2 = 0.5 * (1.0 + tf.math.cos(math.pi * progress))\n",
    "        return (state * lr1 + (1 - state) * lr2) * self.max_lr\n",
    "print(\"슝=3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAkYAAAGwCAYAAABM/qr1AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/d3fzzAAAACXBIWXMAAA9hAAAPYQGoP6dpAABWXElEQVR4nO3de1hU1f4/8PdwmQFFGI2cAQPFG17ybhBm2ckpPJlpmSZxlMgkO3Y7dkqto2bn4vVUx7K0q31P/TQ9lpaXyvCWiqh4RYzUNE0FE2QABVHm8/tjyegoKKPAnsv79TzzzGb2mpnPctB5u/daa+tEREBERERE8NG6ACIiIiJXwWBEREREdAGDEREREdEFDEZEREREFzAYEREREV3AYERERER0AYMRERER0QV+Whfgbmw2G44dO4YGDRpAp9NpXQ4RERFVg4igqKgI4eHh8PGp+rgQg5GTjh07hoiICK3LICIioutw5MgR3HLLLVXuZzByUoMGDQCoP9jg4GCNqyEiIqLqKCwsREREhP17vCoMRk6qOH0WHBzMYERERORmrjUMhoOviYiIiC5gMCIiIiK6gMGIiIiI6AIGIyIiIqILGIyIiIiILmAwIiIiIrqAwYiIiIjoAgYjIiIiogsYjIiIiIguYDAiIiIiuuC6gtGsWbPQrFkzBAQEIDY2Fps3b75q+4ULF6JNmzYICAhAhw4dsHz5cof9IoIJEyYgLCwMgYGBsFgs2Ldvn0Ob/Px8JCYmIjg4GEajEcOHD0dxcbF9/5o1a9C/f3+EhYWhfv366Ny5Mz7//HOH15g7dy50Op3DLSAg4Hr+CIiIiMgDOR2MvvjiC4wePRoTJ07Etm3b0KlTJ8THx+PEiROVtt+4cSMSEhIwfPhwbN++HQMGDMCAAQOQmZlpbzNt2jTMnDkTs2fPRnp6OurXr4/4+HiUlpba2yQmJmLPnj1YuXIlli5dinXr1iElJcXhfTp27IhFixZh165dSE5OxrBhw7B06VKHeoKDg3H8+HH77ddff3X2j4CIiIg8lTgpJiZGRo0aZf+5vLxcwsPDZfLkyZW2Hzx4sPTt29fhsdjYWHnqqadERMRms4nZbJbp06fb9xcUFIjBYJB58+aJiEhWVpYAkC1bttjbrFixQnQ6nRw9erTKWu+//35JTk62//zJJ59ISEhI9TtbCavVKgDEarXe0Ot4pYICkWPHRE6eFCksFCktFbHZtK6KiIi8QHW/v506YlRWVoaMjAxYLBb7Yz4+PrBYLEhLS6v0OWlpaQ7tASA+Pt7e/uDBg8jJyXFoExISgtjYWHubtLQ0GI1GdO/e3d7GYrHAx8cH6enpVdZrtVrRqFEjh8eKi4vRtGlTREREoH///tizZ89V+3z27FkUFhY63Og6bNgA3HQTEB4OhIYCwcFAQADg56d+jo4GevQA+vUDUlKAf/0L+Pxz9byjRwERrXtARERewM+ZxidPnkR5eTlMJpPD4yaTCT/99FOlz8nJyam0fU5Ojn1/xWNXa9O4cWPHwv380KhRI3ubyy1YsABbtmzBnDlz7I9FR0fj448/RseOHWG1WjFjxgz06NEDe/bswS233FLp60yePBmTJk2qdB854dtvgfLyKx+32YC8PHW7mgYNgFtvvXjr2BHo3h0ICqqdeomIyCs5FYzcxerVq5GcnIwPPvgA7du3tz8eFxeHuLg4+889evRA27ZtMWfOHPz973+v9LXGjRuH0aNH238uLCxERERE7RXvqSqOzL35JvDss0BZmbqdPg3k518MR3l56gjRoUPAr7+q+yNHgKIiIC1N3Sr4+KiAdPvtQFwccNddQLNmGnSOiIg8hVPBKDQ0FL6+vsjNzXV4PDc3F2azudLnmM3mq7avuM/NzUVYWJhDm86dO9vbXD64+/z588jPz7/ifdeuXYt+/frhzTffxLBhw67aH39/f3Tp0gX79++vso3BYIDBYLjq61A1VASj9u0BX18gMFDdQkLU6bWrKSsDfv4ZyMxUr7N7N5CRAfz2G7Bjh7rNnq3atmwJ3HsvYLEA99wDGI212CkiIvI0To0x0uv16NatG1JTU+2P2Ww2pKamOhyJuVRcXJxDewBYuXKlvX1UVBTMZrNDm8LCQqSnp9vbxMXFoaCgABkZGfY2q1atgs1mQ2xsrP2xNWvWoG/fvpg6darDjLWqlJeXY/fu3Q6BjGpBaSlQET4vOYJXbXq9On02ZAjw978Dixero0hHjgD/+x/w4ovqiJGvr3qf994DBg5UY5fuuQd45x0VooiIiK7F2VHd8+fPF4PBIHPnzpWsrCxJSUkRo9EoOTk5IiIydOhQGTt2rL39hg0bxM/PT2bMmCF79+6ViRMnir+/v+zevdveZsqUKWI0GmXJkiWya9cu6d+/v0RFRUlJSYm9TZ8+faRLly6Snp4u69evl1atWklCQoJ9/6pVq6RevXoybtw4OX78uP2Wl5dnbzNp0iT57rvv5MCBA5KRkSFDhgyRgIAA2bNnT7X7z1lp12HHDhFAxGis3VloVqvIkiUizzwj0qaNes9Lb7fdJjJ1qsiRI7VXAxERuaTqfn87HYxERN5++22JjIwUvV4vMTExsmnTJvu+Xr16SVJSkkP7BQsWSOvWrUWv10v79u1l2bJlDvttNpuMHz9eTCaTGAwG6d27t2RnZzu0ycvLk4SEBAkKCpLg4GBJTk6WoqIi+/6kpCQBcMWtV69e9jYvvPCCvW6TyST333+/bNu2zam+Mxhdh88/V8Hkjjvq9n0PHBCZMUO9r053MSDpdCK9e4t8+qnIJb9DRETkuar7/a0T4TxoZxQWFiIkJARWqxXBwcFal+MeXn1VTb9PSQEumSVYp3Jy1Cm4efOAdesuPl6vHjBoEPDnPwMxMdrURkREta6639+8VhrVvksHXmvFbAZGjgTWrgV++QV4/XU1UPvMGeDTT4HYWOC224BPPgFKSrSrk4iINMVgRLXPFYLRpaKigPHj1Uy3DRuAoUPVAO+tW4EnngBuuQUYOxY4flzrSomIqI4xGFHtKikBDhxQ264SjCrodGq17f/7PzVrbcoUoGlTta7S1KlqTaSUFOCyCxoTEZHnYjCi2vXTT2rIc6NGwGWrm7uUm28GxoxRIW7xYhWYysqADz5Qlyt55BFg2zatqyQiolrGYES169LTaDqdtrVUh68v0L+/OsX244/AAw+oYLdoEdCtG/Dww2qhSSIi8kgMRlS7XG18kTN69gS++UattJ2YqILdV1+py5A89pgao0RERB6FwYhqlzsHowq33gp89pk6UvTII+oI0rx5QNu2wJNPcpA2EZEHYTCi2uUJwahCu3bAwoVqrNEDDwA2G/DRR0CrVsA//8lp/kREHoDBiGrPmTPAwYNq2xOCUYUuXdQptvXr1aKQp08Df/ubGqT9//6fOqJERERuicGIak/FjLTQUKBxY62rqXl33AGkpakwFBGhLmqbmKhmtO3YoXV1RER0HRiMqPZUnEZr107bOmqTjw+QkABkZ6vTaUFBwKZNagbbX/4CFBVpXSERETmBwYhqjyeNL7qWwEDglVfUUbLBg9X4o7feAtq0UeOSeHqNiMgtMBhR7fGmYFShSRPgiy+Ab78FWrQAjh1TQen++4HDh7WujoiIroHBiGqPNwajCvHxav2jCRPUddi+/VZN+3//fR49IiJyYQxGVDtOn/bMGWnOCAwEJk0Cdu1SA7KLioCnngLuuw84dEjr6oiIqBIMRlQ79u5V9zffrG7eLDoaWLcOeOMNFZZ++AHo0AF47z01FomIiFwGgxHVDm8+jVYZX181S23nTnWpkeJi4M9/VmOPuHI2EZHLYDCi2sFgVLlWrYC1a4H//AcICAC++05de+3rr7WujIiIwGBEtYXBqGo+PsBzzwEZGUCnTsDJk0D//sDTT6vVwomISDMMRlQ7GIyurV07ID0dePFF9fPs2WphyO3bta2LiMiLMRhRzSsuBn79VW0zGF2dwQDMmAGsXAmEh6sFIuPigDlzOK2fiEgDDEZU87Ky1L3JBNx0k7a1uAuLRU3rf/BB4OxZYORI4E9/UiGTiIjqDIMR1TyeRrs+N90ELF4MTJ+uZrH9v/8H3HbbxT9PIiKqdQxGVPMYjK6fTgf89a/AmjUXT63FxAD//a/WlREReQUGI6p5DEY3rmdPNQjbYlEz1YYNU+selZVpXRkRkUdjMKKax2BUMxo3VtdYmzhR/fzee8C99wInTmhbFxGRB2MwoppVWAgcOaK2GYxunK8v8NprwJIlQIMG6tIi3bsD27ZpXRkRkUdiMKKaVTEjLSwMaNhQ21o8yYMPqjWPWrVSwbNnT2DePK2rIiLyOAxGVLN4Gq32tG0LbN4M9OkDlJQAjz0GjBkDlJdrXRkRkcdgMKKaxWBUu4xGYOlSFYgAYNo0YOBA4PRpTcsiIvIUDEZUsxiMap+vLzBlCvD552rl7CVLgF69gOPHta6MiMjtMRhRzaoYY8RgVPseewxITQVCQ9UFaWNjgd27ta6KiMitMRhRzbFagd9+U9vt2mlbi7e44w5g0yYgOloNyr7jDuC777SuiojIbTEYUc2pOFoUHq7GwlDdaNEC2LhRnU4rKgL69gVmz9a6KiIit8RgRDWH44u006gR8P33aoXs8nLg6aeBv/0NENG6MiIit8JgRDWHwUhbej0wdy4waZL6+Z//BEaMAM6f17QsIiJ3wmBENYfBSHs6HTBhAvD++4CPD/DRR8Ajj6h1j4iI6JoYjKjmMBi5jhEjgP/97+J0/vvuA06d0roqIiKXx2BENaOgADh2TG1zRppreOghNe4oJARYvx64666LnxEREVWKwYhqRsXRoltuUV/E5BruuktdeDYsDMjMBHr0APbt07oqIiKXxWBENYOn0VxXx45qOn/r1sCvvwJ33qlCEhERXYHBiGoGg5Fra9YM+PFHFZJyc9WaRxkZWldFRORyGIyoZjAYub7GjYHVq4GYGCA/H7jnHmDDBq2rIiJyKQxGVDMYjNxDo0bADz+osUeFhWq22g8/aF0VEZHLYDCiG5efD+TkqG3OSHN9DRoAK1YAffoAZ84ADzwAfPON1lUREbkEBiO6cRVHiyIj1Zcuub569YDFi9WU/rNngYcfBhYu1LoqIiLNMRjRjeNpNPdkMAALFgCJieqyIQkJDEdE5PUYjOjGMRi5Lz8/4NNPgaQkdfFZhiMi8nJ+WhdAHoDByL35+qprqgEqJCUkqO1Bg7SriYhIIwxGdOMYjNxfZeFIp1MXoCUi8iI8lUY35uRJ4MQJtd22rba10I2pCEfDhqnTakOGqAvREhF5EQYjujEVR4uaNQOCgjQthWqAry/w8ccMR0TktRiM6MbwNJrnqSwcLV6sdVVERHWCwYhuTFaWumcw8iwV4WjoUBWOHn0U+O47rasiIqp1DEZ0Y3jEyHNVhKNBg4CyMrUY5Lp1WldFRFSrGIzoxlQEI14KxDP5+QGffQb07QuUlKj7zZu1roqIqNYwGNH1+/13dQM4I82T6fVq0cd77gGKi4H4eGDnTq2rIiKqFdcVjGbNmoVmzZohICAAsbGx2HyN/0EuXLgQbdq0QUBAADp06IDly5c77BcRTJgwAWFhYQgMDITFYsG+ffsc2uTn5yMxMRHBwcEwGo0YPnw4iouL7fvXrFmD/v37IywsDPXr10fnzp3x+eefO10LOaHiaFFUFFC/vra1UO0KDASWLAF69AAKCoB77wV++knrqoiIapzTweiLL77A6NGjMXHiRGzbtg2dOnVCfHw8TlSsZXOZjRs3IiEhAcOHD8f27dsxYMAADBgwAJmZmfY206ZNw8yZMzF79mykp6ejfv36iI+PR2lpqb1NYmIi9uzZg5UrV2Lp0qVYt24dUlJSHN6nY8eOWLRoEXbt2oXk5GQMGzYMS5cudaoWcgLHF3mXoCBg2TKga1d1pLB3b+CXX7SuioioZomTYmJiZNSoUfafy8vLJTw8XCZPnlxp+8GDB0vfvn0dHouNjZWnnnpKRERsNpuYzWaZPn26fX9BQYEYDAaZN2+eiIhkZWUJANmyZYu9zYoVK0Sn08nRo0errPX++++X5OTkatdSmdLSUrFarfbbkSNHBIBYrdYqn+M1nn5aBBAZM0brSqgu/f67SLt26rNv1kzkKn8HiYhchdVqrdb3t1NHjMrKypCRkQGLxWJ/zMfHBxaLBWlpaZU+Jy0tzaE9AMTHx9vbHzx4EDk5OQ5tQkJCEBsba2+TlpYGo9GI7t2729tYLBb4+PggPT29ynqtVisaNWpU7VoqM3nyZISEhNhvERERVbb1Ojxi5J1CQ4EffgBatgQOHVJjjk6d0roqIqIa4VQwOnnyJMrLy2EymRweN5lMyMnJqfQ5OTk5V21fcX+tNo0bN3bY7+fnh0aNGlX5vgsWLMCWLVuQnJxc7VoqM27cOFitVvvtyJEjVbb1KiIMRt4sLAz4/nt1n5kJ9OsHnDmjdVVERDfMI2elrV69GsnJyfjggw/Q/ga/tA0GA4KDgx1uBHV9tLw8daHRNm20roa0EBWlFn00GoENG4DBg4Fz57SuiojohjgVjEJDQ+Hr64vc3FyHx3Nzc2E2myt9jtlsvmr7ivtrtbl8cPf58+eRn59/xfuuXbsW/fr1w5tvvolhw4Y5VQs5oeJoUfPmQL162tZC2unQAfjmGyAgQA3MHj4csNm0roqI6Lo5FYz0ej26deuG1NRU+2M2mw2pqamIi4ur9DlxcXEO7QFg5cqV9vZRUVEwm80ObQoLC5Genm5vExcXh4KCAmRkZNjbrFq1CjabDbGxsfbH1qxZg759+2Lq1KkOM9aqWws5gafRqELPnmqdI19f4L//BV56SZ1qJSJyR86O6p4/f74YDAaZO3euZGVlSUpKihiNRsnJyRERkaFDh8rYsWPt7Tds2CB+fn4yY8YM2bt3r0ycOFH8/f1l9+7d9jZTpkwRo9EoS5YskV27dkn//v0lKipKSkpK7G369OkjXbp0kfT0dFm/fr20atVKEhIS7PtXrVol9erVk3Hjxsnx48ftt7y8PKdquZbqjmr3eE89pWYljRundSXkKj79VP1OACJTpmhdDRGRg+p+fzsdjERE3n77bYmMjBS9Xi8xMTGyadMm+75evXpJUlKSQ/sFCxZI69atRa/XS/v27WXZsmUO+202m4wfP15MJpMYDAbp3bu3ZGdnO7TJy8uThIQECQoKkuDgYElOTpaioiL7/qSkJAFwxa1Xr15O1XItDEYX9OypvgA/+0zrSsiV/PvfF8PRhx9qXQ0RkV11v791Ijzm7YzCwkKEhITAarV670BsEeCmm9QU7e3bgc6dta6IXMnYscDUqYCPD/DVV8CDD2pdERFRtb+/PXJWGtWynBwVinx8OCONrjR5MvDEE2oQ9pAhwFXWGiMicjUMRuS8ioHXLVqo2UhEl9LpgNmzgT59gJIStcbRgQNaV0VEVC0MRuQ8zkija/H3VzPVKq6r1qePuicicnEMRuQ8BiOqjoqLzjZtCuzfr8YacXVsInJxDEbkPAYjqi6zGVixAmjYENi0CUhMBMrLta6KiKhKDEbkHF4jjZzVti2wZAlgMACLFwMvvMAFIInIZTEYkXOOHQOsVrXKcXS01tWQu7jzTrUqNgC88w7w739rWw8RURUYjMg5FUeLWrZURwCIqmvQoIuB6KWXgC++0LYeIqJKMBiRc7Ky1D1Po9H1+MtfgOeeU9vDhgEbNmhbDxHRZRiMyDkVR4zatdO2DnJPOh3wxhvAQw8BZWXAgAHAL79oXRURkR2DETmHA6/pRvn6qvFGXbsCJ08CDzwAFBRoXRUREQAGI3IGZ6RRTalfH/jmG6BJE2DvXmDwYODcOa2rIiJiMCInHD0KFBaq//G3bq11NeTuwsNVOKpfH1i5Enj2WU7jJyLNMRhR9VUcLWrVijPSqGZ06QLMm6fGHs2ZA7z1ltYVEZGXYzCi6uNpNKoN/foBM2ao7RdfVEeRiIg0wmBE1cdgRLXlL38BUlLUqbSEBGDHDq0rIiIvxWBE1cdgRLVFp1MrYlsswOnTaqbasWNaV0VEXojBiKpHhIs7Uu3y9wcWLlTXVjt6FHjwQeDMGa2rIiIvw2BE1XPkCFBUBPj5qcHXRLXBaASWLgVCQ4GMDOCJJzhTjYjqFIMRVU/FabTWrQG9XttayLM1bw58+aUK4V98AfzrX1pXRERehMGIqofji6gu3Xkn8O67avtvfwOWLNG2HiLyGgxGVD0MRlTXRowAnnlGbScmArt3a1sPEXkFBiOqHgYj0sIbbwD33KNmqj34oLq2GhFRLWIwomuz2TgjjbTh7w8sWAC0aAEcOgQ88givqUZEtYrBiK7t8GH1P3Z/f6BlS62rIW9z001qjFGDBsDatcDzz2tdERF5MAYjuraK02jR0SocEdW19u2Bzz9XC0G+9566ERHVAgYjujaOLyJX0K/fxan7zz0HrF6tbT1E5JEYjOjaGIzIVYwZo66ldv48MGgQ8MsvWldERB6GwYiujcGIXIVOB3z0EdC9O5CXB/TvDxQXa10VEXkQBiO6OpsN2LtXbTMYkSsIDAQWLwbMZiAzk5cNIaIaxWBEV/frr+pCnnq9mjJN5AqaNAH+97+LF56dPl3riojIQzAY0dVVnEZr00Zdu4rIVdxxBzBzptoeNw74/ntt6yEij8BgRFdXEYzatdO2DqLKPPUUMHy4OuU7ZAgHYxPRDWMwoqvjwGtyZTod8M47QEwMcOoU8NBDajFSIqLrxGBEV8dgRK4uIABYtAho3BjYtUtdfJaDsYnoOjEYUdU4I43cxS23qEHYfn7AvHnAm29qXRERuSkGI6rawYNASQlgMHBGGrm+u+4C3nhDbb/0ErBqlbb1EJFbYjCiql06I83XV9taiKrjmWeAYcPU0c7Bg9VyE0RETmAwoqpxfBG5G50OmD0b6NpVrYz90EPqqCcRUTUxGFHVGIzIHQUGAl9+CYSGAtu3qyn9HIxNRNXEYERVYzAid9W0KbBggToF/N//qin9RETVwGBElSsvB376SW0zGJE7+sMfLl4qZPRoYONGbeshIrfAYESV++UXoLRUrRETFaV1NUTX54UX1CDs8+eBQYOAEye0roiIXByDEVWu4jRa27ackUbuS6cDPvxQzaw8dkxdNuT8ea2rIiIXxmBEleP4IvIUDRqowdhBQcDq1cDf/qZ1RUTkwhiMqHIMRuRJ2rYFPv5YbU+dCixerGk5ROS6GIyocgxG5GkGDQL+8he1nZQE7NunbT1E5JIYjOhK589zRhp5pqlTgZ49gcJCYOBA4PRprSsiIhfDYERXOnAAKCsD6tUDmjXTuhqimuPvr9Y3MpmA3buBkSO5+CMROWAwoitdOiPNh78i5GHCwi4u/vjZZ+oSIkREF/Bbj67E8UXk6e66S51WA4DnnwfS07Wth4hcBoMRXYnBiLzB6NFqnNG5c8AjjwC//651RUTkAhiM6EpZWeqewYg8mU6npvBHRwO//QY89pi6FA4ReTUGI3J0/jyQna22GYzI0wUHA4sWqYkGP/wATJyodUVEpDEGI3K0f7+akVa/PhAZqXU1RLWvfXt12RAA+Oc/gaVLta2HiDTFYESOOCONvFFCAvDss2p72DDg0CFNyyEi7VzXN9+sWbPQrFkzBAQEIDY2Fps3b75q+4ULF6JNmzYICAhAhw4dsHz5cof9IoIJEyYgLCwMgYGBsFgs2HfZqrT5+flITExEcHAwjEYjhg8fjuLiYvv+0tJSPP744+jQoQP8/PwwYMCAK+pYs2YNdDrdFbecnJzr+WPwTBx4Td5qxgwgJgY4dQoYPBg4e1briohIA04Hoy+++AKjR4/GxIkTsW3bNnTq1Anx8fE4ceJEpe03btyIhIQEDB8+HNu3b8eAAQMwYMAAZGZm2ttMmzYNM2fOxOzZs5Geno769esjPj4epaWl9jaJiYnYs2cPVq5ciaVLl2LdunVISUmx7y8vL0dgYCCee+45WCyWq/YhOzsbx48ft98aN27s7B+D52IwIm+l16v1jRo2BLZsAV56SeuKiEgL4qSYmBgZNWqU/efy8nIJDw+XyZMnV9p+8ODB0rdvX4fHYmNj5amnnhIREZvNJmazWaZPn27fX1BQIAaDQebNmyciIllZWQJAtmzZYm+zYsUK0el0cvTo0SveMykpSfr373/F46tXrxYAcurUqWr3t7S0VKxWq/125MgRASBWq7Xar+FW2rcXAUSWLdO6EiJtLF2q/g4AIgsWaF0NEdUQq9Vare9vp44YlZWVISMjw+GIjI+PDywWC9LS0ip9Tlpa2hVHcOLj4+3tDx48iJycHIc2ISEhiI2NtbdJS0uD0WhE9+7d7W0sFgt8fHyQfh0Ls3Xu3BlhYWG49957sWHDhqu2nTx5MkJCQuy3iIgIp9/PbZw7B/z8s9rmESPyVn37AmPHqu3hwy/+nSAir+BUMDp58iTKy8thMpkcHjeZTFWO08nJyblq+4r7a7W5/HSXn58fGjVq5NT4oLCwMMyePRuLFi3CokWLEBERgbvvvhvbtm2r8jnjxo2D1Wq1344cOVLt93M7+/apcBQUxBlp5N3+/ne1OnZRETBoEFBSonVFRFRH/LQuoC5FR0cjOjra/nOPHj1w4MABvPnmm/jvf/9b6XMMBgMMBkNdlaitivFF7dqpxe+IvJWfHzB/PtC5M7Brl5qxVjGln4g8mlNHjEJDQ+Hr64vc3FyHx3Nzc2E2myt9jtlsvmr7ivtrtbl8cPf58+eRn59f5ftWV0xMDPbv339Dr+ExOPCa6KKwMGDePPWfhI8+Aj79VOuKiKgOOBWM9Ho9unXrhtTUVPtjNpsNqampiIuLq/Q5cXFxDu0BYOXKlfb2UVFRMJvNDm0KCwuRnp5ubxMXF4eCggJkZGTY26xatQo2mw2xsbHOdOEKO3bsQFhY2A29hsdgMCJydM89wKRJavvpp4FLZtMSkWdy+lTa6NGjkZSUhO7duyMmJgZvvfUWTp8+jeTkZADAsGHD0KRJE0yePBkA8Pzzz6NXr17497//jb59+2L+/PnYunUr3n//fQCATqfDCy+8gH/84x9o1aoVoqKiMH78eISHh9vXImrbti369OmDESNGYPbs2Th37hyeeeYZDBkyBOHh4fbasrKyUFZWhvz8fBQVFWHHjh0A1GBrAHjrrbcQFRWF9u3bo7S0FB9++CFWrVqF77///nr//DwLgxHRlV59FdiwAfjuO3Wx2a1b1Tg8IvJM1zPl7e2335bIyEjR6/USExMjmzZtsu/r1auXJCUlObRfsGCBtG7dWvR6vbRv316WXTYV3Gazyfjx48VkMonBYJDevXtLdna2Q5u8vDxJSEiQoKAgCQ4OluTkZCkqKnJo07RpUwFwxa3C1KlTpUWLFhIQECCNGjWSu+++W1atWuVU36s73c/tnD0r4uenpigfPqx1NUSu5cQJkSZN1N+PhAQRm03riojISdX9/taJiGiYy9xOYWEhQkJCYLVaERwcrHU5NSczE+jQQV1Us6CAg6+JLrdhA9CrF1BeDrz3HjBypNYVEZETqvv9zYthkcIZaURXd8cdwNSpavv554FLxjwSkedgMCKF44uIrm30aKB/f6CsTK1vVFCgdUVEVMMYjEhhMCK6Np0O+OQTICoKOHgQSE5WFw8hIo/BYEQKgxFR9TRsCCxcqC46u3gx8OabWldERDWIwYiAs2eBikUuGYyIrq1bN+Ctt9T2mDHAxo2alkNENYfBiIDsbDXTJiQEuGRdKCK6ipEjgSFDgPPngcGDgZMnta6IiGoAgxE5nkbjjDSi6tHpgPffB1q3Bo4eBZKSAJtN66qI6AYxGBGQlaXueRqNyDkNGqjxRgEBwPLlwIwZWldERDeIwYg48JroRnTsCMycqbZfeUUtBElEbovBiBiMiG7Uk08Cjz2mxuoNGQLk5WldERFdJwYjb1daenFGWrt22tZC5K50OmD2bDXe6LffON6IyI0xGHm77Gz1D7jRCISFaV0Nkfu6dLzRsmXAv/+tdUVEdB0YjLwdZ6QR1ZyOHYH//EdtjxvH9Y2I3BCDkbfj+CKimjViBJCQoMYbPfooxxsRuRkGI2/HYERUs3Q6YM4coFUrjjcickMMRt6OwYio5lWMNzIY1HijN97QuiIiqiYGI29WUgIcOKC2GYyIalanThfXNxo7luONiNwEg5E3++knQARo1AgwmbSuhsjzXDreiOsbEbkFBiNvxhlpRLXr0vFGR45wvBGRG2Aw8mYcX0RU+xo0ABYs4HgjIjfBYOTNGIyI6kbnzo7rG6WlaVoOEVWNwcibMRgR1Z2UFDXO6Px5tb5Rfr7WFRFRJRiMvNWZM8DBg2qbwYio9lU23khE66qI6DIMRt5q7171j3JoKNC4sdbVEHmH4OCL442WLuV4IyIXxGDkrXgajUgbnTsDb72ltseOBTZt0rIaIroMg5G3YjAi0s5TT6lxRhxvRORyGIy8FYMRkXZ0OuD994GWLYHDh4HHH+d4IyIXwWDkrRiMiLR16Xijb74B3nxT64qICAxG3qm4GDh0SG0zGBFpp0uXi+ONxozheCMiF8Bg5I1++kndN26sZqURkXY43ojIpTAYeSOeRiNyHZePN0pO5ngjIg0xGHkjBiMi11Ix3kivB77++uLpNSKqcwxG3ojBiMj1XDre6OWXgfR0Tcsh8lYMRt6oIhi1a6dtHUTkaORIYPBgjjci0hCDkbcpLgZ+/VVt84gRkWvR6YAPPgBatFB/TzneiKjOMRh5m6wsdW8yATfdpG0tRHSl4GBg4cKL443+8x+tKyLyKgxG3obji4hcX5cuFxd8fPllYPNmbesh8iIMRt6GwYjIPTz9NDBoEHDunBp3dOqU1hUReQUGI2/DYETkHjjeiEgTDEbehsGIyH2EhFxc32jJEo43IqoDDEbepLAQOHJEbTMYEbmHrl053oioDjEYeZOKGWlhYUDDhtrWQkTVd+l4o0cf5XgjolrEYORNeBqNyD1VjDdq3hw4dIjjjYhqEYORN2EwInJfISEX1zdasuTi6TUiqlEMRt6EwYjIvXXtevF6amPGABs3aloOkSdiMPImDEZE7m/kSGDIkIvXUzt5UuuKiDwKg5G3KCgAjh5V27x4LJH70umA998HWrcGfvsNGDoUsNm0rorIYzAYeYuKGWlNmgBGo6alENENatAA+N//gIAA4NtvgSlTtK6IyGMwGHkLnkYj8iwdOgDvvqu2x48H1qzRtBwiT8Fg5C0YjIg8T3KyutlsQEICkJOjdUVEbo/ByFswGBF5pnfeAW69VYWixx4Dysu1rojIrTEYeQsGIyLPVK+eGm8UFASsXg1MmqR1RURujcHIG5w6BRw/rrY5I43I80RHq5lqAPCPfwDffadtPURujMHIG1TMSIuIAIKDta2FiGpHQoK6ppoI8Kc/qan8ROQ0BiNvwNNoRN7hjTfU6tgnT6rFH8+d07oiIrfDYOQNKoIRT6MRebaAAHU9tZAQdbmQV17RuiIit3NdwWjWrFlo1qwZAgICEBsbi82bN1+1/cKFC9GmTRsEBASgQ4cOWL58ucN+EcGECRMQFhaGwMBAWCwW7Nu3z6FNfn4+EhMTERwcDKPRiOHDh6O4uNi+v7S0FI8//jg6dOgAPz8/DBgwoNJa1qxZg65du8JgMKBly5aYO3fu9fwRuBceMSLyHs2bA598orZnzFAXnCWianM6GH3xxRcYPXo0Jk6ciG3btqFTp06Ij4/HiRMnKm2/ceNGJCQkYPjw4di+fTsGDBiAAQMGIDMz095m2rRpmDlzJmbPno309HTUr18f8fHxKC0ttbdJTEzEnj17sHLlSixduhTr1q1DSkqKfX95eTkCAwPx3HPPwWKxVFrLwYMH0bdvX/zhD3/Ajh078MILL+DJJ5/Ed54+UJHBiMi7PPQQ8Je/qO3HHwcOHtS0HCK3Ik6KiYmRUaNG2X8uLy+X8PBwmTx5cqXtBw8eLH379nV4LDY2Vp566ikREbHZbGI2m2X69On2/QUFBWIwGGTevHkiIpKVlSUAZMuWLfY2K1asEJ1OJ0ePHr3iPZOSkqR///5XPP7yyy9L+/btHR579NFHJT4+/hq9vshqtQoAsVqt1X6OpvLyRNRwTJHCQq2rIaK6cvasyO23q7/73buLlJZqXRGRpqr7/e3UEaOysjJkZGQ4HJHx8fGBxWJBWlpapc9JS0u74ghOfHy8vf3BgweRk5Pj0CYkJASxsbH2NmlpaTAajejevbu9jcVigY+PD9LT06td/7VqqczZs2dRWFjocHMrFUeLIiPV9ZWIyDvo9cAXXwCNGgFbtwJ//avWFRG5BaeC0cmTJ1FeXg6TyeTwuMlkQk4VS9Hn5ORctX3F/bXaNG7c2GG/n58fGjVqVOX7OlNLYWEhSkpKKn3O5MmTERISYr9FRERU+/1cAk+jEXmvyEjgs8/U9jvvAAsWaFsPkRvgrLRrGDduHKxWq/125MgRrUtyDoMRkXf74x8vzk578kng55+1rYfIxTkVjEJDQ+Hr64vc3FyHx3Nzc2E2myt9jtlsvmr7ivtrtbl8cPf58+eRn59f5fs6U0twcDACAwMrfY7BYEBwcLDDza0wGBHRpElAr15AUREwaBBQxRFyInIyGOn1enTr1g2pqan2x2w2G1JTUxEXF1fpc+Li4hzaA8DKlSvt7aOiomA2mx3aFBYWIj093d4mLi4OBQUFyMjIsLdZtWoVbDYbYmNjq13/tWrxSAxGROTnB8ybBzRuDOzaBfz5z2pKBhFdydlR3fPnzxeDwSBz586VrKwsSUlJEaPRKDk5OSIiMnToUBk7dqy9/YYNG8TPz09mzJghe/fulYkTJ4q/v7/s3r3b3mbKlCliNBplyZIlsmvXLunfv79ERUVJSUmJvU2fPn2kS5cukp6eLuvXr5dWrVpJQkKCQ2179uyR7du3S79+/eTuu++W7du3y/bt2+37f/nlF6lXr5689NJLsnfvXpk1a5b4+vrKt99+W+3+u9WstN9/vzgjrahI62qISGurVon4+Kh/E+bM0boaojpV3e9vp4ORiMjbb78tkZGRotfrJSYmRjZt2mTf16tXL0lKSnJov2DBAmndurXo9Xpp3769LFu2zGG/zWaT8ePHi8lkEoPBIL1795bs7GyHNnl5eZKQkCBBQUESHBwsycnJUnTZl33Tpk0FwBW3S61evVo6d+4ser1emjdvLp988olTfXerYLRmjfoHsFkzrSshIlcxZYr6d0GvF9m8WetqiOpMdb+/dSI8nuqMwsJChISEwGq1uv54o3ffBUaNAvr2BZYu1boaInIFIsDDDwOLF6tZaxkZQGio1lUR1brqfn9zVpon4/giIrqcTgfMnQu0agUcPgwkJgLl5VpXReQyGIw8GYMREVUmJARYtAgIDAS+/x54/XWtKyJyGQxGnozBiIiq0qED8P77avv114HLLu5N5K0YjDzViRPAyZPqsHnbtlpXQ0Su6E9/UlP3K7Z5sVkiBiOPVXG0KCoKqFdP21qIyHW98QYQGwucOgUMHMjFH8nrMRh5Kp5GI6LqMBiAhQvVzLTt24FnntG6IiJNMRh5KgYjIqquiAhg/nzAxwf4+GPgww+1rohIMwxGniorS90zGBFRdfTuDfzjH2r7mWfU+kZEXojByBOJ8IgRETlvzBigXz/g7Fk13ig/X+uKiOocg5EnOnECyMtTh8XbtNG6GiJyFz4+wP/9H9CiBfDrr2qmms2mdVVEdYrByBNVHC1q3lwt4EZEVF1G48XFH1esAP7+d60rIqpTDEaeqCIYtWunbR1E5J46dQJmz1bbkybxWovkVRiMPBHHFxHRjRo2TC3+KKKup/bzz1pXRFQnGIw8EYMREdWEN98EevYECguBAQOAoiKtKyKqdQxGnoYz0oiopuj1avHHJk2AvXuBpCQOxiaPx2DkaXJy1NL+nJFGRDXBbFaDsfV64KuvgMmTta6IqFYxGHmaiqNFLVoAAQHa1kJEniE2Fnj3XbU9fjywfLm29RDVIgYjT8PTaERUG4YPB0aOVKfrH3sM2LdP64qIagWDkadhMCKi2vKf/wA9egBWKwdjk8diMPI0DEZEVFv0euB//wPCw9X1GJOT1REkIg/CYORJOCONiGpbWJgajO3vr+6nTNG6IqIaxWDkSY4dU4e4fX2B6GitqyEiT3X77cCsWWr71VeBb7/Vth6iGsRg5Ekqjha1bAkYDNrWQkSebcQIICVFHalOSAAOHNC6IqIawWDkSXgajYjq0syZQFwcUFDAwdjkMRiMPAmDERHVJYNBDcYOCwMyM4GhQ7kyNrk9BiNPwmBERHUtPBxYvFiFpCVL1AKQRG6MwchTiKjpswCDERHVrZgY4KOP1Pa//gXMm6dtPUQ3gMHIU/z2m7oCtp8f0Lq11tUQkbdJTATGjFHbTzwBbNmibT1E14nByFNUnEZr1UotwkZEVNf++U/ggQeA0lI1GPvYMa0rInIag5Gn4Gk0ItKary/w+efq36Fjx4CHHgJKSrSuisgpDEaeggOvicgVBAerQdiNGgGbN6v1jnjZEHIjDEaegsGIiFxFixbAwoUXjyBNm6Z1RUTVxmDkCTgjjYhczT33qAUgAWDcOOCbb7Sth6iaGIw8wZEjasVZPz81+JqIyBX8+c/AyJHqP2+PPXbxyDaRC2Mw8gQV/9i0bq2ueE1E5CpmzgTuvhsoLgb69QN+/13rioiuisHIE3B8ERG5Kn9/ddmQ5s2BgwfVNP7SUq2rIqoSg5EnYDAiIld2003AsmWA0Qhs3AgkJ3OmGrksBiNPwGBERK6uTRtg0SI1FnL+fOC117SuiKhSDEbuzmbjjDQicg/33APMmaO2X38d+O9/ta2HqBIMRu7u8GHg9Gl1Hr9lS62rISK6uieeAMaOVdvDhwPr1mlbD9FlGIzcXcVptOhozkgjIvfwz38CAwcC586py4bs3691RUR2DEbujuOLiMjd+PgA//d/wG23Afn5QN++6p7IBTAYuTsGIyJyR/XqAV9/DURGAj//DDz8MFBWpnVVRAxGbo/BiIjcldkMLF0KNGgArF0LpKRwGj9pjsHIndlswN69apvBiIjcUYcOwIIF6vTap58CkyZpXRF5OQYjd3boEHDmDKDXq6tZExG5oz59gHffVduTJgEffqhtPeTVGIzcWcVptDZt1KJpRETu6qmngFdfVdsjRwLLl2tbD3ktBiN3xvFFRORJ/v53YNgwoLwcGDQI2LpV64rICzEYuTMGIyLyJDod8MEHwL33qmECffsCv/yidVXkZRiM3BmDERF5Gr1eXVOtc2fgxAk1/uj337WuirwIg5G7Ki/njDQi8kwNGqgxRpGRwL59wIMPqiNIRHWAwchdHToElJYCAQFA8+ZaV0NEVLPCwoBvvwUaNgQ2bQIee0z9h5ColjEYuatLZ6T5+mpbCxFRbWjbVq2ObTAAS5YAzzzDBSCp1jEYuSuOLyIib9CzJ/D552pg9uzZwGuvaV0ReTgGI3fFYERE3mLgQGDWLLX9+uvAzJna1kMejcHIXTEYEZE3efppFYoA4Pnn1VEkolpwXcFo1qxZaNasGQICAhAbG4vNmzdftf3ChQvRpk0bBAQEoEOHDlh+2YqmIoIJEyYgLCwMgYGBsFgs2Ldvn0Ob/Px8JCYmIjg4GEajEcOHD0dxcbFDm127duHOO+9EQEAAIiIiMG3aNIf9c+fOhU6nc7gFBARczx+BtsrLgZ9+Utvt2mlbCxFRXfnb34DnnlPbjz8OLFumaTnkmZwORl988QVGjx6NiRMnYtu2bejUqRPi4+Nx4sSJSttv3LgRCQkJGD58OLZv344BAwZgwIAByMzMtLeZNm0aZs6cidmzZyM9PR3169dHfHw8SktL7W0SExOxZ88erFy5EkuXLsW6deuQkpJi319YWIj77rsPTZs2RUZGBqZPn47XXnsN77//vkM9wcHBOH78uP3266+/OvtHoL1ffrk4Iy0qSutqiIjqhk4HvPkmkJgInD8PPPIIsH691lWRpxEnxcTEyKhRo+w/l5eXS3h4uEyePLnS9oMHD5a+ffs6PBYbGytPPfWUiIjYbDYxm80yffp0+/6CggIxGAwyb948ERHJysoSALJlyxZ7mxUrVohOp5OjR4+KiMi7774rDRs2lLNnz9rbjBkzRqKjo+0/f/LJJxISEuJslx1YrVYBIFar9YZe54Z89ZUIINKli3Y1EBFppaxMpG9f9e9gSIjIjh1aV0RuoLrf304dMSorK0NGRgYsFov9MR8fH1gsFqSlpVX6nLS0NIf2ABAfH29vf/DgQeTk5Di0CQkJQWxsrL1NWloajEYjunfvbm9jsVjg4+OD9PR0e5u77roLer3e4X2ys7Nx6tQp+2PFxcVo2rQpIiIi0L9/f+ypGKtThbNnz6KwsNDhpjmOLyIib+bvDyxYANxxB2C1AvHxwIEDWldFHsKpYHTy5EmUl5fDZDI5PG4ymZCTk1Ppc3Jycq7avuL+Wm0aN27ssN/Pzw+NGjVyaFPZa1z6HtHR0fj444+xZMkSfPbZZ7DZbOjRowd+++23Kvs8efJkhISE2G8RERFVtq0zDEZE5O3q1QOWLgU6dgRyc9X11Y4e1boq8gBeNSstLi4Ow4YNQ+fOndGrVy98+eWXuPnmmzFnzpwqnzNu3DhYrVb77ciRI3VYcRUYjIiIAKNRrY7dvDlw8CBgsajrqxHdAKeCUWhoKHx9fZGbm+vweG5uLsxmc6XPMZvNV21fcX+tNpcP7j5//jzy8/Md2lT2Gpe+x+X8/f3RpUsX7N+/v/IOAzAYDAgODna4aer8+Ysz0hiMiMjbhYUBqanALbeofxstFiAvT+uqyI05FYz0ej26deuG1NRU+2M2mw2pqamIi4ur9DlxcXEO7QFg5cqV9vZRUVEwm80ObQoLC5Genm5vExcXh4KCAmRkZNjbrFq1CjabDbGxsfY269atw7lz5xzeJzo6Gg0bNqy0tvLycuzevRthYWHO/DFo68ABoKxMHUZu1kzraoiItNesGbBqFWA2A7t3qzFHVqvWVZG7cnZU9/z588VgMMjcuXMlKytLUlJSxGg0Sk5OjoiIDB06VMaOHWtvv2HDBvHz85MZM2bI3r17ZeLEieLv7y+7d++2t5kyZYoYjUZZsmSJ7Nq1S/r37y9RUVFSUlJib9OnTx/p0qWLpKeny/r166VVq1aSkJBg319QUCAmk0mGDh0qmZmZMn/+fKlXr57MmTPH3mbSpEny3XffyYEDByQjI0OGDBkiAQEBsmfPnmr3X/NZaYsWqZkY3bpp8/5ERK5qzx6R0FD1b2SPHiJFRVpXRC6kut/fTgcjEZG3335bIiMjRa/XS0xMjGzatMm+r1evXpKUlOTQfsGCBdK6dWvR6/XSvn17WbZsmcN+m80m48ePF5PJJAaDQXr37i3Z2dkObfLy8iQhIUGCgoIkODhYkpOTpeiyX/qdO3dKz549xWAwSJMmTWTKlCkO+1944QV73SaTSe6//37Ztm2bU33XPBi9/rr6Sz9smDbvT0TkyrZvFzEa1b+Td98tcuaM1hWRi6ju97dOhJcqdkZhYSFCQkJgtVq1GW80ZAjwxRfA1KnAyy/X/fsTEbm6zZvVWKOiInVabckSwGDQuirSWHW/v71qVppH4Iw0IqKri4kBli9XYzG/+w4YPFiNzSSqBgYjd3LuHJCdrbYZjIiIqtazJ/D11+pI0ddfMxxRtTEYuZP9+1U4ql8fiIzUuhoiItfWu/fF02hLlqhrq509q3VV5OIYjNxJxWm0du0AH350RETXFB+vjhgFBADffAMMHMhwRFfFb1d3wvFFRETOu+8+FYoCAoBly4CHHwZKS7WuilwUg5E7YTAiIro+FosKRYGBamD2Qw8xHFGlGIzcSVaWumcwIiJy3j33XJyt9u23QP/+QEmJ1lWRi2EwchfnzgE//6y2GYyIiK7P3XercFS/PvD990C/fkBxsdZVkQthMHIX+/apcNSgARARoXU1RETuq1cvYMUKIChIXYD2vvuAggKtqyIXwWDkLi6dkabTaVsLEZG7u/NO4IcfgIYNgbQ0dSTpxAmtqyIXwGDkLjjwmoioZsXGAmvXAiYTsHOnCktHjmhdFWmMwchdMBgREdW8Dh2AH39Ui+b+/LNaMXvfPq2rIg0xGLmLS0+lERFRzWnVSoWj1q2Bw4fVkaPdu7WuijTCYOQOysou/g+GR4yIiGpeZCSwbh3QqROQm6sGaG/cqHVVpAEGI3fw88/A+fNAcDBwyy1aV0NE5JlMJmD1auD224FTpy5ea428CoORO+CMNCKiutGwoZqt9sADamXshx8G5szRuiqqQwxG7oADr4mI6k79+sBXXwHDhwM2GzByJDBhAiCidWVUBxiM3AGDERFR3fLzAz74AJg4Uf38978DTz6phjWQR2MwcgcMRkREdU+nA157TZ1K8/EBPv5YXV/t9GmtK6NaxGDk6s6eBfbvV9sMRkREdS8lRZ1aCwxU11m7807g6FGtq6JawmDk6rKzgfJyICQECA/XuhoiIu/04IPAqlXAzTcD27cDMTFARobWVVEtYDBydZeeRuOMNCIi7dx+O7B5s/r3+NgxdeToyy+1ropqGIORq+P4IiIi19GsmVr4sU8foKQEGDgQmDKFM9Y8CIORq2MwIiJyLcHBwDffAM8+q34eNw5ITlZjQsntMRi5OgYjIiLX4+cHzJwJvPMO4OsLfPopcPfd6hQbuTUGI1dWWgocOKC2GYyIiFzPqFFqpprRCGzaBHTtCqxfr3VVdAMYjFzZTz+pVVcbNgTMZq2rISKiytx3H7B1K9Chg7oA7R/+ALz7LscduSkGI1fGGWlERO6hRQsgLQ149FG1OvaoUcATT6gj/+RWGIxcGccXERG5j/r1gXnzgOnT1UrZc+cCPXsCBw9qXRk5gcHIlWVlqXsGIyIi96DTAX/9K/Ddd0CjRmoRyC5duN6RG2EwcmU8YkRE5J4sFmDbNrUopNWq1jt69llO6XcDDEauqqSEM9KIiNxZ06bAunXAyy+rn995B7jjjov/tpNLYjByVT/9pGY03HQT0Lix1tUQEdH18PcHpk4Fli1T/55XnFqbP1/ryqgKDEauijPSiIg8x/33Azt2qMHYRUVAQgLwpz8BBQVaV0aXYTByVRXBqF07besgIqKaccstwOrVwMSJarXszz9Xax+lpmpdGV2CwchVceA1EZHn8fMDXnsN2LABaNUK+O03NVB79GiueeQiGIxcFYMREZHnio0Ftm8HRo5UP7/5JtCtG7B5s7Z1EYORSzpz5uKCYAxGRESeqX594L331MBsk0mtXRcXp44enT6tdXVei8HIFe3dq2akhYZyRhoRkae7/34gM1MNxrbZ1NGjDh2AH37QujKvxGDkingajYjIu4SGAv/9L7B8ORARoc4a3Huvut5afr7W1XkVBiNXxGBEROSd/vhH9R0wapRaquWTT4DoaOCjj9TRJKp1DEauiMGIiMh7NWigVsn+8UegbVvg5EngySfV+KOtW7WuzuMxGLkiBiMiIrrjDmDnTuDf/1ZhafNmICYGSElRYYlqBYORqykuBg4dUtsMRkRE3s3fX81Sy85Wg7NFgA8+UGsgTZumrqtJNYrByNXs3avuGzdWg/GIiIjCwtTg7HXrgE6d1KVExoxR448+/RQoL9e6Qo/BYORqeBqNiIiqcued6kK0c+eq2WtHjgCPP64uTLtihTqiRDeEwcjVMBgREdHV+PoCSUnq9Nq0aYDRCOzerdZD6tGDAekGMRi5GgYjIiKqjsBA4KWXgAMHgBdfBAICgE2bVECKjQWWLmVAug4MRq6GwYiIiJzRqBEwY4ZaFPLFF1Vg2rIF6NcP6N4dmDcPOHdO6yrdBoORKykqAg4fVtsMRkRE5AyzWQWkQ4eAl19W12Lbtg147DGgeXNg+nQ1aJuuisHIlWRlqXuzWf0PgIiIyFmNGwNTp6qA9Prr6uffflNh6ZZbgGefVddmo0oxGLmSimDEo0VERHSjQkOB8eOBX38FPv5YXZj29Gm1qnaHDmol7Y8/Vo+RHYORK+H4IiIiqmkBAUByslpFe+VK4KGHAD8/NVB7+HC1RlJKCrBmDddDAoORa2EwIiKi2qLTARYL8OWXav2jyZOBFi3U+NYPPgD+8AcgMhL4y1+A9HSvndHGYORKGIyIiKgumM3A2LHAzz8DqanAE08AISHAsWPAW28Bt98ONG0K/PnPwPLlXnXpEZ2Il0bC61RYWIiQkBBYrVYEBwfX5AurX0oAOHVKLdhFRERUV86eBb77Dpg/H1iyBDhz5uK+wECgd2/g3nuBXr3UGCUf9zq2Ut3v7+vq1axZs9CsWTMEBAQgNjYWmzdvvmr7hQsXok2bNggICECHDh2wfPlyh/0iggkTJiAsLAyBgYGwWCzYt2+fQ5v8/HwkJiYiODgYRqMRw4cPR3FxsUObXbt24c4770RAQAAiIiIwbdo0p2vRTMXA6/BwhiIiIqp7BgPw4IPA//t/wO+/A998A4wcqWaylZSoBSOffx7o3Bm46SbV9t//VtdvKyzUuvqaI06aP3++6PV6+fjjj2XPnj0yYsQIMRqNkpubW2n7DRs2iK+vr0ybNk2ysrLkb3/7m/j7+8vu3bvtbaZMmSIhISGyePFi2blzpzz44IMSFRUlJSUl9jZ9+vSRTp06yaZNm+THH3+Uli1bSkJCgn2/1WoVk8kkiYmJkpmZKfPmzZPAwECZM2eOU7Vci9VqFQBitVqd+WO7tg8/FAFELJaafV0iIqIbYbOJ7NwpMnmySHy8SFCQ+r66/NaqlciQISL/+pfIokUiu3eLnDmjdfV21f3+dvpUWmxsLG677Ta88847AACbzYaIiAg8++yzGDt27BXtH330UZw+fRpLly61P3b77bejc+fOmD17NkQE4eHhePHFF/HXv/4VAGC1WmEymTB37lwMGTIEe/fuRbt27bBlyxZ0794dAPDtt9/i/vvvx2+//Ybw8HC89957ePXVV5GTkwO9Xg8AGDt2LBYvXoyffvqpWrVUR62dShs9GnjzTZXG33qr5l6XiIioJp0/rxaOXLsW2LBBbR85UnlbnU5d7DYyUs1+q7iZTEBwMBAUBDRooG4BAeo6cH5+ar/BUKNlV/f728+ZFy0rK0NGRgbGjRtnf8zHxwcWiwVpaWmVPictLQ2jR492eCw+Ph6LFy8GABw8eBA5OTmwWCz2/SEhIYiNjUVaWhqGDBmCtLQ0GI1GeygCAIvFAh8fH6Snp+Ohhx5CWloa7rrrLnsoqnifqVOn4tSpU2jYsOE1a6nM2bNncfbsWfvPhbV1uJADr4mIyB34+QExMer20kvqsd9/VwFp2zY1NOTnn9VFbq1WdUWHiqs6VNfGjWqdJQ04FYxOnjyJ8vJymEwmh8dNJpP9qMzlcnJyKm2fk5Nj31/x2NXaNG7c2LFwPz80atTIoU1UVNQVr1Gxr2HDhtespTKTJ0/GpEmTqtxfY86eVQPZGIyIiMjd3HwzEB+vbhVEgJMngX37gKNHgePH1ay348eBEyfUMgFFRUBxsbo/e1ato3T+vApfGtHund3EuHHjHI4yFRYWIiIioubfaM0aoLRU018GIiKiGqPTqcB0881aV+IUp76FQ0ND4evri9zcXIfHc3NzYTabK32O2Wy+avuK+9zcXISFhTm06dy5s73NiRMnHF7j/PnzyM/Pd3idyt7n0ve4Vi2VMRgMMNTwec4qBQTUzfsQERFRpZyarq/X69GtWzekpqbaH7PZbEhNTUVcFecC4+LiHNoDwMqVK+3to6KiYDabHdoUFhYiPT3d3iYuLg4FBQXIyMiwt1m1ahVsNhtiY2PtbdatW4dz5845vE90dDQaNmxYrVqIiIjIyzk73W3+/PliMBhk7ty5kpWVJSkpKWI0GiUnJ0dERIYOHSpjx461t9+wYYP4+fnJjBkzZO/evTJx4sRKp+sbjUZZsmSJ7Nq1S/r371/pdP0uXbpIenq6rF+/Xlq1auUwXb+goEBMJpMMHTpUMjMzZf78+VKvXr0rputfq5ZrqbXp+kRERFRrqvv97XQwEhF5++23JTIyUvR6vcTExMimTZvs+3r16iVJSUkO7RcsWCCtW7cWvV4v7du3l2XLljnst9lsMn78eDGZTGIwGKR3796SnZ3t0CYvL08SEhIkKChIgoODJTk5WYqKihza7Ny5U3r27CkGg0GaNGkiU6ZMuaL2a9VyLQxGRERE7qfW1jHydrW2jhERERHVmlq9JAgRERGRJ2IwIiIiIrqAwYiIiIjoAgYjIiIiogsYjIiIiIguYDAiIiIiuoDBiIiIiOgCBiMiIiKiCxiMiIiIiC7w07oAd1OxUHhhYaHGlRAREVF1VXxvX+uCHwxGTioqKgIAREREaFwJEREROauoqAghISFV7ue10pxks9lw7NgxNGjQADqdrsZet7CwEBEREThy5IjHXoPN0/vI/rk/T++jp/cP8Pw+sn/XT0RQVFSE8PBw+PhUPZKIR4yc5OPjg1tuuaXWXj84ONgjf9kv5el9ZP/cn6f30dP7B3h+H9m/63O1I0UVOPiaiIiI6AIGIyIiIqILGIxchMFgwMSJE2EwGLQupdZ4eh/ZP/fn6X309P4Bnt9H9q/2cfA1ERER0QU8YkRERER0AYMRERER0QUMRkREREQXMBgRERERXcBg5CJmzZqFZs2aISAgALGxsdi8ebPWJVXLa6+9Bp1O53Br06aNfX9paSlGjRqFm266CUFBQRg4cCByc3MdXuPw4cPo27cv6tWrh8aNG+Oll17C+fPn67orAIB169ahX79+CA8Ph06nw+LFix32iwgmTJiAsLAwBAYGwmKxYN++fQ5t8vPzkZiYiODgYBiNRgwfPhzFxcUObXbt2oU777wTAQEBiIiIwLRp02q7awCu3b/HH3/8is+zT58+Dm1cuX+TJ0/GbbfdhgYNGqBx48YYMGAAsrOzHdrU1O/kmjVr0LVrVxgMBrRs2RJz586t7e4BqF4f77777is+x5EjRzq0cdU+vvfee+jYsaN9gb+4uDisWLHCvt/dP79r9c+dP7vKTJkyBTqdDi+88IL9MZf/DIU0N3/+fNHr9fLxxx/Lnj17ZMSIEWI0GiU3N1fr0q5p4sSJ0r59ezl+/Lj99vvvv9v3jxw5UiIiIiQ1NVW2bt0qt99+u/To0cO+//z583LrrbeKxWKR7du3y/LlyyU0NFTGjRunRXdk+fLl8uqrr8qXX34pAOSrr75y2D9lyhQJCQmRxYsXy86dO+XBBx+UqKgoKSkpsbfp06ePdOrUSTZt2iQ//vijtGzZUhISEuz7rVarmEwmSUxMlMzMTJk3b54EBgbKnDlzNO9fUlKS9OnTx+HzzM/Pd2jjyv2Lj4+XTz75RDIzM2XHjh1y//33S2RkpBQXF9vb1MTv5C+//CL16tWT0aNHS1ZWlrz99tvi6+sr3377rUv0sVevXjJixAiHz9FqtbpFH7/++mtZtmyZ/Pzzz5KdnS2vvPKK+Pv7S2Zmpoi4/+d3rf6582d3uc2bN0uzZs2kY8eO8vzzz9sfd/XPkMHIBcTExMioUaPsP5eXl0t4eLhMnjxZw6qqZ+LEidKpU6dK9xUUFIi/v78sXLjQ/tjevXsFgKSlpYmI+qL28fGRnJwce5v33ntPgoOD5ezZs7Va+7VcHhxsNpuYzWaZPn26/bGCggIxGAwyb948ERHJysoSALJlyxZ7mxUrVohOp5OjR4+KiMi7774rDRs2dOjfmDFjJDo6upZ75KiqYNS/f/8qn+NO/RMROXHihACQtWvXikjN/U6+/PLL0r59e4f3evTRRyU+Pr62u3SFy/soor5cL/0iupy79bFhw4by4YcfeuTnJ3KxfyKe89kVFRVJq1atZOXKlQ59cofPkKfSNFZWVoaMjAxYLBb7Yz4+PrBYLEhLS9Owsurbt28fwsPD0bx5cyQmJuLw4cMAgIyMDJw7d86hb23atEFkZKS9b2lpaejQoQNMJpO9TXx8PAoLC7Fnz5667cg1HDx4EDk5OQ79CQkJQWxsrEN/jEYjunfvbm9jsVjg4+OD9PR0e5u77roLer3e3iY+Ph7Z2dk4depUHfWmamvWrEHjxo0RHR2Np59+Gnl5efZ97tY/q9UKAGjUqBGAmvudTEtLc3iNijZa/J29vI8VPv/8c4SGhuLWW2/FuHHjcObMGfs+d+ljeXk55s+fj9OnTyMuLs7jPr/L+1fBEz67UaNGoW/fvlfU4Q6fIS8iq7GTJ0+ivLzc4RcAAEwmE3766SeNqqq+2NhYzJ07F9HR0Th+/DgmTZqEO++8E5mZmcjJyYFer4fRaHR4jslkQk5ODgAgJyen0r5X7HMlFfVUVu+l/WncuLHDfj8/PzRq1MihTVRU1BWvUbGvYcOGtVJ/dfTp0wcPP/wwoqKicODAAbzyyiv44x//iLS0NPj6+rpV/2w2G1544QXccccduPXWW+3vXxO/k1W1KSwsRElJCQIDA2ujS1eorI8A8Nhjj6Fp06YIDw/Hrl27MGbMGGRnZ+PLL7+8av0V+67Wpi76uHv3bsTFxaG0tBRBQUH46quv0K5dO+zYscMjPr+q+ge4/2cHAPPnz8e2bduwZcuWK/a5w99BBiO6IX/84x/t2x07dkRsbCyaNm2KBQsW1NmXA9WcIUOG2Lc7dOiAjh07okWLFlizZg169+6tYWXOGzVqFDIzM7F+/XqtS6k1VfUxJSXFvt2hQweEhYWhd+/eOHDgAFq0aFHXZTotOjoaO3bsgNVqxf/+9z8kJSVh7dq1WpdVY6rqX7t27dz+szty5Aief/55rFy5EgEBAVqXc114Kk1joaGh8PX1vWJEfm5uLsxms0ZVXT+j0YjWrVtj//79MJvNKCsrQ0FBgUObS/tmNpsr7XvFPldSUc/VPiuz2YwTJ0447D9//jzy8/Pdss/NmzdHaGgo9u/fD8B9+vfMM89g6dKlWL16NW655Rb74zX1O1lVm+Dg4Dr7D0FVfaxMbGwsADh8jq7cR71ej5YtW6Jbt26YPHkyOnXqhP/85z8e8/lV1b/KuNtnl5GRgRMnTqBr167w8/ODn58f1q5di5kzZ8LPzw8mk8nlP0MGI43p9Xp069YNqamp9sdsNhtSU1Mdzjm7i+LiYhw4cABhYWHo1q0b/P39HfqWnZ2Nw4cP2/sWFxeH3bt3O3zZrly5EsHBwfZDy64iKioKZrPZoT+FhYVIT0936E9BQQEyMjLsbVatWgWbzWb/By4uLg7r1q3DuXPn7G1WrlyJ6OhoTU+jVea3335DXl4ewsLCALh+/0QEzzzzDL766iusWrXqilN6NfU7GRcX5/AaFW3q4u/stfpYmR07dgCAw+foyn28nM1mw9mzZz3i86tMRf8q426fXe/evbF7927s2LHDfuvevTsSExPt2y7/Gd7w8G26YfPnzxeDwSBz586VrKwsSUlJEaPR6DAi31W9+OKLsmbNGjl48KBs2LBBLBaLhIaGyokTJ0RETcuMjIyUVatWydatWyUuLk7i4uLsz6+YlnnffffJjh075Ntvv5Wbb75Zs+n6RUVFsn37dtm+fbsAkDfeeEO2b98uv/76q4io6fpGo1GWLFkiu3btkv79+1c6Xb9Lly6Snp4u69evl1atWjlMZy8oKBCTySRDhw6VzMxMmT9/vtSrV69OprNfrX9FRUXy17/+VdLS0uTgwYPyww8/SNeuXaVVq1ZSWlrqFv17+umnJSQkRNasWeMw3fnMmTP2NjXxO1kxVfill16SvXv3yqxZs+psOvS1+rh//355/fXXZevWrXLw4EFZsmSJNG/eXO666y636OPYsWNl7dq1cvDgQdm1a5eMHTtWdDqdfP/99yLi/p/f1frn7p9dVS6faefqnyGDkYt4++23JTIyUvR6vcTExMimTZu0LqlaHn30UQkLCxO9Xi9NmjSRRx99VPbv32/fX1JSIn/+85+lYcOGUq9ePXnooYfk+PHjDq9x6NAh+eMf/yiBgYESGhoqL774opw7d66uuyIiIqtXrxYAV9ySkpJERE3ZHz9+vJhMJjEYDNK7d2/Jzs52eI28vDxJSEiQoKAgCQ4OluTkZCkqKnJos3PnTunZs6cYDAZp0qSJTJkyRfP+nTlzRu677z65+eabxd/fX5o2bSojRoy4IqC7cv8q6xsA+eSTT+xtaup3cvXq1dK5c2fR6/XSvHlzh/eoTdfq4+HDh+Wuu+6SRo0aicFgkJYtW8pLL73ksBaOK/fxiSeekKZNm4per5ebb75ZevfubQ9FIu7/+V2tf+7+2VXl8mDk6p+hTkTkxo87EREREbk/jjEiIiIiuoDBiIiIiOgCBiMiIiKiCxiMiIiIiC5gMCIiIiK6gMGIiIiI6AIGIyIiIqILGIyIiIiILmAwIiIiIrqAwYiIPNLvv/+Op59+GpGRkTAYDDCbzYiPj8eGDRsAADqdDosXL9a2SCJyOX5aF0BEVBsGDhyIsrIyfPrpp2jevDlyc3ORmpqKvLw8rUsjIhfGa6URkccpKChAw4YNsWbNGvTq1euK/c2aNcOvv/5q/7lp06Y4dOgQAGDJkiWYNGkSsrKyEB4ejqSkJLz66qvw81P/j9TpdHj33Xfx9ddfY82aNQgLC8O0adPwyCOP1EnfiKh28VQaEXmcoKAgBAUFYfHixTh79uwV+7ds2QIA+OSTT3D8+HH7zz/++COGDRuG559/HllZWZgzZw7mzp2Lf/7znw7PHz9+PAYOHIidO3ciMTERQ4YMwd69e2u/Y0RU63jEiIg80qJFizBixAiUlJSga9eu6NWrF4YMGYKOHTsCUEd+vvrqKwwYMMD+HIvFgt69e2PcuHH2xz777DO8/PLLOHbsmP15I0eOxHvvvWdvc/vtt6Nr1654991366ZzRFRreMSIiDzSwIEDcezYMXz99dfo06cP1qxZg65du2Lu3LlVPmfnzp14/fXX7UecgoKCMGLECBw/fhxnzpyxt4uLi3N4XlxcHI8YEXkIDr4mIo8VEBCAe++9F/feey/Gjx+PJ598EhMnTsTjjz9eafvi4mJMmjQJDz/8cKWvRUSej0eMiMhrtGvXDqdPnwYA+Pv7o7y83GF/165dkZ2djZYtW15x8/G5+M/lpk2bHJ63adMmtG3btvY7QES1jkeMiMjj5OXlYdCgQXjiiSfQsWNHNGjQAFu3bsW0adPQv39/AGpmWmpqKu644w4YDAY0bNgQEyZMwAMPPIDIyEg88sgj8PHxwc6dO5GZmYl//OMf9tdfuHAhunfvjp49e+Lzzz/H5s2b8dFHH2nVXSKqQRx8TUQe5+zZs3jttdfw/fff48CBAzh37hwiIiIwaNAgvPLKKwgMDMQ333yD0aNH49ChQ2jSpIl9uv53332H119/Hdu3b4e/vz/atGmDJ598EiNGjACgBl/PmjULixcvxrp16xAWFoapU6di8ODBGvaYiGoKgxERkRMqm81GRJ6DY4yIiIiILmAwIiIiIrqAg6+JiJzA0QdEno1HjIiIiIguYDAiIiIiuoDBiIiIiOgCBiMiIiKiCxiMiIiIiC5gMCIiIiK6gMGIiIiI6AIGIyIiIqIL/j/iqHLy+pvzUwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# compute lr \n",
    "test_schedule = CosineSchedule(train_steps=4000, warmup_steps=500)\n",
    "lrs = []\n",
    "for step_num in range(4000):\n",
    "    lrs.append(test_schedule(float(step_num)).numpy())\n",
    "\n",
    "# draw\n",
    "plt.plot(lrs, 'r-', label='learning_rate')\n",
    "plt.xlabel('Step')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_3\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "enc_tokens (InputLayer)         [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "segments (InputLayer)           [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "bert (BERT)                     ((None, 256), (None, 10629632    enc_tokens[0][0]                 \n",
      "                                                                 segments[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "pooled_nsp (PooledOutput)       (None, 2)            66304       bert[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "nsp (Softmax)                   (None, 2)            0           pooled_nsp[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "mlm (Softmax)                   (None, None, 32007)  0           bert[0][1]                       \n",
      "==================================================================================================\n",
      "Total params: 10,695,936\n",
      "Trainable params: 10,695,936\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# 모델 생성\n",
    "pre_train_model = build_model_pre_train(config)\n",
    "pre_train_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_steps: 2000\n"
     ]
    }
   ],
   "source": [
    "epochs = 1 # 1에폭당 10분 이상 소요 -> 테스트로 1에폭만 수행\n",
    "batch_size = 64\n",
    "\n",
    "# optimizer\n",
    "train_steps = math.ceil(len(pre_train_inputs[0]) / batch_size) * epochs\n",
    "print(\"train_steps:\", train_steps)\n",
    "learning_rate = CosineSchedule(train_steps=train_steps, warmup_steps=max(100, train_steps // 10))\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate, beta_1=0.9, beta_2=0.98, epsilon=1e-9)\n",
    "\n",
    "# compile\n",
    "pre_train_model.compile(loss=(tf.keras.losses.sparse_categorical_crossentropy, lm_loss), optimizer=optimizer, metrics={\"nsp\": \"acc\", \"mlm\": lm_acc})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 에러 Function call stack: train_function -> train_function <<<\n",
    "\n",
    "# from tensorflow.compat.v1 import ConfigProto\n",
    "# from tensorflow.compat.v1 import InteractiveSession\n",
    "\n",
    "# config = ConfigProto()\n",
    "# config.gpu_options.allow_growth = True\n",
    "# session = InteractiveSession(config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x7f6e14755dd0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x7f6e14755dd0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: closure mismatch, requested ('self', 'step_function'), but source function had ()\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "ename": "ResourceExhaustedError",
     "evalue": "2 root error(s) found.\n  (0) Resource exhausted:  OOM when allocating tensor with shape[64,128,32007] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[node functional_3/mlm/Softmax (defined at <ipython-input-66-1a6d7d8b5678>:4) ]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n\t [[gradient_tape/functional_3/bert/position_embedding/embedding_2/embedding_lookup/Reshape/_42]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n  (1) Resource exhausted:  OOM when allocating tensor with shape[64,128,32007] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[node functional_3/mlm/Softmax (defined at <ipython-input-66-1a6d7d8b5678>:4) ]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n0 successful operations.\n0 derived errors ignored. [Op:__inference_train_function_100937]\n\nErrors may have originated from an input operation.\nInput Source operations connected to node functional_3/mlm/Softmax:\n functional_3/bert/weight_shared_embedding/Reshape_1 (defined at <ipython-input-48-1a0a9c127264>:63)\n\nInput Source operations connected to node functional_3/mlm/Softmax:\n functional_3/bert/weight_shared_embedding/Reshape_1 (defined at <ipython-input-48-1a0a9c127264>:63)\n\nFunction call stack:\ntrain_function -> train_function\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-66-1a6d7d8b5678>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0msave_weights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModelCheckpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{model_dir}/bert_pre_train.hdf5\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmonitor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"mlm_lm_acc\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_best_only\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"max\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_freq\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"epoch\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_weights_only\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# train\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpre_train_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpre_train_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpre_train_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msave_weights\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    106\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m   \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mInput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m   \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m   \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m   \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1096\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresults\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1097\u001b[0m           \u001b[0;32mreturn\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1098\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1099\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1100\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mpredict_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    778\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    779\u001b[0m     \u001b[0mReturns\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 780\u001b[0;31m       \u001b[0mA\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mConcreteFunction\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0mobject\u001b[0m \u001b[0mwhich\u001b[0m \u001b[0minitializes\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mvariables\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthis\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    781\u001b[0m       \u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    782\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    838\u001b[0m     \u001b[0;31m# that if the cache key has changed, the function will be traced again.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    839\u001b[0m     \u001b[0mconcrete_functions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 840\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mseen_signatures\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    841\u001b[0m       \u001b[0mconcrete_functions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_concrete_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    842\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mconcrete_functions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2827\u001b[0m   \u001b[0mthe\u001b[0m \u001b[0msubgraph\u001b[0m \u001b[0mof\u001b[0m \u001b[0mTensorFlow\u001b[0m \u001b[0moperations\u001b[0m \u001b[0mthat\u001b[0m \u001b[0mwere\u001b[0m \u001b[0mexecuted\u001b[0m \u001b[0mwhen\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mPython\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2828\u001b[0m   \u001b[0mfunction\u001b[0m \u001b[0mwas\u001b[0m \u001b[0mcalled\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0ma\u001b[0m \u001b[0mparticular\u001b[0m \u001b[0minput\u001b[0m \u001b[0msignature\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdefined\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0ma\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2829\u001b[0;31m   \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mshapes\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mdtypes\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mPython\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;31m'\u001b[0m\u001b[0ms\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mvalued\u001b[0m \u001b[0marguments\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2830\u001b[0m   \u001b[0mthe\u001b[0m \u001b[0mvalues\u001b[0m \u001b[0mof\u001b[0m \u001b[0mits\u001b[0m \u001b[0mnon\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mTensor\u001b[0m \u001b[0mPython\u001b[0m \u001b[0mobjects\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2831\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[0;34m(self, args, kwargs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1846\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1847\u001b[0m     \u001b[0mArgs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1848\u001b[0;31m       \u001b[0mg\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIf\u001b[0m \u001b[0mspecified\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mregisters\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mfunction\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mthis\u001b[0m \u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m \u001b[0mthe\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1849\u001b[0m         \u001b[0mcurrent\u001b[0m \u001b[0mcontext\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0meither\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mdefault\u001b[0m \u001b[0mgraph\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mthe\u001b[0m \u001b[0meager\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1850\u001b[0m     \"\"\"\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1922\u001b[0m         \u001b[0;31m# We can avoid computing second-order gradients in some cases by doing a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1923\u001b[0m         \u001b[0;31m# delayed rewrite when graph building. Since we know we'll only compute\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1924\u001b[0;31m         \u001b[0;31m# first-order tape gradients, the delayed rewrite is safe: we won't need\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1925\u001b[0m         \u001b[0;31m# to tell the tape about side outputs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1926\u001b[0m         \u001b[0;31m#\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    548\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m   \u001b[0;32mdef\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mResourceExhaustedError\u001b[0m: 2 root error(s) found.\n  (0) Resource exhausted:  OOM when allocating tensor with shape[64,128,32007] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[node functional_3/mlm/Softmax (defined at <ipython-input-66-1a6d7d8b5678>:4) ]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n\t [[gradient_tape/functional_3/bert/position_embedding/embedding_2/embedding_lookup/Reshape/_42]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n  (1) Resource exhausted:  OOM when allocating tensor with shape[64,128,32007] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[node functional_3/mlm/Softmax (defined at <ipython-input-66-1a6d7d8b5678>:4) ]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n0 successful operations.\n0 derived errors ignored. [Op:__inference_train_function_100937]\n\nErrors may have originated from an input operation.\nInput Source operations connected to node functional_3/mlm/Softmax:\n functional_3/bert/weight_shared_embedding/Reshape_1 (defined at <ipython-input-48-1a0a9c127264>:63)\n\nInput Source operations connected to node functional_3/mlm/Softmax:\n functional_3/bert/weight_shared_embedding/Reshape_1 (defined at <ipython-input-48-1a0a9c127264>:63)\n\nFunction call stack:\ntrain_function -> train_function\n"
     ]
    }
   ],
   "source": [
    "# save weights callback\n",
    "save_weights = tf.keras.callbacks.ModelCheckpoint(f\"{model_dir}/bert_pre_train.hdf5\", monitor=\"mlm_lm_acc\", verbose=1, save_best_only=True, mode=\"max\", save_freq=\"epoch\", save_weights_only=True)\n",
    "# train\n",
    "history = pre_train_model.fit(pre_train_inputs, pre_train_labels, epochs=epochs, batch_size=batch_size, callbacks=[save_weights])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'history' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-67-4eac24868e5b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'nsp_loss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'b-'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'nsp_loss'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'mlm_loss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r--'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'mlm_loss'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxlabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Epoch'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'history' is not defined"
     ]
    }
   ],
   "source": [
    "# training result\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['nsp_loss'], 'b-', label='nsp_loss')\n",
    "plt.plot(history.history['mlm_loss'], 'r--', label='mlm_loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['nsp_acc'], 'g-', label='nsp_acc')\n",
    "plt.plot(history.history['mlm_lm_acc'], 'k--', label='mlm_acc')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
