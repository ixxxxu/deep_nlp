{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Seq2seq 번역기 만들기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1. 데이터 다운로드\n",
    "아래 링크에서 korean-english-park.train.tar.gz 를 다운로드받아 한영 병렬 데이터를 확보합니다.\n",
    "\n",
    "- jungyeul/korean-parallel-corpora -https://github.com/jungyeul/korean-parallel-corpora/tree/master/korean-english-news-v1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2. 데이터 정제\n",
    "1) set 데이터형이 중복을 허용하지 않는다는 것을 활용해 중복된 데이터를 제거하도록 합니다. 데이터의 병렬 쌍이 흐트러지지 않게 주의하세요! 중복을 제거한 데이터를 cleaned_corpus 에 저장합니다.\n",
    "\n",
    "2) 앞서 정의한 preprocessing() 함수는 한글에 대해 동작하지 않습니다. 한글에 적용할 수 있는 정규식을 추가하여 함수를 재정의하세요!\n",
    "\n",
    "3) 타겟 언어인 영문엔 <start> 토큰과 <end> 토큰을 추가하고 split() 함수로 토큰화합니다. 한글 토큰화는 KoNLPy의 mecab 클래스를 사용합니다. KoNLPy가 설치되어 있지 않다면 아래 문서를 참고해 설치해 주세요.\n",
    "\n",
    "설치하기-KoNLP https://konlpy.org/ko/latest/install/#ubuntu\n",
    "\n",
    "- 모든 데이터를 사용할 경우 학습에 굉장히 오랜 시간이 걸립니다. cleaned_corpus로부터 토큰의 길이가 40 이하인 데이터를 선별하여 eng_corpus와 kor_corpus를 각각 구축하세요."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3. 데이터 토큰화\n",
    "    \n",
    "앞서 정의한 tokenize() 함수를 사용해 데이터를 텐서로 변환하고 각각의 tokenizer를 얻으세요! 단어의 수는 실험을 통해 적당한 값을 맞춰주도록 합니다! (최소 10,000 이상!)\n",
    "\n",
    "난이도에 비해 데이터가 많지 않아 훈련 데이터와 검증 데이터를 따로 나누지는 않습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4. 모델 설계\n",
    "한국어를 영어로 잘 번역해 줄 멋진 Attention 기반 Seq2seq 모델을 설계하세요! 앞서 만든 모델에 Dropout 모듈을 추가하면 성능이 더 좋아질 거랍니다! Embedding Size와 Hidden Size는 실험을 통해 적당한 값을 맞춰 주도록 합니다!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5. 훈련하기\n",
    "훈련엔 위에서 사용한 코드를 그대로 사용하되, eval_step() 부분이 없음에 유의합니다! 매 스텝 아래의 예문에 대한 번역을 생성하여 본인이 생각하기에 가장 멋지게 번역한 Case를 제출하세요! (Attention Map을 시각화해보는 것도 재밌을 거예요!)\n",
    "\n",
    "참고: 데이터의 난이도가 높은 편이므로 생각만큼 결과가 잘 안나올 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 루브릭\n",
    "\n",
    "1. 번역기 모델 학습에 필요한 텍스트 데이터 전처리가 한국어 포함하여 잘 이루어졌다.\n",
    "    - 구두점, 대소문자, 띄어쓰기, 한글 형태소분석 등 번역기 모델에 요구되는 전처리가 정상적으로 진행되었다.\n",
    "\n",
    "2. Attentional Seq2seq 모델이 정상적으로 구동된다.\n",
    "    - seq2seq 모델 훈련 과정에서 training loss가 안정적으로 떨어지면서 학습이 진행됨이 확인되었다.\n",
    "    \n",
    "3. 테스트 결과 의미가 통하는 수준의 번역문이 생성되었다.\n",
    "    - 테스트용 디코더 모델이 정상적으로 만들어져서, 정답과 어느 정도 유사한 영어 번역이 진행됨을 확인하였다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 예문 ##\n",
    "# K1) 오바마는 대통령이다.\n",
    "# K2) 시민들은 도시 속에 산다.\n",
    "# K3) 커피는 필요 없다.\n",
    "# K4) 일곱 명의 사망자가 발생했다.\n",
    "\n",
    "## 제출 ##\n",
    "# E1) obama is the president . <end>\n",
    "# E2) people are victims of the city . <end>\n",
    "# E2) the price is not enough . <end>\n",
    "# E2) seven people have died . <end>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 한글폰트설치\n",
    "#!sudo apt -qq -y install fonts-nanum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# matplotlib 폰트 설정\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    " \n",
    "%config InlineBackend.figure_format = 'retina'\n",
    " \n",
    "import matplotlib.font_manager as fm\n",
    "fontpath = '/usr/share/fonts/truetype/nanum/NanumBarunGothic.ttf'\n",
    "font = fm.FontProperties(fname=fontpath, size=9)\n",
    "plt### Seq2seq 번역기 만들기.rc('font', family='NanumBarunGothic') \n",
    "mpl.font_manager._rebuild()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 데이터 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import matplotlib.ticker as ticker\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import time\n",
    "import re\n",
    "import os\n",
    "import io\n",
    "\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #데이터 다운로드 -> get_file()함수로 URL로 부터 데이터 다운로드(압축파일일 경우 압축해제 수행)\n",
    "# path_to_zip = tf.keras.utils.get_file(\n",
    "#     'spa-eng.zip',\n",
    "#     origin='http://storage.googleapis.com/download.tensorflow.org/data/spa-eng.zip',\n",
    "#     extract=True)\n",
    "\n",
    "# path_to_file = os.path.dirname(path_to_zip)+\"/spa-eng/spa.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "#데이터 다운로드 -> get_file()함수로 URL로 부터 데이터 다운로드(압축파일일 경우 압축해제 수행)\n",
    "# path_to_zip = tf.keras.utils.get_file(\n",
    "#     'korean-english-park.train.tar.gz',\n",
    "#     origin='https://github.com/jungyeul/korean-parallel-corpora/blob/master/korean-english-news-v1/korean-english-park.train.tar.gz?raw=true',\n",
    "#     extract=True)\n",
    "#home = '/home/aiffel/aiffel/s2s_translation/korean-english-park.train'\n",
    "kor_train = \"/home/aiffel/aiffel/s2s_translation/korean-english-park.train/korean-english-park.train.ko\"\n",
    "eng_train = \"/home/aiffel/aiffel/s2s_translation/korean-english-park.train/korean-english-park.train.en\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/aiffel/aiffel/s2s_translation/korean-english-park.train/korean-english-park.train.ko'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kor_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Size: 94123\n",
      "Example:\n",
      ">> 개인용 컴퓨터 사용의 상당 부분은 \"이것보다 뛰어날 수 있느냐?\"\n",
      ">> 북한의 핵무기 계획을 포기하도록 하려는 압력이 거세지고 있는 가운데, 일본과 북한의 외교관들이 외교 관계를 정상화하려는 회담을 재개했다.\n",
      ">> \"경호 로보트가 침입자나 화재를 탐지하기 위해서 개인적으로, 그리고 전문적으로 사용되고 있습니다.\"\n",
      ">> 수자원부 당국은 논란이 되고 있고, 막대한 비용이 드는 이 사업에 대해 내년에 건설을 시작할 계획이다.\n",
      ">> 또한 근력 운동은 활발하게 걷는 것이나 최소한 20분 동안 뛰는 것과 같은 유산소 활동에서 얻는 운동 효과를 심장과 폐에 주지 않기 때문에, 연구학자들은 근력 운동이 심장에 큰 영향을 미치는지 여부에 대해 논쟁을 해왔다.\n"
     ]
    }
   ],
   "source": [
    "#데이터 불러오기\n",
    "with open(kor_train, \"r\") as f:\n",
    "    raw = f.read().splitlines()\n",
    "\n",
    "print(\"Data Size:\", len(raw))\n",
    "print(\"Example:\")\n",
    "\n",
    "for sen in raw[0:100][::20]: print(\">>\", sen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 데이터 전처리 : 정제하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_sentence(sentence, s_token=False, e_token=False):\n",
    "    sentence = sentence.lower().strip()\n",
    "\n",
    "    sentence = re.sub(r\"([?.!,])\", r\" \\1 \", sentence)\n",
    "    sentence = re.sub(r'[\" \"]+', \" \", sentence)\n",
    "    sentence = re.sub(r\"[^a-zA-Z?.!,]+\", \" \", sentence)\n",
    "    sentence = re.sub(r\"[^[가-힣]+\",\" \", sentence) #한글 정규표현식\n",
    "    sentence = sentence.strip()\n",
    "\n",
    "    if s_token:\n",
    "        sentence = '<start> ' + sentence #시작문자\n",
    "\n",
    "    if e_token:\n",
    "        sentence += ' <end>'#종료문자\n",
    "    \n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English: go away !\n",
      "Spanish: <start> salga de aqu ! <end>\n"
     ]
    }
   ],
   "source": [
    "enc_corpus = []\n",
    "dec_corpus = []\n",
    "\n",
    "num_examples = 30000\n",
    "\n",
    "for pair in raw[:num_examples]:\n",
    "    eng, spa = pair.split(\"\\t\")\n",
    "\n",
    "    enc_corpus.append(preprocess_sentence(eng))\n",
    "    dec_corpus.append(preprocess_sentence(spa, s_token=True, e_token=True))\n",
    "\n",
    "print(\"Korean:\", enc_corpus[100])   # go away !\n",
    "print(\"English:\", dec_corpus[100])   # <start> salga de aqu ! <end>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 데이터 전처리 : 토큰화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def tokenize(corpus):\n",
    "#     tokenizer = tf.keras.preprocessing.text.Tokenizer(filters='',num_words=10000)\n",
    "#     tokenizer.fit_on_texts(corpus)\n",
    "\n",
    "#     tensor = tokenizer.texts_to_sequences(corpus)\n",
    "\n",
    "#     tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor, padding='post')\n",
    "\n",
    "#     return tensor, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from konlpy.tag import Mecab\n",
    "def tokenize(corpus):\n",
    "    tokenizer = Mecab()\n",
    "\n",
    "    output_tokens = [ '[PAD]', '[UNK]', '[CLS]', '[SEP]', '[MASK]' ]\n",
    "    for wst in whitespace_tokenize(korquad_data):   # wst : 공백,탭,엔터 기준 문자열 하나\n",
    "          count = 0\n",
    "          for token in tokenizer.morphs(wst):       # token : wst를 형태소 분석한 토큰 하나\n",
    "              tk = token\n",
    "\n",
    "              if count > 0:\n",
    "                  tk = \"##\" + tk\n",
    "                  output_tokens.append(tk)\n",
    "              else:  # count==0\n",
    "                  output_tokens.append(tk)  # 맨 처음 token만 앞에 ##을 붙이지 않음 \n",
    "                  count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English Vocab Size: 4931\n",
      "Spanish Vocab Size: 8893\n"
     ]
    }
   ],
   "source": [
    "# 토큰화하기\n",
    "enc_tensor, enc_tokenizer = tokenize(enc_corpus)\n",
    "dec_tensor, dec_tokenizer = tokenize(dec_corpus)\n",
    "\n",
    "# 훈련 데이터와 검증 데이터로 분리하기\n",
    "enc_train, enc_val, dec_train, dec_val = \\\n",
    "train_test_split(enc_tensor, dec_tensor, test_size=0.2)\n",
    "\n",
    "#분리된 단어장 크기 확인 \n",
    "print(\"Korean Vocab Size:\", len(enc_tokenizer.index_word))\n",
    "print(\"English Vocab Size:\", len(dec_tokenizer.index_word))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 모델 설계\n",
    "- 각각 1개의 GRU를 갖는 Encoder-Decoder 구조 설계\n",
    "- Encoder \n",
    "    - 모든 Time-step의 Hidden State를 출력으로\n",
    "- Decoder \n",
    "    - Encoder의 출력과 Decoder의 t-1 Step의 Hidden State로 Attention을 취함 -> tStep Hidden Stat를 만들어 냄\n",
    "    - tStep의 단어로 예측된 결과 <-> 실제 정답과 대조 => Loss 값을 구함\n",
    "    - tStep의 Hidden State t+1 Step의 Hidden State를 만들기 위해 다시 Decoder로 전달\n",
    "    - t=1 일때 Hidden State 는 Encoder의 Final State를 사용\n",
    "    - Attention은 Bahdanau을 사용\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BahdanauAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, units):\n",
    "        super(BahdanauAttention, self).__init__()\n",
    "        self.w_dec = tf.keras.layers.Dense(units)\n",
    "        self.w_enc = tf.keras.layers.Dense(units)\n",
    "        self.w_com = tf.keras.layers.Dense(1)\n",
    "    \n",
    "    def call(self, h_enc, h_dec):\n",
    "        # h_enc shape: [batch x length x units]\n",
    "        # h_dec shape: [batch x units]\n",
    "\n",
    "        h_enc = self.w_enc(h_enc)\n",
    "        h_dec = tf.expand_dims(h_dec, 1)\n",
    "        h_dec = self.w_dec(h_dec)\n",
    "\n",
    "        score = self.w_com(tf.nn.tanh(h_dec + h_enc))\n",
    "        \n",
    "        attn = tf.nn.softmax(score, axis=1)\n",
    "\n",
    "        context_vec = attn * h_enc\n",
    "        context_vec = tf.reduce_sum(context_vec, axis=1)\n",
    "\n",
    "        return context_vec, attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, enc_units):\n",
    "        super(Encoder, self).__init__()\n",
    "\n",
    "        self.enc_units = enc_units\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "        self.gru = tf.keras.layers.GRU(enc_units,\n",
    "                                       return_sequences=True)\n",
    "\n",
    "    def call(self, x):\n",
    "        out = self.embedding(x)\n",
    "        out = self.gru(out)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, dec_units):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.dec_units = dec_units\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "        self.gru = tf.keras.layers.GRU(dec_units,\n",
    "                                       return_sequences=True,\n",
    "                                       return_state=True)\n",
    "        self.fc = tf.keras.layers.Dense(vocab_size)\n",
    "\n",
    "        self.attention = BahdanauAttention(self.dec_units)\n",
    "\n",
    "    def call(self, x, h_dec, enc_out):\n",
    "        context_vec, attn = self.attention(enc_out, h_dec)\n",
    "\n",
    "        out = self.embedding(x)\n",
    "        out = tf.concat([tf.expand_dims(context_vec, 1), out], axis=-1)\n",
    "\n",
    "        out, h_dec = self.gru(out)\n",
    "        out = tf.reshape(out, (-1, out.shape[2]))\n",
    "        out = self.fc(out)\n",
    "\n",
    "        return out, h_dec, attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder Output: (64, 30, 1024)\n",
      "Decoder Output: (64, 8894)\n",
      "Decoder Hidden State: (64, 1024)\n",
      "Attention: (64, 30, 1)\n"
     ]
    }
   ],
   "source": [
    "# 코드를 실행하세요.\n",
    "\n",
    "BATCH_SIZE     = 64\n",
    "SRC_VOCAB_SIZE = len(enc_tokenizer.index_word) + 1\n",
    "TGT_VOCAB_SIZE = len(dec_tokenizer.index_word) + 1\n",
    "\n",
    "units         = 1024\n",
    "embedding_dim = 512\n",
    "\n",
    "encoder = Encoder(SRC_VOCAB_SIZE, embedding_dim, units)\n",
    "decoder = Decoder(TGT_VOCAB_SIZE, embedding_dim, units)\n",
    "\n",
    "# sample input\n",
    "sequence_len = 30\n",
    "\n",
    "sample_enc = tf.random.uniform((BATCH_SIZE, sequence_len))\n",
    "sample_output = encoder(sample_enc)\n",
    "\n",
    "print ('Encoder Output:', sample_output.shape)\n",
    "\n",
    "sample_state = tf.random.uniform((BATCH_SIZE, units))\n",
    "\n",
    "sample_logits, h_dec, attn = decoder(tf.random.uniform((BATCH_SIZE, 1)),\n",
    "                                     sample_state, sample_output)\n",
    "\n",
    "print ('Decoder Output:', sample_logits.shape)\n",
    "print ('Decoder Hidden State:', h_dec.shape)\n",
    "print ('Attention:', attn.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 모델 훈련 \n",
    "#### Optimizer & Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam()\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True, reduction='none')\n",
    "\n",
    "def loss_function(real, pred):\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    loss = loss_object(real, pred)\n",
    "    \n",
    "    mask = tf.cast(mask, dtype=loss.dtype)\n",
    "    loss *= mask\n",
    "    \n",
    "    return tf.reduce_mean(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### train_step 구현\n",
    "- train_step()을 통해 학습에 필요한 것을 가져와 Loss를 계산한 후 반환하는 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(src, tgt, encoder, decoder, optimizer, dec_tok):\n",
    "    bsz = src.shape[0]\n",
    "    loss = 0\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        enc_out = encoder(src)\n",
    "        h_dec = enc_out[:, -1]\n",
    "        \n",
    "        dec_src = tf.expand_dims([dec_tok.word_index['<start>']] * bsz, 1)\n",
    "\n",
    "        for t in range(1, tgt.shape[1]):\n",
    "            pred, h_dec, _ = decoder(dec_src, h_dec, enc_out)\n",
    "\n",
    "            loss += loss_function(tgt[:, t], pred)\n",
    "            dec_src = tf.expand_dims(tgt[:, t], 1)\n",
    "        \n",
    "    batch_loss = (loss / int(tgt.shape[1]))\n",
    "\n",
    "    variables = encoder.trainable_variables + decoder.trainable_variables\n",
    "    gradients = tape.gradient(loss, variables)\n",
    "    optimizer.apply_gradients(zip(gradients, variables))\n",
    "    \n",
    "    return batch_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 훈련 시작하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch  1: 100%|██████████| 375/375 [01:06<00:00,  5.67it/s, Loss 1.3362]\n",
      "Epoch  2: 100%|██████████| 375/375 [01:09<00:00,  5.39it/s, Loss 0.8762]\n",
      "Epoch  3: 100%|██████████| 375/375 [01:17<00:00,  4.82it/s, Loss 0.6103]\n",
      "Epoch  4: 100%|██████████| 375/375 [01:19<00:00,  4.70it/s, Loss 0.4374]\n",
      "Epoch  5: 100%|██████████| 375/375 [01:21<00:00,  4.63it/s, Loss 0.3241]\n",
      "Epoch  6: 100%|██████████| 375/375 [01:22<00:00,  4.55it/s, Loss 0.2532]\n",
      "Epoch  7: 100%|██████████| 375/375 [01:22<00:00,  4.52it/s, Loss 0.2066]\n",
      "Epoch  8: 100%|██████████| 375/375 [01:22<00:00,  4.52it/s, Loss 0.1762]\n",
      "Epoch  9: 100%|██████████| 375/375 [01:23<00:00,  4.52it/s, Loss 0.1554]\n",
      "Epoch 10: 100%|██████████| 375/375 [01:22<00:00,  4.53it/s, Loss 0.1421]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm    # tqdm\n",
    "import random\n",
    "\n",
    "EPOCHS = 10\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    total_loss = 0\n",
    "    \n",
    "    idx_list = list(range(0, enc_train.shape[0], BATCH_SIZE))\n",
    "    random.shuffle(idx_list)\n",
    "    t = tqdm(idx_list)    # tqdm\n",
    "\n",
    "    for (batch, idx) in enumerate(t):\n",
    "        batch_loss = train_step(enc_train[idx:idx+BATCH_SIZE],\n",
    "                                dec_train[idx:idx+BATCH_SIZE],\n",
    "                                encoder,\n",
    "                                decoder,\n",
    "                                optimizer,\n",
    "                                dec_tokenizer)\n",
    "    \n",
    "        total_loss += batch_loss\n",
    "        \n",
    "        t.set_description_str('Epoch %2d' % (epoch + 1))    # tqdm\n",
    "        t.set_postfix_str('Loss %.4f' % (total_loss.numpy() / (batch + 1)))    # tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 실습하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch  1: 100%|██████████| 375/375 [01:16<00:00,  4.92it/s, Loss 0.1288]\n",
      "Test Epoch  1: 100%|██████████| 94/94 [00:19<00:00,  4.85it/s, Test Loss 0.6983]\n",
      "Epoch  2: 100%|██████████| 375/375 [01:20<00:00,  4.67it/s, Loss 0.1212]\n",
      "Test Epoch  2: 100%|██████████| 94/94 [00:07<00:00, 13.21it/s, Test Loss 0.7075]\n",
      "Epoch  3: 100%|██████████| 375/375 [01:24<00:00,  4.42it/s, Loss 0.1144]\n",
      "Test Epoch  3: 100%|██████████| 94/94 [00:07<00:00, 13.23it/s, Test Loss 0.7089]\n",
      "Epoch  4: 100%|██████████| 375/375 [01:24<00:00,  4.44it/s, Loss 0.1069]\n",
      "Test Epoch  4: 100%|██████████| 94/94 [00:07<00:00, 13.14it/s, Test Loss 0.7179]\n",
      "Epoch  5: 100%|██████████| 375/375 [01:16<00:00,  4.92it/s, Loss 0.1018]\n",
      "Test Epoch  5: 100%|██████████| 94/94 [00:06<00:00, 14.50it/s, Test Loss 0.7370]\n",
      "Epoch  6: 100%|██████████| 375/375 [01:14<00:00,  5.00it/s, Loss 0.0969]\n",
      "Test Epoch  6: 100%|██████████| 94/94 [00:06<00:00, 14.37it/s, Test Loss 0.7402]\n",
      "Epoch  7: 100%|██████████| 375/375 [01:15<00:00,  4.96it/s, Loss 0.0959]\n",
      "Test Epoch  7: 100%|██████████| 94/94 [00:06<00:00, 14.44it/s, Test Loss 0.7368]\n",
      "Epoch  8: 100%|██████████| 375/375 [01:15<00:00,  4.99it/s, Loss 0.0911]\n",
      "Test Epoch  8: 100%|██████████| 94/94 [00:06<00:00, 14.49it/s, Test Loss 0.7600]\n",
      "Epoch  9: 100%|██████████| 375/375 [01:15<00:00,  5.00it/s, Loss 0.0899]\n",
      "Test Epoch  9: 100%|██████████| 94/94 [00:06<00:00, 14.24it/s, Test Loss 0.7638]\n",
      "Epoch 10: 100%|██████████| 375/375 [01:15<00:00,  4.98it/s, Loss 0.0902]\n",
      "Test Epoch 10: 100%|██████████| 94/94 [00:06<00:00, 14.45it/s, Test Loss 0.7631]\n"
     ]
    }
   ],
   "source": [
    "# eval_step() 정의하기\n",
    "# train_step() 이후 eval_step() 진행하도록 소스 수정하기\n",
    "\n",
    "# Define eval_step\n",
    "\n",
    "@tf.function\n",
    "def eval_step(src, tgt, encoder, decoder, dec_tok):\n",
    "    bsz = src.shape[0]\n",
    "    loss = 0\n",
    "\n",
    "    enc_out = encoder(src)\n",
    "\n",
    "    h_dec = enc_out[:, -1]\n",
    "\n",
    "    dec_src = tf.expand_dims([dec_tok.word_index['<start>']] * bsz, 1)\n",
    "\n",
    "    for t in range(1, tgt.shape[1]):\n",
    "        pred, h_dec, _ = decoder(dec_src, h_dec, enc_out)\n",
    "\n",
    "        loss += loss_function(tgt[:, t], pred)\n",
    "        dec_src = tf.expand_dims(tgt[:, t], 1)\n",
    "\n",
    "    batch_loss = (loss / int(tgt.shape[1]))\n",
    "\n",
    "    return batch_loss\n",
    "\n",
    "\n",
    "# Training Process\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "EPOCHS = 10\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    total_loss = 0\n",
    "\n",
    "    idx_list = list(range(0, enc_train.shape[0], BATCH_SIZE))\n",
    "    random.shuffle(idx_list)\n",
    "    t = tqdm(idx_list)\n",
    "\n",
    "    for (batch, idx) in enumerate(t):\n",
    "        batch_loss = train_step(enc_train[idx:idx+BATCH_SIZE],\n",
    "                                dec_train[idx:idx+BATCH_SIZE],\n",
    "                                encoder,\n",
    "                                decoder,\n",
    "                                optimizer,\n",
    "                                dec_tokenizer)\n",
    "\n",
    "        total_loss += batch_loss\n",
    "\n",
    "        t.set_description_str('Epoch %2d' % (epoch + 1))\n",
    "        t.set_postfix_str('Loss %.4f' % (total_loss.numpy() / (batch + 1)))\n",
    "\n",
    "    test_loss = 0\n",
    "\n",
    "    idx_list = list(range(0, enc_val.shape[0], BATCH_SIZE))\n",
    "    random.shuffle(idx_list)\n",
    "    t = tqdm(idx_list)\n",
    "\n",
    "    for (test_batch, idx) in enumerate(t):\n",
    "        test_batch_loss = eval_step(enc_val[idx:idx+BATCH_SIZE],\n",
    "                                    dec_val[idx:idx+BATCH_SIZE],\n",
    "                                    encoder,\n",
    "                                    decoder,\n",
    "                                    dec_tokenizer)\n",
    "\n",
    "        test_loss += test_batch_loss\n",
    "\n",
    "        t.set_description_str('Test Epoch %2d' % (epoch + 1))\n",
    "        t.set_postfix_str('Test Loss %.4f' % (test_loss.numpy() / (test_batch + 1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 평가하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: can i have some coffee ?\n",
      "Predicted translation: puedo tomar un poco caf ? <end> \n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 608,
       "width": 528
      },
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def evaluate(sentence, encoder, decoder):\n",
    "    attention = np.zeros((dec_train.shape[-1], enc_train.shape[-1]))\n",
    "    \n",
    "    sentence = preprocess_sentence(sentence)\n",
    "    inputs = enc_tokenizer.texts_to_sequences([sentence.split()])\n",
    "    inputs = tf.keras.preprocessing.sequence.pad_sequences(inputs,\n",
    "                                                           maxlen=enc_train.shape[-1],\n",
    "                                                           padding='post')\n",
    "\n",
    "    result = ''\n",
    "\n",
    "    enc_out = encoder(inputs)\n",
    "\n",
    "    dec_hidden = enc_out[:, -1]\n",
    "    dec_input = tf.expand_dims([dec_tokenizer.word_index['<start>']], 0)\n",
    "\n",
    "    for t in range(dec_train.shape[-1]):\n",
    "        predictions, dec_hidden, attention_weights = decoder(dec_input,\n",
    "                                                             dec_hidden,\n",
    "                                                             enc_out)\n",
    "\n",
    "        attention_weights = tf.reshape(attention_weights, (-1, ))\n",
    "        attention[t] = attention_weights.numpy()\n",
    "\n",
    "        predicted_id = \\\n",
    "        tf.argmax(tf.math.softmax(predictions, axis=-1)[0]).numpy()\n",
    "\n",
    "        result += dec_tokenizer.index_word[predicted_id] + ' '\n",
    "\n",
    "        if dec_tokenizer.index_word[predicted_id] == '<end>':\n",
    "            return result, sentence, attention\n",
    "\n",
    "        dec_input = tf.expand_dims([predicted_id], 0)\n",
    "\n",
    "    return result, sentence, attention\n",
    "\n",
    "\n",
    "def plot_attention(attention, sentence, predicted_sentence):\n",
    "    fig = plt.figure(figsize=(10,10))\n",
    "    ax = fig.add_subplot(1, 1, 1)\n",
    "    ax.matshow(attention, cmap='viridis')\n",
    "\n",
    "    fontdict = {'fontsize': 14}\n",
    "\n",
    "    ax.set_xticklabels([''] + sentence, fontdict=fontdict, rotation=90)\n",
    "    ax.set_yticklabels([''] + predicted_sentence, fontdict=fontdict)\n",
    "\n",
    "    ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "    ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def translate(sentence, encoder, decoder):\n",
    "    result, sentence, attention = evaluate(sentence, encoder, decoder)\n",
    "\n",
    "    print('Input: %s' % (sentence))\n",
    "    print('Predicted translation: {}'.format(result))\n",
    "    \n",
    "    attention = attention[:len(result.split()), :len(sentence.split())]\n",
    "    plot_attention(attention, sentence.split(), result.split(' '))\n",
    "\n",
    "\n",
    "translate(\"Can I have some coffee?\", encoder, decoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 회고 \n",
    "- 2개로 분리된 데이터를 정제하는 과정의 코드 구현이 잘 이루어지지 않음\n",
    "- 한글 정규표현식 적용이 잘 이루어지지 않음\n",
    "- mecab() 을 이용한 한글 토크나이저 함수 선언과 적용이 잘 이루어지지 않음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
